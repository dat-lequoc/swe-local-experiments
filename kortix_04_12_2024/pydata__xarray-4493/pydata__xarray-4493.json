{
  "instance_id": "pydata__xarray-4493",
  "model_patch": "diff --git a/xarray/core/merge.py b/xarray/core/merge.py\nindex d29a9e1f..e8119374 100644\n--- a/xarray/core/merge.py\n+++ b/xarray/core/merge.py\n@@ -916,6 +916,17 @@ def dataset_update_method(\n     \"\"\"\n     from .dataarray import DataArray\n     from .dataset import Dataset\n+    from .duck_array_ops import is_duck_dask_array\n+\n+    def _maybe_preserve_chunks(var, data):\n+        \"\"\"Preserve chunks in variable if data is a dask array\"\"\"\n+        if is_duck_dask_array(data):\n+            if hasattr(data, 'chunks'):\n+                var.encoding['chunks'] = data.chunks\n+                # Ensure the variable data remains dask array\n+                if not is_duck_dask_array(var.data):\n+                    var.data = data\n+        return var\n \n     if not isinstance(other, Dataset):\n         other = dict(other)\n@@ -929,6 +940,14 @@ def dataset_update_method(\n                 ]\n                 if coord_names:\n                     other[key] = value.drop_vars(coord_names)\n+                # Preserve chunks if present\n+                if is_duck_dask_array(value.data):\n+                    other[key] = _maybe_preserve_chunks(other[key], value.data)\n+            elif isinstance(value, tuple) and len(value) >= 2:\n+                dims, data = value[:2]\n+                if is_duck_dask_array(data):\n+                    var = as_variable(value)\n+                    other[key] = _maybe_preserve_chunks(var, data)\n \n     # use ds.coords and not ds.indexes, else str coords are cast to object\n     indexes = {key: dataset.coords[key] for key in dataset.indexes.keys()}\n@@ -937,4 +956,4 @@ def dataset_update_method(\n         priority_arg=1,\n         indexes=indexes,\n         combine_attrs=\"override\",\n-    )\n+    )\n\\ No newline at end of file\ndiff --git a/xarray/tests/test_dask.py b/xarray/tests/test_dask.py\nindex 19a61c60..d81d4bae 100644\n--- a/xarray/tests/test_dask.py\n+++ b/xarray/tests/test_dask.py\n@@ -674,6 +674,32 @@ class TestDataArrayAndDataset(DaskTestCase):\n         assert not ds1[\"y\"]._in_memory\n         assert not ds2[\"y\"]._in_memory\n \n+    def test_update_preserve_chunks(self):\n+        \"\"\"Test that update preserves chunks in dask arrays\"\"\"\n+        import dask.array as da\n+        \n+        # Create chunked dataset\n+        data = da.random.random((4, 4), chunks=(2, 2))\n+        ds = Dataset({\"foo\": ((\"x\", \"y\"), data)})\n+        \n+        # Verify initial chunks\n+        assert isinstance(ds.foo.data, da.Array)\n+        assert ds.foo.chunks == ((2, 2), (2, 2))\n+        \n+        # Update with sliced data\n+        update_data = ds.foo[1:, :]\n+        ds.update({\"foo\": ((\"x\", \"y\"), update_data)})\n+        \n+        # Verify chunks are preserved\n+        assert isinstance(ds.foo.data, da.Array)\n+        assert ds.foo.chunks is not None\n+        \n+        # Test updating with DataArray\n+        ds2 = Dataset({\"bar\": ((\"x\", \"y\"), da.ones((4, 4), chunks=(2, 2)))})\n+        ds.update({\"bar\": ds2.bar})\n+        assert isinstance(ds.bar.data, da.Array)\n+        assert ds.bar.chunks == ((2, 2), (2, 2))\n+\n     def test_dataarray_getattr(self):\n         # ipython/jupyter does a long list of getattr() calls to when trying to\n         # represent an object.\n@@ -1598,4 +1624,4 @@ def test_optimize():\n     a = dask.array.ones((10, 4), chunks=(5, 2))\n     arr = xr.DataArray(a).chunk(5)\n     (arr2,) = dask.optimize(arr)\n-    arr2.compute()\n+    arr2.compute()\n\\ No newline at end of file\n",
  "model_name_or_path": "KortixAI"
}