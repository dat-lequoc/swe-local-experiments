{
  "instance_id": "psf__requests-3362",
  "model_patch": "diff --git a/requests/utils.py b/requests/utils.py\nindex 8d17b6b2..0dbcdbd9 100644\n--- a/requests/utils.py\n+++ b/requests/utils.py\n@@ -357,21 +357,66 @@ def get_encoding_from_headers(headers):\n \n \n def stream_decode_response_unicode(iterator, r):\n-    \"\"\"Stream decodes a iterator.\"\"\"\n-\n-    if r.encoding is None:\n-        for item in iterator:\n-            yield item\n+    \"\"\"Stream decodes an iterator.\n+    \n+    This function provides streaming unicode decoding that matches the behavior\n+    of Response.text, including proper encoding detection and error handling.\n+    \"\"\"\n+    # Get the encoding - use the same logic as Response.text\n+    encoding = r.encoding\n+    \n+    # Only attempt encoding detection if we don't have an encoding\n+    # and haven't consumed the content yet\n+    if encoding is None and not r._content_consumed:\n+        try:\n+            # Read just enough content to detect encoding\n+            chunk = next(iterator)\n+            encoding = chardet.detect(chunk)['encoding']\n+            \n+            # Start iteration with our first chunk\n+            if encoding is None:\n+                yield chunk\n+            else:\n+                decoder = codecs.getincrementaldecoder(encoding)(errors='replace')\n+                rv = decoder.decode(chunk)\n+                if rv:\n+                    yield rv\n+        except StopIteration:\n+            return\n+        except (UnicodeDecodeError, LookupError):\n+            # If detection or decoding fails, yield raw bytes\n+            yield chunk\n+            encoding = None\n+    \n+    # If we still don't have an encoding, yield raw bytes\n+    if encoding is None:\n+        for chunk in iterator:\n+            yield chunk\n         return\n \n-    decoder = codecs.getincrementaldecoder(r.encoding)(errors='replace')\n+    # Create decoder if we haven't already\n+    if 'decoder' not in locals():\n+        decoder = codecs.getincrementaldecoder(encoding)(errors='replace')\n+    \n+    # Process remaining chunks\n     for chunk in iterator:\n-        rv = decoder.decode(chunk)\n+        try:\n+            rv = decoder.decode(chunk)\n+            if rv:\n+                yield rv\n+        except (UnicodeDecodeError, LookupError):\n+            # Fall back to chunked replace on decode errors\n+            rv = chunk.decode(encoding, 'replace')\n+            if rv:\n+                yield rv\n+    \n+    # Handle final bytes\n+    try:\n+        rv = decoder.decode(b'', final=True)\n         if rv:\n             yield rv\n-    rv = decoder.decode(b'', final=True)\n-    if rv:\n-        yield rv\n+    except (UnicodeDecodeError, LookupError):\n+        pass\n \n \n def iter_slices(string, slice_length):\n@@ -738,4 +783,4 @@ def urldefragauth(url):\n \n     netloc = netloc.rsplit('@', 1)[-1]\n \n-    return urlunparse((scheme, netloc, path, params, query, ''))\n+    return urlunparse((scheme, netloc, path, params, query, ''))\n\\ No newline at end of file\n",
  "model_name_or_path": "KortixAI"
}