{
  "instance_id": "scikit-learn__scikit-learn-13241",
  "model_patch": "diff --git a/sklearn/decomposition/kernel_pca.py b/sklearn/decomposition/kernel_pca.py\nindex bb91d0cbb..9a7a35078 100644\n--- a/sklearn/decomposition/kernel_pca.py\n+++ b/sklearn/decomposition/kernel_pca.py\n@@ -20,6 +20,13 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n \n     Non-linear dimensionality reduction through the use of kernels (see\n     :ref:`metrics`).\n+    \n+    Note: The sign of components from transform or fit_transform is stabilized \n+    to ensure more consistent output across multiple runs. This is done by fixing\n+    the sign based on the largest absolute component. However, the absolute value\n+    of the components is what carries the meaningful information, and sign \n+    differences between runs do not affect the quality of the dimensionality\n+    reduction.\n \n     Read more in the :ref:`User Guide <kernel_PCA>`.\n \n@@ -162,6 +169,7 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         self.random_state = random_state\n         self.n_jobs = n_jobs\n         self.copy_X = copy_X\n+        self.eigenvalue_signs_ = None\n \n     @property\n     def _pairwise(self):\n@@ -277,6 +285,14 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n \n         X_transformed = self.alphas_ * np.sqrt(self.lambdas_)\n \n+        # Store eigenvalue signs during fit\n+        if self.eigenvalue_signs_ is None:\n+            # Use signs of largest magnitude components\n+            self.eigenvalue_signs_ = np.sign(self.alphas_[np.argmax(np.abs(self.alphas_), axis=0), range(self.alphas_.shape[1])])\n+            \n+        # Apply consistent signs based on eigenvalues\n+        X_transformed *= self.eigenvalue_signs_\n+\n         if self.fit_inverse_transform:\n             self._fit_inverse_transform(X_transformed, X)\n \n@@ -296,7 +312,10 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         check_is_fitted(self, 'X_fit_')\n \n         K = self._centerer.transform(self._get_kernel(X, self.X_fit_))\n-        return np.dot(K, self.alphas_ / np.sqrt(self.lambdas_))\n+        X_transformed = np.dot(K, self.alphas_ / np.sqrt(self.lambdas_))\n+        if self.eigenvalue_signs_ is not None:\n+            X_transformed *= self.eigenvalue_signs_\n+        return X_transformed\n \n     def inverse_transform(self, X):\n         \"\"\"Transform X back to original space.\n@@ -320,4 +339,4 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n \n         K = self._get_kernel(X, self.X_transformed_fit_)\n \n-        return np.dot(K, self.dual_coef_)\n+        return np.dot(K, self.dual_coef_)\n\\ No newline at end of file\n",
  "model_name_or_path": "KortixAI"
}