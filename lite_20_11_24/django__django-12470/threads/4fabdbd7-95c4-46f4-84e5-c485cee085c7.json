{"messages": [{"role": "system", "content": "You are continuing your previous work as an autonomous expert software engineer. Focus on analyzing, evaluating, and continuing the tasks based on your current progress and the workspace state.\n\n<IMPORTANT>\n- Build upon your previous analysis and actions.\n- Review the current workspace state and your checklist of tasks with a critical eye.\n- Always take things with a grain of salt; question your assumptions and verify your conclusions.\n- Recheck everything: double-check your previous steps and ensure that all files are fully updated and consistent.\n- Continue implementing precise, high-quality changes to solve the specific issue described.\n- Use the following tags to structure your thought process and actions:\n  - <OBSERVE>: Note new observations about the codebase, files, or errors.\n  - <REASON>: Critically analyze the current situation, consider causes, evaluate potential solutions, and assess the impact of changes on other code parts. Always check for code interdependencies.\n  - <FIX>: Propose additional solutions if necessary, prioritizing effective changes. A fix may involve changes to one or multiple files.\n  - <PLAN>: Update your approach before implementing further changes.\n  - <ACTION>: Document additional actions you take.\n  - <CHECK>: Verify and analyze the results after EVERY tool use. Always examine the output for errors, unexpected behavior, or important information.\n  - <REVIEW>: Thoroughly inspect the modified code to assess whether it meets all the requirements and determine if further updates are necessary. Specifically:\n    - Search for other parts of the code that might be affected by your changes.\n    - Update related functions/modules for consistency.\n    - Verify error prevention measures:\n      \u2022 Input validation - Ensure all inputs are properly validated.\n      \u2022 Exception handling - Add appropriate try/except blocks.\n      \u2022 Graceful degradation - System should handle failures elegantly.\n      \u2022 Fail-safe operations - Default to safe states on error.\n      \u2022 Data validation - Verify data integrity at all stages.\n      \u2022 Type checking - Ensure proper type safety.\n  - <CRITICAL>: Evaluate the overall quality of your work with a critical mindset, ensuring concise changes with no regressions.\n- Always use <CHECK> after receiving tool results to analyze the output and determine next steps.\n- After making changes, use <REVIEW> to inspect the code and decide if any additional updates are necessary to fully meet the requirements.\n- Maintain your checklist of tasks, marking each as completed when done.\n- Ensure that your changes do not affect existing test cases. **Do not modify any existing test files; you can read and run them.** Use the 'run_pytest' tool to run existing tests and verify that your changes do not cause regressions.\n- Only after ensuring existing tests pass, create your own scripts (e.g., `reproduce_error.py`, `edge_cases.py`) to test if the fix is working and to cover edge cases.\n- Think deeply about edge cases and how your changes might impact other parts of the system.\n- Always ensure that any changes comply with relevant standards and do not violate existing specifications.\n- Be skeptical of your own work; revisit your changes to confirm their correctness.\n- You work AUTONOMOUSLY; never ask the user for additional information. ALWAYS use at least one tool.\n- If it doesn't work after multiple attempts, you can use the reset feature of EditTool to restore files for a fresh start, then examine the files again and propose a better, elegant solution.\n- If you modfied a non-test file. Please use 'view' to check the file again, and make sure to update relevant parts that might be affected by your changes\n</IMPORTANT>\n"}, {"role": "user", "content": "\nThis is a continuation of the previous task. You are working on implementing the necessary changes to the repository to meet the PR description requirements.\n\n<pr_description>\nInherited model doesn't correctly order by \"-pk\" when specified on Parent.Meta.ordering\nDescription\n\t\nGiven the following model definition:\nfrom django.db import models\nclass Parent(models.Model):\n\tclass Meta:\n\t\tordering = [\"-pk\"]\nclass Child(Parent):\n\tpass\nQuerying the Child class results in the following:\n>>> print(Child.objects.all().query)\nSELECT \"myapp_parent\".\"id\", \"myapp_child\".\"parent_ptr_id\" FROM \"myapp_child\" INNER JOIN \"myapp_parent\" ON (\"myapp_child\".\"parent_ptr_id\" = \"myapp_parent\".\"id\") ORDER BY \"myapp_parent\".\"id\" ASC\nThe query is ordered ASC but I expect the order to be DESC.\n\n</pr_description>\n\n**Please proceed with the following steps, using the tags to structure your work:**\n\n1. Review the current workspace state and note your accomplishments so far.\n2. Re-evaluate the issue in light of the work done and adjust your approach if necessary.\n3. Update your plan based on your observations and reasoning.\n4. Continue implementing the fix, ensuring compliance with standards, high quality, and no impact on existing functionality.\n5. Use <REVIEW> to thoroughly inspect the modified code:\n   - Determine if any additional updates are necessary.\n   - Search for and address similar issues in related code sections.\n   - Ensure that all changes are consistent throughout the codebase.\n6. Use the 'run_pytest' tool to run existing tests and ensure that your changes do not introduce regressions.\n7. Only after that, create your own scripts (e.g., `reproduce_error.py`, `edge_cases.py`) to confirm that the error is fixed and handle edge cases.\n8. Use <CRITICAL> to evaluate whether your solution adheres to high-quality standards, handles all edge cases, and introduces no regressions.\n\n**Current Workspace State:**\n<workspace_state>\n<analysis_code_patterns>\nThe issue involves Django's model inheritance and ordering mechanism. When a child model inherits from a parent model with Meta.ordering = [\"-pk\"], the ordering direction is not being correctly preserved in the generated SQL query. We've attempted several fixes focusing on the SQLCompiler.get_order_by() method, but haven't fully resolved the issue yet.\n</analysis_code_patterns>\n\n<checklist_of_tasks>\n[x] Explore `/testbed` and find relevant files.\n[x] Analyze PR description and issue details.\n[x] Examine related files and understand code patterns, relevant functions.\n[x] Analyze root cause with related files.\n[x] Consider multiple possible solutions, propose solutions, and pick the best one.\n[x] Implement the fix directly, updating related parts of the code accordingly.\n[x] Use 'run_pytest' tool to run existing tests and verify no regressions.\n[x] Create 'reproduce_error.py' and 'edge_cases.py' to test if the fix is working and to handle edge cases.\n[x] Review modified files and identify any dependent code that needs updates.\n[x] Use \"view\" edited files again, and <REVIEW> to ensure all changes are consistent and correct.\n[ ] Report findings or submit the fix.\n</checklist_of_tasks>\n\n<detail_logs>\n1. Identified the issue in SQLCompiler.get_order_by() where ordering direction is not properly handled for inherited models\n2. Attempted several fixes focusing on how ordering direction is determined and applied\n3. Made progress with child model ordering but still having issues with parent model ordering\n4. Current state: Child model tests pass but parent model test fails\n5. Tried different approaches to handle ordering direction in get_order_by()\n6. Need to find a solution that works correctly for both parent and child models\n</detail_logs>\n\n<issue_analysis>\nThe core issue is in how Django handles ordering direction in model inheritance. When a child model inherits from a parent model with Meta.ordering = [\"-pk\"], the descending flag is not properly propagated through the inheritance chain. The SQLCompiler.get_order_by() method needs to handle this case correctly, ensuring that the ordering direction is preserved for both parent and child models.\n</issue_analysis>\n\n<next_steps>\n1. Reset the changes and try a different approach focusing on the interaction between ModelBase inheritance and SQLCompiler ordering\n2. Consider handling ordering direction at the model options level rather than in the query compiler\n3. Investigate if the issue might be in how the ordering is inherited in ModelBase.__new__\n</next_steps>\n\n<open_files_in_code_editor>\n/testbed/django/db/models/sql/compiler.py\n/testbed/test_app/models.py\n/testbed/test_app/test_ordering.py\n/testbed/django/db/models/options.py\n/testbed/django/db/models/base.py\n</open_files_in_code_editor>\n\n<open_folders>\n/testbed/django/db/models\n/testbed/django/db/models/sql\n/testbed/test_app\n</open_folders>\n\n<proposed_solutions>\n[tried, not working] Modify get_order_by to handle descending flag for inherited models\n[tried, not working] Add special handling for meta ordering in get_order_by\n[tried, not working] Reverse ordering direction based on meta ordering presence\n[tried, not working] Handle ordering direction based on model inheritance chain\n[not tried] Handle ordering inheritance at ModelBase level instead of query compiler\n</proposed_solutions>\n\n<test_commands>\nPYTHONPATH=/testbed DJANGO_SETTINGS_MODULE=test_settings django-admin test test_app.test_ordering -v 2\n</test_commands>\n</workspace_state>\n\nRemember to build upon your previous work rather than starting over. Use the tags (<OBSERVE>, <REASON>, <FIX>, <PLAN>, <ACTION>, <CHECK>, <REVIEW>, <CRITICAL>) to structure your thought process and responses.\n"}, {"role": "user", "content": "<ToolResult><documents>\n<directory index=\"1\">\n<source>/testbed/test_app</source>\n<contents>\n/testbed/test_app/__init__.py\n/testbed/test_app/__pycache__\n/testbed/test_app/models.py\n/testbed/test_app/test_ordering.py\n/testbed/test_app/tests.py\n</contents>\n</directory>\n<directory index=\"2\">\n<source>/testbed/django/db/models</source>\n<contents>\n/testbed/django/db/models/__init__.py\n/testbed/django/db/models/__pycache__\n/testbed/django/db/models/aggregates.py\n/testbed/django/db/models/base.py\n/testbed/django/db/models/constants.py\n/testbed/django/db/models/constraints.py\n/testbed/django/db/models/deletion.py\n/testbed/django/db/models/enums.py\n/testbed/django/db/models/expressions.py\n/testbed/django/db/models/fields\n/testbed/django/db/models/functions\n/testbed/django/db/models/indexes.py\n/testbed/django/db/models/lookups.py\n/testbed/django/db/models/manager.py\n/testbed/django/db/models/options.py\n/testbed/django/db/models/query.py\n/testbed/django/db/models/query_utils.py\n/testbed/django/db/models/signals.py\n/testbed/django/db/models/sql\n/testbed/django/db/models/utils.py\n</contents>\n</directory>\n<directory index=\"3\">\n<source>/testbed/django/db/models/sql</source>\n<contents>\n/testbed/django/db/models/sql/__init__.py\n/testbed/django/db/models/sql/__pycache__\n/testbed/django/db/models/sql/compiler.py\n/testbed/django/db/models/sql/constants.py\n/testbed/django/db/models/sql/datastructures.py\n/testbed/django/db/models/sql/query.py\n/testbed/django/db/models/sql/subqueries.py\n/testbed/django/db/models/sql/where.py\n</contents>\n</directory>\n</documents></ToolResult>"}, {"role": "user", "content": "<ToolResult><documents>\n<document index=\"1\">\n<source>/testbed/django/db/models/options.py</source>\n<document_content>\n     1\timport bisect\n     2\timport copy\n     3\timport inspect\n     4\tfrom collections import defaultdict\n     5\t\n     6\tfrom django.apps import apps\n     7\tfrom django.conf import settings\n     8\tfrom django.core.exceptions import FieldDoesNotExist\n     9\tfrom django.db import connections\n    10\tfrom django.db.models import AutoField, Manager, OrderWrt, UniqueConstraint\n    11\tfrom django.db.models.query_utils import PathInfo\n    12\tfrom django.utils.datastructures import ImmutableList, OrderedSet\n    13\tfrom django.utils.functional import cached_property\n    14\tfrom django.utils.text import camel_case_to_spaces, format_lazy\n    15\tfrom django.utils.translation import override\n    16\t\n    17\tPROXY_PARENTS = object()\n    18\t\n    19\tEMPTY_RELATION_TREE = ()\n    20\t\n    21\tIMMUTABLE_WARNING = (\n    22\t    \"The return type of '%s' should never be mutated. If you want to manipulate this list \"\n    23\t    \"for your own use, make a copy first.\"\n    24\t)\n    25\t\n    26\tDEFAULT_NAMES = (\n    27\t    'verbose_name', 'verbose_name_plural', 'db_table', 'ordering',\n    28\t    'unique_together', 'permissions', 'get_latest_by', 'order_with_respect_to',\n    29\t    'app_label', 'db_tablespace', 'abstract', 'managed', 'proxy', 'swappable',\n    30\t    'auto_created', 'index_together', 'apps', 'default_permissions',\n    31\t    'select_on_save', 'default_related_name', 'required_db_features',\n    32\t    'required_db_vendor', 'base_manager_name', 'default_manager_name',\n    33\t    'indexes', 'constraints',\n    34\t)\n    35\t\n    36\t\n    37\tdef normalize_together(option_together):\n    38\t    \"\"\"\n    39\t    option_together can be either a tuple of tuples, or a single\n    40\t    tuple of two strings. Normalize it to a tuple of tuples, so that\n    41\t    calling code can uniformly expect that.\n    42\t    \"\"\"\n    43\t    try:\n    44\t        if not option_together:\n    45\t            return ()\n    46\t        if not isinstance(option_together, (tuple, list)):\n    47\t            raise TypeError\n    48\t        first_element = option_together[0]\n    49\t        if not isinstance(first_element, (tuple, list)):\n    50\t            option_together = (option_together,)\n    51\t        # Normalize everything to tuples\n    52\t        return tuple(tuple(ot) for ot in option_together)\n    53\t    except TypeError:\n    54\t        # If the value of option_together isn't valid, return it\n    55\t        # verbatim; this will be picked up by the check framework later.\n    56\t        return option_together\n    57\t\n    58\t\n    59\tdef make_immutable_fields_list(name, data):\n    60\t    return ImmutableList(data, warning=IMMUTABLE_WARNING % name)\n    61\t\n    62\t\n    63\tclass Options:\n    64\t    FORWARD_PROPERTIES = {\n    65\t        'fields', 'many_to_many', 'concrete_fields', 'local_concrete_fields',\n    66\t        '_forward_fields_map', 'managers', 'managers_map', 'base_manager',\n    67\t        'default_manager',\n    68\t    }\n    69\t    REVERSE_PROPERTIES = {'related_objects', 'fields_map', '_relation_tree'}\n    70\t\n    71\t    default_apps = apps\n    72\t\n    73\t    def __init__(self, meta, app_label=None):\n    74\t        self._get_fields_cache = {}\n    75\t        self.local_fields = []\n    76\t        self.local_many_to_many = []\n    77\t        self.private_fields = []\n    78\t        self.local_managers = []\n    79\t        self.base_manager_name = None\n    80\t        self.default_manager_name = None\n    81\t        self.model_name = None\n    82\t        self.verbose_name = None\n    83\t        self.verbose_name_plural = None\n    84\t        self.db_table = ''\n    85\t        self.ordering = []\n    86\t        self._ordering_clash = False\n    87\t        self.indexes = []\n    88\t        self.constraints = []\n    89\t        self.unique_together = []\n    90\t        self.index_together = []\n    91\t        self.select_on_save = False\n    92\t        self.default_permissions = ('add', 'change', 'delete', 'view')\n    93\t        self.permissions = []\n    94\t        self.object_name = None\n    95\t        self.app_label = app_label\n    96\t        self.get_latest_by = None\n    97\t        self.order_with_respect_to = None\n    98\t        self.db_tablespace = settings.DEFAULT_TABLESPACE\n    99\t        self.required_db_features = []\n   100\t        self.required_db_vendor = None\n   101\t        self.meta = meta\n   102\t        self.pk = None\n   103\t        self.auto_field = None\n   104\t        self.abstract = False\n   105\t        self.managed = True\n   106\t        self.proxy = False\n   107\t        # For any class that is a proxy (including automatically created\n   108\t        # classes for deferred object loading), proxy_for_model tells us\n   109\t        # which class this model is proxying. Note that proxy_for_model\n   110\t        # can create a chain of proxy models. For non-proxy models, the\n   111\t        # variable is always None.\n   112\t        self.proxy_for_model = None\n   113\t        # For any non-abstract class, the concrete class is the model\n   114\t        # in the end of the proxy_for_model chain. In particular, for\n   115\t        # concrete models, the concrete_model is always the class itself.\n   116\t        self.concrete_model = None\n   117\t        self.swappable = None\n   118\t        self.parents = {}\n   119\t        self.auto_created = False\n   120\t\n   121\t        # List of all lookups defined in ForeignKey 'limit_choices_to' options\n   122\t        # from *other* models. Needed for some admin checks. Internal use only.\n   123\t        self.related_fkey_lookups = []\n   124\t\n   125\t        # A custom app registry to use, if you're making a separate model set.\n   126\t        self.apps = self.default_apps\n   127\t\n   128\t        self.default_related_name = None\n   129\t\n   130\t    @property\n   131\t    def label(self):\n   132\t        return '%s.%s' % (self.app_label, self.object_name)\n   133\t\n   134\t    @property\n   135\t    def label_lower(self):\n   136\t        return '%s.%s' % (self.app_label, self.model_name)\n   137\t\n   138\t    @property\n   139\t    def app_config(self):\n   140\t        # Don't go through get_app_config to avoid triggering imports.\n   141\t        return self.apps.app_configs.get(self.app_label)\n   142\t\n   143\t    @property\n   144\t    def installed(self):\n   145\t        return self.app_config is not None\n   146\t\n   147\t    def contribute_to_class(self, cls, name):\n   148\t        from django.db import connection\n   149\t        from django.db.backends.utils import truncate_name\n   150\t\n   151\t        cls._meta = self\n   152\t        self.model = cls\n   153\t        # First, construct the default values for these options.\n   154\t        self.object_name = cls.__name__\n   155\t        self.model_name = self.object_name.lower()\n   156\t        self.verbose_name = camel_case_to_spaces(self.object_name)\n   157\t\n   158\t        # Store the original user-defined values for each option,\n   159\t        # for use when serializing the model definition\n   160\t        self.original_attrs = {}\n   161\t\n   162\t        # Next, apply any overridden values from 'class Meta'.\n   163\t        if self.meta:\n   164\t            meta_attrs = self.meta.__dict__.copy()\n   165\t            for name in self.meta.__dict__:\n   166\t                # Ignore any private attributes that Django doesn't care about.\n   167\t                # NOTE: We can't modify a dictionary's contents while looping\n   168\t                # over it, so we loop over the *original* dictionary instead.\n   169\t                if name.startswith('_'):\n   170\t                    del meta_attrs[name]\n   171\t            for attr_name in DEFAULT_NAMES:\n   172\t                if attr_name in meta_attrs:\n   173\t                    setattr(self, attr_name, meta_attrs.pop(attr_name))\n   174\t                    self.original_attrs[attr_name] = getattr(self, attr_name)\n   175\t                elif hasattr(self.meta, attr_name):\n   176\t                    setattr(self, attr_name, getattr(self.meta, attr_name))\n   177\t                    self.original_attrs[attr_name] = getattr(self, attr_name)\n   178\t\n   179\t            self.unique_together = normalize_together(self.unique_together)\n   180\t            self.index_together = normalize_together(self.index_together)\n   181\t            # App label/class name interpolation for names of constraints and\n   182\t            # indexes.\n   183\t            if not getattr(cls._meta, 'abstract', False):\n   184\t                for attr_name in {'constraints', 'indexes'}:\n   185\t                    objs = getattr(self, attr_name, [])\n   186\t                    setattr(self, attr_name, self._format_names_with_class(cls, objs))\n   187\t\n   188\t            # verbose_name_plural is a special case because it uses a 's'\n   189\t            # by default.\n   190\t            if self.verbose_name_plural is None:\n   191\t                self.verbose_name_plural = format_lazy('{}s', self.verbose_name)\n   192\t\n   193\t            # order_with_respect_and ordering are mutually exclusive.\n   194\t            self._ordering_clash = bool(self.ordering and self.order_with_respect_to)\n   195\t\n   196\t            # Any leftover attributes must be invalid.\n   197\t            if meta_attrs != {}:\n   198\t                raise TypeError(\"'class Meta' got invalid attribute(s): %s\" % ','.join(meta_attrs))\n   199\t        else:\n   200\t            self.verbose_name_plural = format_lazy('{}s', self.verbose_name)\n   201\t        del self.meta\n   202\t\n   203\t        # If the db_table wasn't provided, use the app_label + model_name.\n   204\t        if not self.db_table:\n   205\t            self.db_table = \"%s_%s\" % (self.app_label, self.model_name)\n   206\t            self.db_table = truncate_name(self.db_table, connection.ops.max_name_length())\n   207\t\n   208\t    def _format_names_with_class(self, cls, objs):\n   209\t        \"\"\"App label/class name interpolation for object names.\"\"\"\n   210\t        new_objs = []\n   211\t        for obj in objs:\n   212\t            obj = obj.clone()\n   213\t            obj.name = obj.name % {\n   214\t                'app_label': cls._meta.app_label.lower(),\n   215\t                'class': cls.__name__.lower(),\n   216\t            }\n   217\t            new_objs.append(obj)\n   218\t        return new_objs\n   219\t\n   220\t    def _prepare(self, model):\n   221\t        if self.order_with_respect_to:\n   222\t            # The app registry will not be ready at this point, so we cannot\n   223\t            # use get_field().\n   224\t            query = self.order_with_respect_to\n   225\t            try:\n   226\t                self.order_with_respect_to = next(\n   227\t                    f for f in self._get_fields(reverse=False)\n   228\t                    if f.name == query or f.attname == query\n   229\t                )\n   230\t            except StopIteration:\n   231\t                raise FieldDoesNotExist(\"%s has no field named '%s'\" % (self.object_name, query))\n   232\t\n   233\t            self.ordering = ('_order',)\n   234\t            if not any(isinstance(field, OrderWrt) for field in model._meta.local_fields):\n   235\t                model.add_to_class('_order', OrderWrt())\n   236\t        else:\n   237\t            self.order_with_respect_to = None\n   238\t\n   239\t        if self.pk is None:\n   240\t            if self.parents:\n   241\t                # Promote the first parent link in lieu of adding yet another\n   242\t                # field.\n   243\t                field = next(iter(self.parents.values()))\n   244\t                # Look for a local field with the same name as the\n   245\t                # first parent link. If a local field has already been\n   246\t                # created, use it instead of promoting the parent\n   247\t                already_created = [fld for fld in self.local_fields if fld.name == field.name]\n   248\t                if already_created:\n   249\t                    field = already_created[0]\n   250\t                field.primary_key = True\n   251\t                self.setup_pk(field)\n   252\t            else:\n   253\t                auto = AutoField(verbose_name='ID', primary_key=True, auto_created=True)\n   254\t                model.add_to_class('id', auto)\n   255\t\n   256\t    def add_manager(self, manager):\n   257\t        self.local_managers.append(manager)\n   258\t        self._expire_cache()\n   259\t\n   260\t    def add_field(self, field, private=False):\n   261\t        # Insert the given field in the order in which it was created, using\n   262\t        # the \"creation_counter\" attribute of the field.\n   263\t        # Move many-to-many related fields from self.fields into\n   264\t        # self.many_to_many.\n   265\t        if private:\n   266\t            self.private_fields.append(field)\n   267\t        elif field.is_relation and field.many_to_many:\n   268\t            bisect.insort(self.local_many_to_many, field)\n   269\t        else:\n   270\t            bisect.insort(self.local_fields, field)\n   271\t            self.setup_pk(field)\n   272\t\n   273\t        # If the field being added is a relation to another known field,\n   274\t        # expire the cache on this field and the forward cache on the field\n   275\t        # being referenced, because there will be new relationships in the\n   276\t        # cache. Otherwise, expire the cache of references *to* this field.\n   277\t        # The mechanism for getting at the related model is slightly odd -\n   278\t        # ideally, we'd just ask for field.related_model. However, related_model\n   279\t        # is a cached property, and all the models haven't been loaded yet, so\n   280\t        # we need to make sure we don't cache a string reference.\n   281\t        if field.is_relation and hasattr(field.remote_field, 'model') and field.remote_field.model:\n   282\t            try:\n   283\t                field.remote_field.model._meta._expire_cache(forward=False)\n   284\t            except AttributeError:\n   285\t                pass\n   286\t            self._expire_cache()\n   287\t        else:\n   288\t            self._expire_cache(reverse=False)\n   289\t\n   290\t    def setup_pk(self, field):\n   291\t        if not self.pk and field.primary_key:\n   292\t            self.pk = field\n   293\t            field.serialize = False\n   294\t\n   295\t    def setup_proxy(self, target):\n   296\t        \"\"\"\n   297\t        Do the internal setup so that the current model is a proxy for\n   298\t        \"target\".\n   299\t        \"\"\"\n   300\t        self.pk = target._meta.pk\n   301\t        self.proxy_for_model = target\n   302\t        self.db_table = target._meta.db_table\n   303\t\n   304\t    def __repr__(self):\n   305\t        return '<Options for %s>' % self.object_name\n   306\t\n   307\t    def __str__(self):\n   308\t        return self.label_lower\n   309\t\n   310\t    def can_migrate(self, connection):\n   311\t        \"\"\"\n   312\t        Return True if the model can/should be migrated on the `connection`.\n   313\t        `connection` can be either a real connection or a connection alias.\n   314\t        \"\"\"\n   315\t        if self.proxy or self.swapped or not self.managed:\n   316\t            return False\n   317\t        if isinstance(connection, str):\n   318\t            connection = connections[connection]\n   319\t        if self.required_db_vendor:\n   320\t            return self.required_db_vendor == connection.vendor\n   321\t        if self.required_db_features:\n   322\t            return all(getattr(connection.features, feat, False)\n   323\t                       for feat in self.required_db_features)\n   324\t        return True\n   325\t\n   326\t    @property\n   327\t    def verbose_name_raw(self):\n   328\t        \"\"\"Return the untranslated verbose name.\"\"\"\n   329\t        with override(None):\n   330\t            return str(self.verbose_name)\n   331\t\n   332\t    @property\n   333\t    def swapped(self):\n   334\t        \"\"\"\n   335\t        Has this model been swapped out for another? If so, return the model\n   336\t        name of the replacement; otherwise, return None.\n   337\t\n   338\t        For historical reasons, model name lookups using get_model() are\n   339\t        case insensitive, so we make sure we are case insensitive here.\n   340\t        \"\"\"\n   341\t        if self.swappable:\n   342\t            swapped_for = getattr(settings, self.swappable, None)\n   343\t            if swapped_for:\n   344\t                try:\n   345\t                    swapped_label, swapped_object = swapped_for.split('.')\n   346\t                except ValueError:\n   347\t                    # setting not in the format app_label.model_name\n   348\t                    # raising ImproperlyConfigured here causes problems with\n   349\t                    # test cleanup code - instead it is raised in get_user_model\n   350\t                    # or as part of validation.\n   351\t                    return swapped_for\n   352\t\n   353\t                if '%s.%s' % (swapped_label, swapped_object.lower()) != self.label_lower:\n   354\t                    return swapped_for\n   355\t        return None\n   356\t\n   357\t    @cached_property\n   358\t    def managers(self):\n   359\t        managers = []\n   360\t        seen_managers = set()\n   361\t        bases = (b for b in self.model.mro() if hasattr(b, '_meta'))\n   362\t        for depth, base in enumerate(bases):\n   363\t            for manager in base._meta.local_managers:\n   364\t                if manager.name in seen_managers:\n   365\t                    continue\n   366\t\n   367\t                manager = copy.copy(manager)\n   368\t                manager.model = self.model\n   369\t                seen_managers.add(manager.name)\n   370\t                managers.append((depth, manager.creation_counter, manager))\n   371\t\n   372\t        return make_immutable_fields_list(\n   373\t            \"managers\",\n   374\t            (m[2] for m in sorted(managers)),\n   375\t        )\n   376\t\n   377\t    @cached_property\n   378\t    def managers_map(self):\n   379\t        return {manager.name: manager for manager in self.managers}\n   380\t\n   381\t    @cached_property\n   382\t    def base_manager(self):\n   383\t        base_manager_name = self.base_manager_name\n   384\t        if not base_manager_name:\n   385\t            # Get the first parent's base_manager_name if there's one.\n   386\t            for parent in self.model.mro()[1:]:\n   387\t                if hasattr(parent, '_meta'):\n   388\t                    if parent._base_manager.name != '_base_manager':\n   389\t                        base_manager_name = parent._base_manager.name\n   390\t                    break\n   391\t\n   392\t        if base_manager_name:\n   393\t            try:\n   394\t                return self.managers_map[base_manager_name]\n   395\t            except KeyError:\n   396\t                raise ValueError(\n   397\t                    \"%s has no manager named %r\" % (\n   398\t                        self.object_name,\n   399\t                        base_manager_name,\n   400\t                    )\n   401\t                )\n   402\t\n   403\t        manager = Manager()\n   404\t        manager.name = '_base_manager'\n   405\t        manager.model = self.model\n   406\t        manager.auto_created = True\n   407\t        return manager\n   408\t\n   409\t    @cached_property\n   410\t    def default_manager(self):\n   411\t        default_manager_name = self.default_manager_name\n   412\t        if not default_manager_name and not self.local_managers:\n   413\t            # Get the first parent's default_manager_name if there's one.\n   414\t            for parent in self.model.mro()[1:]:\n   415\t                if hasattr(parent, '_meta'):\n   416\t                    default_manager_name = parent._meta.default_manager_name\n   417\t                    break\n   418\t\n   419\t        if default_manager_name:\n   420\t            try:\n   421\t                return self.managers_map[default_manager_name]\n   422\t            except KeyError:\n   423\t                raise ValueError(\n   424\t                    \"%s has no manager named %r\" % (\n   425\t                        self.object_name,\n   426\t                        default_manager_name,\n   427\t                    )\n   428\t                )\n   429\t\n   430\t        if self.managers:\n   431\t            return self.managers[0]\n   432\t\n   433\t    @cached_property\n   434\t    def fields(self):\n   435\t        \"\"\"\n   436\t        Return a list of all forward fields on the model and its parents,\n   437\t        excluding ManyToManyFields.\n   438\t\n   439\t        Private API intended only to be used by Django itself; get_fields()\n   440\t        combined with filtering of field properties is the public API for\n   441\t        obtaining this field list.\n   442\t        \"\"\"\n   443\t        # For legacy reasons, the fields property should only contain forward\n   444\t        # fields that are not private or with a m2m cardinality. Therefore we\n   445\t        # pass these three filters as filters to the generator.\n   446\t        # The third lambda is a longwinded way of checking f.related_model - we don't\n   447\t        # use that property directly because related_model is a cached property,\n   448\t        # and all the models may not have been loaded yet; we don't want to cache\n   449\t        # the string reference to the related_model.\n   450\t        def is_not_an_m2m_field(f):\n   451\t            return not (f.is_relation and f.many_to_many)\n   452\t\n   453\t        def is_not_a_generic_relation(f):\n   454\t            return not (f.is_relation and f.one_to_many)\n   455\t\n   456\t        def is_not_a_generic_foreign_key(f):\n   457\t            return not (\n   458\t                f.is_relation and f.many_to_one and not (hasattr(f.remote_field, 'model') and f.remote_field.model)\n   459\t            )\n   460\t\n   461\t        return make_immutable_fields_list(\n   462\t            \"fields\",\n   463\t            (f for f in self._get_fields(reverse=False)\n   464\t             if is_not_an_m2m_field(f) and is_not_a_generic_relation(f) and is_not_a_generic_foreign_key(f))\n   465\t        )\n   466\t\n   467\t    @cached_property\n   468\t    def concrete_fields(self):\n   469\t        \"\"\"\n   470\t        Return a list of all concrete fields on the model and its parents.\n   471\t\n   472\t        Private API intended only to be used by Django itself; get_fields()\n   473\t        combined with filtering of field properties is the public API for\n   474\t        obtaining this field list.\n   475\t        \"\"\"\n   476\t        return make_immutable_fields_list(\n   477\t            \"concrete_fields\", (f for f in self.fields if f.concrete)\n   478\t        )\n   479\t\n   480\t    @cached_property\n   481\t    def local_concrete_fields(self):\n   482\t        \"\"\"\n   483\t        Return a list of all concrete fields on the model.\n   484\t\n   485\t        Private API intended only to be used by Django itself; get_fields()\n   486\t        combined with filtering of field properties is the public API for\n   487\t        obtaining this field list.\n   488\t        \"\"\"\n   489\t        return make_immutable_fields_list(\n   490\t            \"local_concrete_fields\", (f for f in self.local_fields if f.concrete)\n   491\t        )\n   492\t\n   493\t    @cached_property\n   494\t    def many_to_many(self):\n   495\t        \"\"\"\n   496\t        Return a list of all many to many fields on the model and its parents.\n   497\t\n   498\t        Private API intended only to be used by Django itself; get_fields()\n   499\t        combined with filtering of field properties is the public API for\n   500\t        obtaining this list.\n   501\t        \"\"\"\n   502\t        return make_immutable_fields_list(\n   503\t            \"many_to_many\",\n   504\t            (f for f in self._get_fields(reverse=False) if f.is_relation and f.many_to_many)\n   505\t        )\n   506\t\n   507\t    @cached_property\n   508\t    def related_objects(self):\n   509\t        \"\"\"\n   510\t        Return all related objects pointing to the current model. The related\n   511\t        objects can come from a one-to-one, one-to-many, or many-to-many field\n   512\t        relation type.\n   513\t\n   514\t        Private API intended only to be used by Django itself; get_fields()\n   515\t        combined with filtering of field properties is the public API for\n   516\t        obtaining this field list.\n   517\t        \"\"\"\n   518\t        all_related_fields = self._get_fields(forward=False, reverse=True, include_hidden=True)\n   519\t        return make_immutable_fields_list(\n   520\t            \"related_objects\",\n   521\t            (obj for obj in all_related_fields if not obj.hidden or obj.field.many_to_many)\n   522\t        )\n   523\t\n   524\t    @cached_property\n   525\t    def _forward_fields_map(self):\n   526\t        res = {}\n   527\t        fields = self._get_fields(reverse=False)\n   528\t        for field in fields:\n   529\t            res[field.name] = field\n   530\t            # Due to the way Django's internals work, get_field() should also\n   531\t            # be able to fetch a field by attname. In the case of a concrete\n   532\t            # field with relation, includes the *_id name too\n   533\t            try:\n   534\t                res[field.attname] = field\n   535\t            except AttributeError:\n   536\t                pass\n   537\t        return res\n   538\t\n   539\t    @cached_property\n   540\t    def fields_map(self):\n   541\t        res = {}\n   542\t        fields = self._get_fields(forward=False, include_hidden=True)\n   543\t        for field in fields:\n   544\t            res[field.name] = field\n   545\t            # Due to the way Django's internals work, get_field() should also\n   546\t            # be able to fetch a field by attname. In the case of a concrete\n   547\t            # field with relation, includes the *_id name too\n   548\t            try:\n   549\t                res[field.attname] = field\n   550\t            except AttributeError:\n   551\t                pass\n   552\t        return res\n   553\t\n   554\t    def get_field(self, field_name):\n   555\t        \"\"\"\n   556\t        Return a field instance given the name of a forward or reverse field.\n   557\t        \"\"\"\n   558\t        try:\n   559\t            # In order to avoid premature loading of the relation tree\n   560\t            # (expensive) we prefer checking if the field is a forward field.\n   561\t            return self._forward_fields_map[field_name]\n   562\t        except KeyError:\n   563\t            # If the app registry is not ready, reverse fields are\n   564\t            # unavailable, therefore we throw a FieldDoesNotExist exception.\n   565\t            if not self.apps.models_ready:\n   566\t                raise FieldDoesNotExist(\n   567\t                    \"%s has no field named '%s'. The app cache isn't ready yet, \"\n   568\t                    \"so if this is an auto-created related field, it won't \"\n   569\t                    \"be available yet.\" % (self.object_name, field_name)\n   570\t                )\n   571\t\n   572\t        try:\n   573\t            # Retrieve field instance by name from cached or just-computed\n   574\t            # field map.\n   575\t            return self.fields_map[field_name]\n   576\t        except KeyError:\n   577\t            raise FieldDoesNotExist(\"%s has no field named '%s'\" % (self.object_name, field_name))\n   578\t\n   579\t    def get_base_chain(self, model):\n   580\t        \"\"\"\n   581\t        Return a list of parent classes leading to `model` (ordered from\n   582\t        closest to most distant ancestor). This has to handle the case where\n   583\t        `model` is a grandparent or even more distant relation.\n   584\t        \"\"\"\n   585\t        if not self.parents:\n   586\t            return []\n   587\t        if model in self.parents:\n   588\t            return [model]\n   589\t        for parent in self.parents:\n   590\t            res = parent._meta.get_base_chain(model)\n   591\t            if res:\n   592\t                res.insert(0, parent)\n   593\t                return res\n   594\t        return []\n   595\t\n   596\t    def get_parent_list(self):\n   597\t        \"\"\"\n   598\t        Return all the ancestors of this model as a list ordered by MRO.\n   599\t        Useful for determining if something is an ancestor, regardless of lineage.\n   600\t        \"\"\"\n   601\t        result = OrderedSet(self.parents)\n   602\t        for parent in self.parents:\n   603\t            for ancestor in parent._meta.get_parent_list():\n   604\t                result.add(ancestor)\n   605\t        return list(result)\n   606\t\n   607\t    def get_ancestor_link(self, ancestor):\n   608\t        \"\"\"\n   609\t        Return the field on the current model which points to the given\n   610\t        \"ancestor\". This is possible an indirect link (a pointer to a parent\n   611\t        model, which points, eventually, to the ancestor). Used when\n   612\t        constructing table joins for model inheritance.\n   613\t\n   614\t        Return None if the model isn't an ancestor of this one.\n   615\t        \"\"\"\n   616\t        if ancestor in self.parents:\n   617\t            return self.parents[ancestor]\n   618\t        for parent in self.parents:\n   619\t            # Tries to get a link field from the immediate parent\n   620\t            parent_link = parent._meta.get_ancestor_link(ancestor)\n   621\t            if parent_link:\n   622\t                # In case of a proxied model, the first link\n   623\t                # of the chain to the ancestor is that parent\n   624\t                # links\n   625\t                return self.parents[parent] or parent_link\n   626\t\n   627\t    def get_path_to_parent(self, parent):\n   628\t        \"\"\"\n   629\t        Return a list of PathInfos containing the path from the current\n   630\t        model to the parent model, or an empty list if parent is not a\n   631\t        parent of the current model.\n   632\t        \"\"\"\n   633\t        if self.model is parent:\n   634\t            return []\n   635\t        # Skip the chain of proxy to the concrete proxied model.\n   636\t        proxied_model = self.concrete_model\n   637\t        path = []\n   638\t        opts = self\n   639\t        for int_model in self.get_base_chain(parent):\n   640\t            if int_model is proxied_model:\n   641\t                opts = int_model._meta\n   642\t            else:\n   643\t                final_field = opts.parents[int_model]\n   644\t                targets = (final_field.remote_field.get_related_field(),)\n   645\t                opts = int_model._meta\n   646\t                path.append(PathInfo(\n   647\t                    from_opts=final_field.model._meta,\n   648\t                    to_opts=opts,\n   649\t                    target_fields=targets,\n   650\t                    join_field=final_field,\n   651\t                    m2m=False,\n   652\t                    direct=True,\n   653\t                    filtered_relation=None,\n   654\t                ))\n   655\t        return path\n   656\t\n   657\t    def get_path_from_parent(self, parent):\n   658\t        \"\"\"\n   659\t        Return a list of PathInfos containing the path from the parent\n   660\t        model to the current model, or an empty list if parent is not a\n   661\t        parent of the current model.\n   662\t        \"\"\"\n   663\t        if self.model is parent:\n   664\t            return []\n   665\t        model = self.concrete_model\n   666\t        # Get a reversed base chain including both the current and parent\n   667\t        # models.\n   668\t        chain = model._meta.get_base_chain(parent)\n   669\t        chain.reverse()\n   670\t        chain.append(model)\n   671\t        # Construct a list of the PathInfos between models in chain.\n   672\t        path = []\n   673\t        for i, ancestor in enumerate(chain[:-1]):\n   674\t            child = chain[i + 1]\n   675\t            link = child._meta.get_ancestor_link(ancestor)\n   676\t            path.extend(link.get_reverse_path_info())\n   677\t        return path\n   678\t\n   679\t    def _populate_directed_relation_graph(self):\n   680\t        \"\"\"\n   681\t        This method is used by each model to find its reverse objects. As this\n   682\t        method is very expensive and is accessed frequently (it looks up every\n   683\t        field in a model, in every app), it is computed on first access and then\n   684\t        is set as a property on every model.\n   685\t        \"\"\"\n   686\t        related_objects_graph = defaultdict(list)\n   687\t\n   688\t        all_models = self.apps.get_models(include_auto_created=True)\n   689\t        for model in all_models:\n   690\t            opts = model._meta\n   691\t            # Abstract model's fields are copied to child models, hence we will\n   692\t            # see the fields from the child models.\n   693\t            if opts.abstract:\n   694\t                continue\n   695\t            fields_with_relations = (\n   696\t                f for f in opts._get_fields(reverse=False, include_parents=False)\n   697\t                if f.is_relation and f.related_model is not None\n   698\t            )\n   699\t            for f in fields_with_relations:\n   700\t                if not isinstance(f.remote_field.model, str):\n   701\t                    related_objects_graph[f.remote_field.model._meta.concrete_model._meta].append(f)\n   702\t\n   703\t        for model in all_models:\n   704\t            # Set the relation_tree using the internal __dict__. In this way\n   705\t            # we avoid calling the cached property. In attribute lookup,\n   706\t            # __dict__ takes precedence over a data descriptor (such as\n   707\t            # @cached_property). This means that the _meta._relation_tree is\n   708\t            # only called if related_objects is not in __dict__.\n   709\t            related_objects = related_objects_graph[model._meta.concrete_model._meta]\n   710\t            model._meta.__dict__['_relation_tree'] = related_objects\n   711\t        # It seems it is possible that self is not in all_models, so guard\n   712\t        # against that with default for get().\n   713\t        return self.__dict__.get('_relation_tree', EMPTY_RELATION_TREE)\n   714\t\n   715\t    @cached_property\n   716\t    def _relation_tree(self):\n   717\t        return self._populate_directed_relation_graph()\n   718\t\n   719\t    def _expire_cache(self, forward=True, reverse=True):\n   720\t        # This method is usually called by apps.cache_clear(), when the\n   721\t        # registry is finalized, or when a new field is added.\n   722\t        if forward:\n   723\t            for cache_key in self.FORWARD_PROPERTIES:\n   724\t                if cache_key in self.__dict__:\n   725\t                    delattr(self, cache_key)\n   726\t        if reverse and not self.abstract:\n   727\t            for cache_key in self.REVERSE_PROPERTIES:\n   728\t                if cache_key in self.__dict__:\n   729\t                    delattr(self, cache_key)\n   730\t        self._get_fields_cache = {}\n   731\t\n   732\t    def get_fields(self, include_parents=True, include_hidden=False):\n   733\t        \"\"\"\n   734\t        Return a list of fields associated to the model. By default, include\n   735\t        forward and reverse fields, fields derived from inheritance, but not\n   736\t        hidden fields. The returned fields can be changed using the parameters:\n   737\t\n   738\t        - include_parents: include fields derived from inheritance\n   739\t        - include_hidden:  include fields that have a related_name that\n   740\t                           starts with a \"+\"\n   741\t        \"\"\"\n   742\t        if include_parents is False:\n   743\t            include_parents = PROXY_PARENTS\n   744\t        return self._get_fields(include_parents=include_parents, include_hidden=include_hidden)\n   745\t\n   746\t    def _get_fields(self, forward=True, reverse=True, include_parents=True, include_hidden=False,\n   747\t                    seen_models=None):\n   748\t        \"\"\"\n   749\t        Internal helper function to return fields of the model.\n   750\t        * If forward=True, then fields defined on this model are returned.\n   751\t        * If reverse=True, then relations pointing to this model are returned.\n   752\t        * If include_hidden=True, then fields with is_hidden=True are returned.\n   753\t        * The include_parents argument toggles if fields from parent models\n   754\t          should be included. It has three values: True, False, and\n   755\t          PROXY_PARENTS. When set to PROXY_PARENTS, the call will return all\n   756\t          fields defined for the current model or any of its parents in the\n   757\t          parent chain to the model's concrete model.\n   758\t        \"\"\"\n   759\t        if include_parents not in (True, False, PROXY_PARENTS):\n   760\t            raise TypeError(\"Invalid argument for include_parents: %s\" % (include_parents,))\n   761\t        # This helper function is used to allow recursion in ``get_fields()``\n   762\t        # implementation and to provide a fast way for Django's internals to\n   763\t        # access specific subsets of fields.\n   764\t\n   765\t        # We must keep track of which models we have already seen. Otherwise we\n   766\t        # could include the same field multiple times from different models.\n   767\t        topmost_call = seen_models is None\n   768\t        if topmost_call:\n   769\t            seen_models = set()\n   770\t        seen_models.add(self.model)\n   771\t\n   772\t        # Creates a cache key composed of all arguments\n   773\t        cache_key = (forward, reverse, include_parents, include_hidden, topmost_call)\n   774\t\n   775\t        try:\n   776\t            # In order to avoid list manipulation. Always return a shallow copy\n   777\t            # of the results.\n   778\t            return self._get_fields_cache[cache_key]\n   779\t        except KeyError:\n   780\t            pass\n   781\t\n   782\t        fields = []\n   783\t        # Recursively call _get_fields() on each parent, with the same\n   784\t        # options provided in this call.\n   785\t        if include_parents is not False:\n   786\t            for parent in self.parents:\n   787\t                # In diamond inheritance it is possible that we see the same\n   788\t                # model from two different routes. In that case, avoid adding\n   789\t                # fields from the same parent again.\n   790\t                if parent in seen_models:\n   791\t                    continue\n   792\t                if (parent._meta.concrete_model != self.concrete_model and\n   793\t                        include_parents == PROXY_PARENTS):\n   794\t                    continue\n   795\t                for obj in parent._meta._get_fields(\n   796\t                        forward=forward, reverse=reverse, include_parents=include_parents,\n   797\t                        include_hidden=include_hidden, seen_models=seen_models):\n   798\t                    if not getattr(obj, 'parent_link', False) or obj.model == self.concrete_model:\n   799\t                        fields.append(obj)\n   800\t        if reverse and not self.proxy:\n   801\t            # Tree is computed once and cached until the app cache is expired.\n   802\t            # It is composed of a list of fields pointing to the current model\n   803\t            # from other models.\n   804\t            all_fields = self._relation_tree\n   805\t            for field in all_fields:\n   806\t                # If hidden fields should be included or the relation is not\n   807\t                # intentionally hidden, add to the fields dict.\n   808\t                if include_hidden or not field.remote_field.hidden:\n   809\t                    fields.append(field.remote_field)\n   810\t\n   811\t        if forward:\n   812\t            fields += self.local_fields\n   813\t            fields += self.local_many_to_many\n   814\t            # Private fields are recopied to each child model, and they get a\n   815\t            # different model as field.model in each child. Hence we have to\n   816\t            # add the private fields separately from the topmost call. If we\n   817\t            # did this recursively similar to local_fields, we would get field\n   818\t            # instances with field.model != self.model.\n   819\t            if topmost_call:\n   820\t                fields += self.private_fields\n   821\t\n   822\t        # In order to avoid list manipulation. Always\n   823\t        # return a shallow copy of the results\n   824\t        fields = make_immutable_fields_list(\"get_fields()\", fields)\n   825\t\n   826\t        # Store result into cache for later access\n   827\t        self._get_fields_cache[cache_key] = fields\n   828\t        return fields\n   829\t\n   830\t    @cached_property\n   831\t    def total_unique_constraints(self):\n   832\t        \"\"\"\n   833\t        Return a list of total unique constraints. Useful for determining set\n   834\t        of fields guaranteed to be unique for all rows.\n   835\t        \"\"\"\n   836\t        return [\n   837\t            constraint\n   838\t            for constraint in self.constraints\n   839\t            if isinstance(constraint, UniqueConstraint) and constraint.condition is None\n   840\t        ]\n   841\t\n   842\t    @cached_property\n   843\t    def _property_names(self):\n   844\t        \"\"\"Return a set of the names of the properties defined on the model.\"\"\"\n   845\t        names = []\n   846\t        for name in dir(self.model):\n   847\t            attr = inspect.getattr_static(self.model, name)\n   848\t            if isinstance(attr, property):\n   849\t                names.append(name)\n   850\t        return frozenset(names)\n   851\t\n   852\t    @cached_property\n   853\t    def db_returning_fields(self):\n   854\t        \"\"\"\n   855\t        Private API intended only to be used by Django itself.\n   856\t        Fields to be returned after a database insert.\n   857\t        \"\"\"\n   858\t        return [\n   859\t            field for field in self._get_fields(forward=True, reverse=False, include_parents=PROXY_PARENTS)\n   860\t            if getattr(field, 'db_returning', False)\n   861\t        ]\n</document_content>\n</document>\n<document index=\"2\">\n<source>/testbed/test_app/test_ordering.py</source>\n<document_content>\n     1\tfrom django.test import TestCase\n     2\tfrom .models import Parent, Child\n     3\t\n     4\tclass OrderingInheritanceTest(TestCase):\n     5\t    @classmethod\n     6\t    def setUpTestData(cls):\n     7\t        # Create some test data\n     8\t        for i in range(3):\n     9\t            Child.objects.create()\n    10\t            Parent.objects.create()\n    11\t\n    12\t    def test_parent_ordering(self):\n    13\t        # Verify parent ordering\n    14\t        parents = list(Parent.objects.all())\n    15\t        pks = [parent.pk for parent in parents]\n    16\t        self.assertEqual(pks, sorted(pks, reverse=True))\n    17\t\n    18\t    def test_child_ordering(self):\n    19\t        # Verify child ordering\n    20\t        children = list(Child.objects.all())\n    21\t        pks = [child.pk for child in children]\n    22\t        self.assertEqual(pks, sorted(pks, reverse=True))\n    23\t\n    24\t    def test_child_query_ordering(self):\n    25\t        # Verify the actual SQL query contains DESC\n    26\t        query = str(Child.objects.all().query)\n    27\t        self.assertIn('DESC', query.upper())</document_content>\n</document>\n<document index=\"3\">\n<source>/testbed/django/db/models/sql/compiler.py</source>\n<document_content>\n     1\timport collections\n     2\timport re\n     3\tfrom functools import partial\n     4\tfrom itertools import chain\n     5\t\n     6\tfrom django.core.exceptions import EmptyResultSet, FieldError\n     7\tfrom django.db import DatabaseError, NotSupportedError\n     8\tfrom django.db.models.constants import LOOKUP_SEP\n     9\tfrom django.db.models.expressions import OrderBy, Random, RawSQL, Ref, Value\n    10\tfrom django.db.models.functions import Cast\n    11\tfrom django.db.models.query_utils import Q, select_related_descend\n    12\tfrom django.db.models.sql.constants import (\n    13\t    CURSOR, GET_ITERATOR_CHUNK_SIZE, MULTI, NO_RESULTS, ORDER_DIR, SINGLE,\n    14\t)\n    15\tfrom django.db.models.sql.query import Query, get_order_dir\n    16\tfrom django.db.transaction import TransactionManagementError\n    17\tfrom django.utils.functional import cached_property\n    18\tfrom django.utils.hashable import make_hashable\n    19\t\n    20\t\n    21\tclass SQLCompiler:\n    22\t    def __init__(self, query, connection, using):\n    23\t        self.query = query\n    24\t        self.connection = connection\n    25\t        self.using = using\n    26\t        self.quote_cache = {'*': '*'}\n    27\t        # The select, klass_info, and annotations are needed by QuerySet.iterator()\n    28\t        # these are set as a side-effect of executing the query. Note that we calculate\n    29\t        # separately a list of extra select columns needed for grammatical correctness\n    30\t        # of the query, but these columns are not included in self.select.\n    31\t        self.select = None\n    32\t        self.annotation_col_map = None\n    33\t        self.klass_info = None\n    34\t        # Multiline ordering SQL clause may appear from RawSQL.\n    35\t        self.ordering_parts = re.compile(r'^(.*)\\s(ASC|DESC)(.*)', re.MULTILINE | re.DOTALL)\n    36\t        self._meta_ordering = None\n    37\t\n    38\t    def setup_query(self):\n    39\t        if all(self.query.alias_refcount[a] == 0 for a in self.query.alias_map):\n    40\t            self.query.get_initial_alias()\n    41\t        self.select, self.klass_info, self.annotation_col_map = self.get_select()\n    42\t        self.col_count = len(self.select)\n    43\t\n    44\t    def pre_sql_setup(self):\n    45\t        \"\"\"\n    46\t        Do any necessary class setup immediately prior to producing SQL. This\n    47\t        is for things that can't necessarily be done in __init__ because we\n    48\t        might not have all the pieces in place at that time.\n    49\t        \"\"\"\n    50\t        self.setup_query()\n    51\t        order_by = self.get_order_by()\n    52\t        self.where, self.having = self.query.where.split_having()\n    53\t        extra_select = self.get_extra_select(order_by, self.select)\n    54\t        self.has_extra_select = bool(extra_select)\n    55\t        group_by = self.get_group_by(self.select + extra_select, order_by)\n    56\t        return extra_select, order_by, group_by\n    57\t\n    58\t    def get_group_by(self, select, order_by):\n    59\t        \"\"\"\n    60\t        Return a list of 2-tuples of form (sql, params).\n    61\t\n    62\t        The logic of what exactly the GROUP BY clause contains is hard\n    63\t        to describe in other words than \"if it passes the test suite,\n    64\t        then it is correct\".\n    65\t        \"\"\"\n    66\t        # Some examples:\n    67\t        #     SomeModel.objects.annotate(Count('somecol'))\n    68\t        #     GROUP BY: all fields of the model\n    69\t        #\n    70\t        #    SomeModel.objects.values('name').annotate(Count('somecol'))\n    71\t        #    GROUP BY: name\n    72\t        #\n    73\t        #    SomeModel.objects.annotate(Count('somecol')).values('name')\n    74\t        #    GROUP BY: all cols of the model\n    75\t        #\n    76\t        #    SomeModel.objects.values('name', 'pk').annotate(Count('somecol')).values('pk')\n    77\t        #    GROUP BY: name, pk\n    78\t        #\n    79\t        #    SomeModel.objects.values('name').annotate(Count('somecol')).values('pk')\n    80\t        #    GROUP BY: name, pk\n    81\t        #\n    82\t        # In fact, the self.query.group_by is the minimal set to GROUP BY. It\n    83\t        # can't be ever restricted to a smaller set, but additional columns in\n    84\t        # HAVING, ORDER BY, and SELECT clauses are added to it. Unfortunately\n    85\t        # the end result is that it is impossible to force the query to have\n    86\t        # a chosen GROUP BY clause - you can almost do this by using the form:\n    87\t        #     .values(*wanted_cols).annotate(AnAggregate())\n    88\t        # but any later annotations, extra selects, values calls that\n    89\t        # refer some column outside of the wanted_cols, order_by, or even\n    90\t        # filter calls can alter the GROUP BY clause.\n    91\t\n    92\t        # The query.group_by is either None (no GROUP BY at all), True\n    93\t        # (group by select fields), or a list of expressions to be added\n    94\t        # to the group by.\n    95\t        if self.query.group_by is None:\n    96\t            return []\n    97\t        expressions = []\n    98\t        if self.query.group_by is not True:\n    99\t            # If the group by is set to a list (by .values() call most likely),\n   100\t            # then we need to add everything in it to the GROUP BY clause.\n   101\t            # Backwards compatibility hack for setting query.group_by. Remove\n   102\t            # when  we have public API way of forcing the GROUP BY clause.\n   103\t            # Converts string references to expressions.\n   104\t            for expr in self.query.group_by:\n   105\t                if not hasattr(expr, 'as_sql'):\n   106\t                    expressions.append(self.query.resolve_ref(expr))\n   107\t                else:\n   108\t                    expressions.append(expr)\n   109\t        # Note that even if the group_by is set, it is only the minimal\n   110\t        # set to group by. So, we need to add cols in select, order_by, and\n   111\t        # having into the select in any case.\n   112\t        ref_sources = {\n   113\t            expr.source for expr in expressions if isinstance(expr, Ref)\n   114\t        }\n   115\t        for expr, _, _ in select:\n   116\t            # Skip members of the select clause that are already included\n   117\t            # by reference.\n   118\t            if expr in ref_sources:\n   119\t                continue\n   120\t            cols = expr.get_group_by_cols()\n   121\t            for col in cols:\n   122\t                expressions.append(col)\n   123\t        for expr, (sql, params, is_ref) in order_by:\n   124\t            # Skip References to the select clause, as all expressions in the\n   125\t            # select clause are already part of the group by.\n   126\t            if not is_ref:\n   127\t                expressions.extend(expr.get_group_by_cols())\n   128\t        having_group_by = self.having.get_group_by_cols() if self.having else ()\n   129\t        for expr in having_group_by:\n   130\t            expressions.append(expr)\n   131\t        result = []\n   132\t        seen = set()\n   133\t        expressions = self.collapse_group_by(expressions, having_group_by)\n   134\t\n   135\t        for expr in expressions:\n   136\t            sql, params = self.compile(expr)\n   137\t            params_hash = make_hashable(params)\n   138\t            if (sql, params_hash) not in seen:\n   139\t                result.append((sql, params))\n   140\t                seen.add((sql, params_hash))\n   141\t        return result\n   142\t\n   143\t    def collapse_group_by(self, expressions, having):\n   144\t        # If the DB can group by primary key, then group by the primary key of\n   145\t        # query's main model. Note that for PostgreSQL the GROUP BY clause must\n   146\t        # include the primary key of every table, but for MySQL it is enough to\n   147\t        # have the main table's primary key.\n   148\t        if self.connection.features.allows_group_by_pk:\n   149\t            # Determine if the main model's primary key is in the query.\n   150\t            pk = None\n   151\t            for expr in expressions:\n   152\t                # Is this a reference to query's base table primary key? If the\n   153\t                # expression isn't a Col-like, then skip the expression.\n   154\t                if (getattr(expr, 'target', None) == self.query.model._meta.pk and\n   155\t                        getattr(expr, 'alias', None) == self.query.base_table):\n   156\t                    pk = expr\n   157\t                    break\n   158\t            # If the main model's primary key is in the query, group by that\n   159\t            # field, HAVING expressions, and expressions associated with tables\n   160\t            # that don't have a primary key included in the grouped columns.\n   161\t            if pk:\n   162\t                pk_aliases = {\n   163\t                    expr.alias for expr in expressions\n   164\t                    if hasattr(expr, 'target') and expr.target.primary_key\n   165\t                }\n   166\t                expressions = [pk] + [\n   167\t                    expr for expr in expressions\n   168\t                    if expr in having or (\n   169\t                        getattr(expr, 'alias', None) is not None and expr.alias not in pk_aliases\n   170\t                    )\n   171\t                ]\n   172\t        elif self.connection.features.allows_group_by_selected_pks:\n   173\t            # Filter out all expressions associated with a table's primary key\n   174\t            # present in the grouped columns. This is done by identifying all\n   175\t            # tables that have their primary key included in the grouped\n   176\t            # columns and removing non-primary key columns referring to them.\n   177\t            # Unmanaged models are excluded because they could be representing\n   178\t            # database views on which the optimization might not be allowed.\n   179\t            pks = {\n   180\t                expr for expr in expressions\n   181\t                if (\n   182\t                    hasattr(expr, 'target') and\n   183\t                    expr.target.primary_key and\n   184\t                    self.connection.features.allows_group_by_selected_pks_on_model(expr.target.model)\n   185\t                )\n   186\t            }\n   187\t            aliases = {expr.alias for expr in pks}\n   188\t            expressions = [\n   189\t                expr for expr in expressions if expr in pks or getattr(expr, 'alias', None) not in aliases\n   190\t            ]\n   191\t        return expressions\n   192\t\n   193\t    def get_select(self):\n   194\t        \"\"\"\n   195\t        Return three values:\n   196\t        - a list of 3-tuples of (expression, (sql, params), alias)\n   197\t        - a klass_info structure,\n   198\t        - a dictionary of annotations\n   199\t\n   200\t        The (sql, params) is what the expression will produce, and alias is the\n   201\t        \"AS alias\" for the column (possibly None).\n   202\t\n   203\t        The klass_info structure contains the following information:\n   204\t        - The base model of the query.\n   205\t        - Which columns for that model are present in the query (by\n   206\t          position of the select clause).\n   207\t        - related_klass_infos: [f, klass_info] to descent into\n   208\t\n   209\t        The annotations is a dictionary of {'attname': column position} values.\n   210\t        \"\"\"\n   211\t        select = []\n   212\t        klass_info = None\n   213\t        annotations = {}\n   214\t        select_idx = 0\n   215\t        for alias, (sql, params) in self.query.extra_select.items():\n   216\t            annotations[alias] = select_idx\n   217\t            select.append((RawSQL(sql, params), alias))\n   218\t            select_idx += 1\n   219\t        assert not (self.query.select and self.query.default_cols)\n   220\t        if self.query.default_cols:\n   221\t            cols = self.get_default_columns()\n   222\t        else:\n   223\t            # self.query.select is a special case. These columns never go to\n   224\t            # any model.\n   225\t            cols = self.query.select\n   226\t        if cols:\n   227\t            select_list = []\n   228\t            for col in cols:\n   229\t                select_list.append(select_idx)\n   230\t                select.append((col, None))\n   231\t                select_idx += 1\n   232\t            klass_info = {\n   233\t                'model': self.query.model,\n   234\t                'select_fields': select_list,\n   235\t            }\n   236\t        for alias, annotation in self.query.annotation_select.items():\n   237\t            annotations[alias] = select_idx\n   238\t            select.append((annotation, alias))\n   239\t            select_idx += 1\n   240\t\n   241\t        if self.query.select_related:\n   242\t            related_klass_infos = self.get_related_selections(select)\n   243\t            klass_info['related_klass_infos'] = related_klass_infos\n   244\t\n   245\t            def get_select_from_parent(klass_info):\n   246\t                for ki in klass_info['related_klass_infos']:\n   247\t                    if ki['from_parent']:\n   248\t                        ki['select_fields'] = (klass_info['select_fields'] +\n   249\t                                               ki['select_fields'])\n   250\t                    get_select_from_parent(ki)\n   251\t            get_select_from_parent(klass_info)\n   252\t\n   253\t        ret = []\n   254\t        for col, alias in select:\n   255\t            try:\n   256\t                sql, params = self.compile(col)\n   257\t            except EmptyResultSet:\n   258\t                # Select a predicate that's always False.\n   259\t                sql, params = '0', ()\n   260\t            else:\n   261\t                sql, params = col.select_format(self, sql, params)\n   262\t            ret.append((col, (sql, params), alias))\n   263\t        return ret, klass_info, annotations\n   264\t\n   265\t    def get_order_by(self):\n   266\t        \"\"\"\n   267\t        Return a list of 2-tuples of form (expr, (sql, params, is_ref)) for the\n   268\t        ORDER BY clause.\n   269\t\n   270\t        The order_by clause can alter the select clause (for example it\n   271\t        can add aliases to clauses that do not yet have one, or it can\n   272\t        add totally new select clauses).\n   273\t        \"\"\"\n   274\t        if self.query.extra_order_by:\n   275\t            ordering = self.query.extra_order_by\n   276\t        elif not self.query.default_ordering:\n   277\t            ordering = self.query.order_by\n   278\t        elif self.query.order_by:\n   279\t            ordering = self.query.order_by\n   280\t        elif self.query.get_meta().ordering:\n   281\t            ordering = self.query.get_meta().ordering\n   282\t            self._meta_ordering = ordering\n   283\t            # Ensure descending flag is preserved for inherited models\n   284\t            if not self.query.standard_ordering:\n   285\t                ordering = [\n   286\t                    f.replace('-', '') if f.startswith('-') else '-' + f\n   287\t                    for f in ordering\n   288\t                ]\n   289\t                # Reverse the order for inherited models\n   290\t                if self._meta_ordering:\n   291\t                    ordering = ['-' + f.lstrip('-') if not f.startswith('-') else f.lstrip('-') for f in ordering]\n   292\t        else:\n   293\t            ordering = []\n   294\t        # Get the base ordering direction\n   295\t        if self.query.standard_ordering:\n   296\t            asc, desc = ORDER_DIR['ASC']\n   297\t        else:\n   298\t            asc, desc = ORDER_DIR['DESC']\n   299\t\n   300\t        # Check if we need to reverse the ordering based on meta ordering\n   301\t        meta = self.query.model._meta\n   302\t        if meta.ordering and isinstance(meta.ordering[0], str):\n   303\t            if meta.ordering[0].startswith('-'):\n   304\t                # For descending meta ordering, we need to swap the direction\n   305\t                if not self.query.standard_ordering:\n   306\t                    asc, desc = ORDER_DIR['ASC']\n   307\t                else:\n   308\t                    asc, desc = ORDER_DIR['DESC']\n   309\t\n   310\t        order_by = []\n   311\t        for field in ordering:\n   312\t            if hasattr(field, 'resolve_expression'):\n   313\t                if isinstance(field, Value):\n   314\t                    # output_field must be resolved for constants.\n   315\t                    field = Cast(field, field.output_field)\n   316\t                if not isinstance(field, OrderBy):\n   317\t                    field = field.asc()\n   318\t                if not self.query.standard_ordering:\n   319\t                    field = field.copy()\n   320\t                    field.reverse_ordering()\n   321\t                order_by.append((field, False))\n   322\t                continue\n   323\t            if field == '?':  # random\n   324\t                order_by.append((OrderBy(Random()), False))\n   325\t                continue\n   326\t\n   327\t            col, order = get_order_dir(field, asc)\n   328\t            # Handle descending flag for inherited models\n   329\t            if not self.query.standard_ordering:\n   330\t                order = 'DESC' if order == 'ASC' else 'ASC'\n   331\t            descending = order == 'DESC'\n   332\t\n   333\t            if col in self.query.annotation_select:\n   334\t                # Reference to expression in SELECT clause\n   335\t                order_by.append((\n   336\t                    OrderBy(Ref(col, self.query.annotation_select[col]), descending=descending),\n   337\t                    True))\n   338\t                continue\n   339\t            if col in self.query.annotations:\n   340\t                # References to an expression which is masked out of the SELECT\n   341\t                # clause.\n   342\t                expr = self.query.annotations[col]\n   343\t                if isinstance(expr, Value):\n   344\t                    # output_field must be resolved for constants.\n   345\t                    expr = Cast(expr, expr.output_field)\n   346\t                order_by.append((OrderBy(expr, descending=descending), False))\n   347\t                continue\n   348\t\n   349\t            if '.' in field:\n   350\t                # This came in through an extra(order_by=...) addition. Pass it\n   351\t                # on verbatim.\n   352\t                table, col = col.split('.', 1)\n   353\t                order_by.append((\n   354\t                    OrderBy(\n   355\t                        RawSQL('%s.%s' % (self.quote_name_unless_alias(table), col), []),\n   356\t                        descending=descending\n   357\t                    ), False))\n   358\t                continue\n   359\t\n   360\t            if not self.query.extra or col not in self.query.extra:\n   361\t                # 'col' is of the form 'field' or 'field1__field2' or\n   362\t                # '-field1__field2__field', etc.\n   363\t                order_by.extend(self.find_ordering_name(\n   364\t                    field, self.query.get_meta(), default_order=asc))\n   365\t            else:\n   366\t                if col not in self.query.extra_select:\n   367\t                    order_by.append((\n   368\t                        OrderBy(RawSQL(*self.query.extra[col]), descending=descending),\n   369\t                        False))\n   370\t                else:\n   371\t                    order_by.append((\n   372\t                        OrderBy(Ref(col, RawSQL(*self.query.extra[col])), descending=descending),\n   373\t                        True))\n   374\t        result = []\n   375\t        seen = set()\n   376\t\n   377\t        for expr, is_ref in order_by:\n   378\t            resolved = expr.resolve_expression(self.query, allow_joins=True, reuse=None)\n   379\t            if self.query.combinator:\n   380\t                src = resolved.get_source_expressions()[0]\n   381\t                # Relabel order by columns to raw numbers if this is a combined\n   382\t                # query; necessary since the columns can't be referenced by the\n   383\t                # fully qualified name and the simple column names may collide.\n   384\t                for idx, (sel_expr, _, col_alias) in enumerate(self.select):\n   385\t                    if is_ref and col_alias == src.refs:\n   386\t                        src = src.source\n   387\t                    elif col_alias:\n   388\t                        continue\n   389\t                    if src == sel_expr:\n   390\t                        resolved.set_source_expressions([RawSQL('%d' % (idx + 1), ())])\n   391\t                        break\n   392\t                else:\n   393\t                    if col_alias:\n   394\t                        raise DatabaseError('ORDER BY term does not match any column in the result set.')\n   395\t                    # Add column used in ORDER BY clause without an alias to\n   396\t                    # the selected columns.\n   397\t                    self.query.add_select_col(src)\n   398\t                    resolved.set_source_expressions([RawSQL('%d' % len(self.query.select), ())])\n   399\t            sql, params = self.compile(resolved)\n   400\t            # Don't add the same column twice, but the order direction is\n   401\t            # not taken into account so we strip it. When this entire method\n   402\t            # is refactored into expressions, then we can check each part as we\n   403\t            # generate it.\n   404\t            without_ordering = self.ordering_parts.search(sql).group(1)\n   405\t            params_hash = make_hashable(params)\n   406\t            if (without_ordering, params_hash) in seen:\n   407\t                continue\n   408\t            seen.add((without_ordering, params_hash))\n   409\t            result.append((resolved, (sql, params, is_ref)))\n   410\t        return result\n   411\t\n   412\t    def get_extra_select(self, order_by, select):\n   413\t        extra_select = []\n   414\t        if self.query.distinct and not self.query.distinct_fields:\n   415\t            select_sql = [t[1] for t in select]\n   416\t            for expr, (sql, params, is_ref) in order_by:\n   417\t                without_ordering = self.ordering_parts.search(sql).group(1)\n   418\t                if not is_ref and (without_ordering, params) not in select_sql:\n   419\t                    extra_select.append((expr, (without_ordering, params), None))\n   420\t        return extra_select\n   421\t\n   422\t    def quote_name_unless_alias(self, name):\n   423\t        \"\"\"\n   424\t        A wrapper around connection.ops.quote_name that doesn't quote aliases\n   425\t        for table names. This avoids problems with some SQL dialects that treat\n   426\t        quoted strings specially (e.g. PostgreSQL).\n   427\t        \"\"\"\n   428\t        if name in self.quote_cache:\n   429\t            return self.quote_cache[name]\n   430\t        if ((name in self.query.alias_map and name not in self.query.table_map) or\n   431\t                name in self.query.extra_select or (\n   432\t                    self.query.external_aliases.get(name) and name not in self.query.table_map)):\n   433\t            self.quote_cache[name] = name\n   434\t            return name\n   435\t        r = self.connection.ops.quote_name(name)\n   436\t        self.quote_cache[name] = r\n   437\t        return r\n   438\t\n   439\t    def compile(self, node):\n   440\t        vendor_impl = getattr(node, 'as_' + self.connection.vendor, None)\n   441\t        if vendor_impl:\n   442\t            sql, params = vendor_impl(self, self.connection)\n   443\t        else:\n   444\t            sql, params = node.as_sql(self, self.connection)\n   445\t        return sql, params\n   446\t\n   447\t    def get_combinator_sql(self, combinator, all):\n   448\t        features = self.connection.features\n   449\t        compilers = [\n   450\t            query.get_compiler(self.using, self.connection)\n   451\t            for query in self.query.combined_queries if not query.is_empty()\n   452\t        ]\n   453\t        if not features.supports_slicing_ordering_in_compound:\n   454\t            for query, compiler in zip(self.query.combined_queries, compilers):\n   455\t                if query.low_mark or query.high_mark:\n   456\t                    raise DatabaseError('LIMIT/OFFSET not allowed in subqueries of compound statements.')\n   457\t                if compiler.get_order_by():\n   458\t                    raise DatabaseError('ORDER BY not allowed in subqueries of compound statements.')\n   459\t        parts = ()\n   460\t        for compiler in compilers:\n   461\t            try:\n   462\t                # If the columns list is limited, then all combined queries\n   463\t                # must have the same columns list. Set the selects defined on\n   464\t                # the query on all combined queries, if not already set.\n   465\t                if not compiler.query.values_select and self.query.values_select:\n   466\t                    compiler.query = compiler.query.clone()\n   467\t                    compiler.query.set_values((\n   468\t                        *self.query.extra_select,\n   469\t                        *self.query.values_select,\n   470\t                        *self.query.annotation_select,\n   471\t                    ))\n   472\t                part_sql, part_args = compiler.as_sql()\n   473\t                if compiler.query.combinator:\n   474\t                    # Wrap in a subquery if wrapping in parentheses isn't\n   475\t                    # supported.\n   476\t                    if not features.supports_parentheses_in_compound:\n   477\t                        part_sql = 'SELECT * FROM ({})'.format(part_sql)\n   478\t                    # Add parentheses when combining with compound query if not\n   479\t                    # already added for all compound queries.\n   480\t                    elif not features.supports_slicing_ordering_in_compound:\n   481\t                        part_sql = '({})'.format(part_sql)\n   482\t                parts += ((part_sql, part_args),)\n   483\t            except EmptyResultSet:\n   484\t                # Omit the empty queryset with UNION and with DIFFERENCE if the\n   485\t                # first queryset is nonempty.\n   486\t                if combinator == 'union' or (combinator == 'difference' and parts):\n   487\t                    continue\n   488\t                raise\n   489\t        if not parts:\n   490\t            raise EmptyResultSet\n   491\t        combinator_sql = self.connection.ops.set_operators[combinator]\n   492\t        if all and combinator == 'union':\n   493\t            combinator_sql += ' ALL'\n   494\t        braces = '({})' if features.supports_slicing_ordering_in_compound else '{}'\n   495\t        sql_parts, args_parts = zip(*((braces.format(sql), args) for sql, args in parts))\n   496\t        result = [' {} '.format(combinator_sql).join(sql_parts)]\n   497\t        params = []\n   498\t        for part in args_parts:\n   499\t            params.extend(part)\n   500\t        return result, params\n   501\t\n   502\t    def as_sql(self, with_limits=True, with_col_aliases=False):\n   503\t        \"\"\"\n   504\t        Create the SQL for this query. Return the SQL string and list of\n   505\t        parameters.\n   506\t\n   507\t        If 'with_limits' is False, any limit/offset information is not included\n   508\t        in the query.\n   509\t        \"\"\"\n   510\t        refcounts_before = self.query.alias_refcount.copy()\n   511\t        try:\n   512\t            extra_select, order_by, group_by = self.pre_sql_setup()\n   513\t            for_update_part = None\n   514\t            # Is a LIMIT/OFFSET clause needed?\n   515\t            with_limit_offset = with_limits and (self.query.high_mark is not None or self.query.low_mark)\n   516\t            combinator = self.query.combinator\n   517\t            features = self.connection.features\n   518\t            if combinator:\n   519\t                if not getattr(features, 'supports_select_{}'.format(combinator)):\n   520\t                    raise NotSupportedError('{} is not supported on this database backend.'.format(combinator))\n   521\t                result, params = self.get_combinator_sql(combinator, self.query.combinator_all)\n   522\t            else:\n   523\t                distinct_fields, distinct_params = self.get_distinct()\n   524\t                # This must come after 'select', 'ordering', and 'distinct'\n   525\t                # (see docstring of get_from_clause() for details).\n   526\t                from_, f_params = self.get_from_clause()\n   527\t                where, w_params = self.compile(self.where) if self.where is not None else (\"\", [])\n   528\t                having, h_params = self.compile(self.having) if self.having is not None else (\"\", [])\n   529\t                result = ['SELECT']\n   530\t                params = []\n   531\t\n   532\t                if self.query.distinct:\n   533\t                    distinct_result, distinct_params = self.connection.ops.distinct_sql(\n   534\t                        distinct_fields,\n   535\t                        distinct_params,\n   536\t                    )\n   537\t                    result += distinct_result\n   538\t                    params += distinct_params\n   539\t\n   540\t                out_cols = []\n   541\t                col_idx = 1\n   542\t                for _, (s_sql, s_params), alias in self.select + extra_select:\n   543\t                    if alias:\n   544\t                        s_sql = '%s AS %s' % (s_sql, self.connection.ops.quote_name(alias))\n   545\t                    elif with_col_aliases:\n   546\t                        s_sql = '%s AS %s' % (s_sql, 'Col%d' % col_idx)\n   547\t                        col_idx += 1\n   548\t                    params.extend(s_params)\n   549\t                    out_cols.append(s_sql)\n   550\t\n   551\t                result += [', '.join(out_cols), 'FROM', *from_]\n   552\t                params.extend(f_params)\n   553\t\n   554\t                if self.query.select_for_update and self.connection.features.has_select_for_update:\n   555\t                    if self.connection.get_autocommit():\n   556\t                        raise TransactionManagementError('select_for_update cannot be used outside of a transaction.')\n   557\t\n   558\t                    if with_limit_offset and not self.connection.features.supports_select_for_update_with_limit:\n   559\t                        raise NotSupportedError(\n   560\t                            'LIMIT/OFFSET is not supported with '\n   561\t                            'select_for_update on this database backend.'\n   562\t                        )\n   563\t                    nowait = self.query.select_for_update_nowait\n   564\t                    skip_locked = self.query.select_for_update_skip_locked\n   565\t                    of = self.query.select_for_update_of\n   566\t                    # If it's a NOWAIT/SKIP LOCKED/OF query but the backend\n   567\t                    # doesn't support it, raise NotSupportedError to prevent a\n   568\t                    # possible deadlock.\n   569\t                    if nowait and not self.connection.features.has_select_for_update_nowait:\n   570\t                        raise NotSupportedError('NOWAIT is not supported on this database backend.')\n   571\t                    elif skip_locked and not self.connection.features.has_select_for_update_skip_locked:\n   572\t                        raise NotSupportedError('SKIP LOCKED is not supported on this database backend.')\n   573\t                    elif of and not self.connection.features.has_select_for_update_of:\n   574\t                        raise NotSupportedError('FOR UPDATE OF is not supported on this database backend.')\n   575\t                    for_update_part = self.connection.ops.for_update_sql(\n   576\t                        nowait=nowait,\n   577\t                        skip_locked=skip_locked,\n   578\t                        of=self.get_select_for_update_of_arguments(),\n   579\t                    )\n   580\t\n   581\t                if for_update_part and self.connection.features.for_update_after_from:\n   582\t                    result.append(for_update_part)\n   583\t\n   584\t                if where:\n   585\t                    result.append('WHERE %s' % where)\n   586\t                    params.extend(w_params)\n   587\t\n   588\t                grouping = []\n   589\t                for g_sql, g_params in group_by:\n   590\t                    grouping.append(g_sql)\n   591\t                    params.extend(g_params)\n   592\t                if grouping:\n   593\t                    if distinct_fields:\n   594\t                        raise NotImplementedError('annotate() + distinct(fields) is not implemented.')\n   595\t                    order_by = order_by or self.connection.ops.force_no_ordering()\n   596\t                    result.append('GROUP BY %s' % ', '.join(grouping))\n   597\t                    if self._meta_ordering:\n   598\t                        order_by = None\n   599\t                if having:\n   600\t                    result.append('HAVING %s' % having)\n   601\t                    params.extend(h_params)\n   602\t\n   603\t            if self.query.explain_query:\n   604\t                result.insert(0, self.connection.ops.explain_query_prefix(\n   605\t                    self.query.explain_format,\n   606\t                    **self.query.explain_options\n   607\t                ))\n   608\t\n   609\t            if order_by:\n   610\t                ordering = []\n   611\t                for _, (o_sql, o_params, _) in order_by:\n   612\t                    ordering.append(o_sql)\n   613\t                    params.extend(o_params)\n   614\t                result.append('ORDER BY %s' % ', '.join(ordering))\n   615\t\n   616\t            if with_limit_offset:\n   617\t                result.append(self.connection.ops.limit_offset_sql(self.query.low_mark, self.query.high_mark))\n   618\t\n   619\t            if for_update_part and not self.connection.features.for_update_after_from:\n   620\t                result.append(for_update_part)\n   621\t\n   622\t            if self.query.subquery and extra_select:\n   623\t                # If the query is used as a subquery, the extra selects would\n   624\t                # result in more columns than the left-hand side expression is\n   625\t                # expecting. This can happen when a subquery uses a combination\n   626\t                # of order_by() and distinct(), forcing the ordering expressions\n   627\t                # to be selected as well. Wrap the query in another subquery\n   628\t                # to exclude extraneous selects.\n   629\t                sub_selects = []\n   630\t                sub_params = []\n   631\t                for index, (select, _, alias) in enumerate(self.select, start=1):\n   632\t                    if not alias and with_col_aliases:\n   633\t                        alias = 'col%d' % index\n   634\t                    if alias:\n   635\t                        sub_selects.append(\"%s.%s\" % (\n   636\t                            self.connection.ops.quote_name('subquery'),\n   637\t                            self.connection.ops.quote_name(alias),\n   638\t                        ))\n   639\t                    else:\n   640\t                        select_clone = select.relabeled_clone({select.alias: 'subquery'})\n   641\t                        subselect, subparams = select_clone.as_sql(self, self.connection)\n   642\t                        sub_selects.append(subselect)\n   643\t                        sub_params.extend(subparams)\n   644\t                return 'SELECT %s FROM (%s) subquery' % (\n   645\t                    ', '.join(sub_selects),\n   646\t                    ' '.join(result),\n   647\t                ), tuple(sub_params + params)\n   648\t\n   649\t            return ' '.join(result), tuple(params)\n   650\t        finally:\n   651\t            # Finally do cleanup - get rid of the joins we created above.\n   652\t            self.query.reset_refcounts(refcounts_before)\n   653\t\n   654\t    def get_default_columns(self, start_alias=None, opts=None, from_parent=None):\n   655\t        \"\"\"\n   656\t        Compute the default columns for selecting every field in the base\n   657\t        model. Will sometimes be called to pull in related models (e.g. via\n   658\t        select_related), in which case \"opts\" and \"start_alias\" will be given\n   659\t        to provide a starting point for the traversal.\n   660\t\n   661\t        Return a list of strings, quoted appropriately for use in SQL\n   662\t        directly, as well as a set of aliases used in the select statement (if\n   663\t        'as_pairs' is True, return a list of (alias, col_name) pairs instead\n   664\t        of strings as the first component and None as the second component).\n   665\t        \"\"\"\n   666\t        result = []\n   667\t        if opts is None:\n   668\t            opts = self.query.get_meta()\n   669\t        only_load = self.deferred_to_columns()\n   670\t        start_alias = start_alias or self.query.get_initial_alias()\n   671\t        # The 'seen_models' is used to optimize checking the needed parent\n   672\t        # alias for a given field. This also includes None -> start_alias to\n   673\t        # be used by local fields.\n   674\t        seen_models = {None: start_alias}\n   675\t\n   676\t        for field in opts.concrete_fields:\n   677\t            model = field.model._meta.concrete_model\n   678\t            # A proxy model will have a different model and concrete_model. We\n   679\t            # will assign None if the field belongs to this model.\n   680\t            if model == opts.model:\n   681\t                model = None\n   682\t            if from_parent and model is not None and issubclass(\n   683\t                    from_parent._meta.concrete_model, model._meta.concrete_model):\n   684\t                # Avoid loading data for already loaded parents.\n   685\t                # We end up here in the case select_related() resolution\n   686\t                # proceeds from parent model to child model. In that case the\n   687\t                # parent model data is already present in the SELECT clause,\n   688\t                # and we want to avoid reloading the same data again.\n   689\t                continue\n   690\t            if field.model in only_load and field.attname not in only_load[field.model]:\n   691\t                continue\n   692\t            alias = self.query.join_parent_model(opts, model, start_alias,\n   693\t                                                 seen_models)\n   694\t            column = field.get_col(alias)\n   695\t            result.append(column)\n   696\t        return result\n   697\t\n   698\t    def get_distinct(self):\n   699\t        \"\"\"\n   700\t        Return a quoted list of fields to use in DISTINCT ON part of the query.\n   701\t\n   702\t        This method can alter the tables in the query, and thus it must be\n   703\t        called before get_from_clause().\n   704\t        \"\"\"\n   705\t        result = []\n   706\t        params = []\n   707\t        opts = self.query.get_meta()\n   708\t\n   709\t        for name in self.query.distinct_fields:\n   710\t            parts = name.split(LOOKUP_SEP)\n   711\t            _, targets, alias, joins, path, _, transform_function = self._setup_joins(parts, opts, None)\n   712\t            targets, alias, _ = self.query.trim_joins(targets, joins, path)\n   713\t            for target in targets:\n   714\t                if name in self.query.annotation_select:\n   715\t                    result.append(name)\n   716\t                else:\n   717\t                    r, p = self.compile(transform_function(target, alias))\n   718\t                    result.append(r)\n   719\t                    params.append(p)\n   720\t        return result, params\n   721\t\n   722\t    def find_ordering_name(self, name, opts, alias=None, default_order='ASC',\n   723\t                           already_seen=None):\n   724\t        \"\"\"\n   725\t        Return the table alias (the name might be ambiguous, the alias will\n   726\t        not be) and column name for ordering by the given 'name' parameter.\n   727\t        The 'name' is of the form 'field1__field2__...__fieldN'.\n   728\t        \"\"\"\n   729\t        name, order = get_order_dir(name, default_order)\n   730\t        descending = order == 'DESC'\n   731\t        pieces = name.split(LOOKUP_SEP)\n   732\t        field, targets, alias, joins, path, opts, transform_function = self._setup_joins(pieces, opts, alias)\n   733\t\n   734\t        # If we get to this point and the field is a relation to another model,\n   735\t        # append the default ordering for that model unless the attribute name\n   736\t        # of the field is specified.\n   737\t        if field.is_relation and opts.ordering and getattr(field, 'attname', None) != name:\n   738\t            # Firstly, avoid infinite loops.\n   739\t            already_seen = already_seen or set()\n   740\t            join_tuple = tuple(getattr(self.query.alias_map[j], 'join_cols', None) for j in joins)\n   741\t            if join_tuple in already_seen:\n   742\t                raise FieldError('Infinite loop caused by ordering.')\n   743\t            already_seen.add(join_tuple)\n   744\t\n   745\t            results = []\n   746\t            for item in opts.ordering:\n   747\t                if hasattr(item, 'resolve_expression') and not isinstance(item, OrderBy):\n   748\t                    item = item.desc() if descending else item.asc()\n   749\t                if isinstance(item, OrderBy):\n   750\t                    results.append((item, False))\n   751\t                    continue\n   752\t                results.extend(self.find_ordering_name(item, opts, alias,\n   753\t                                                       order, already_seen))\n   754\t            return results\n   755\t        targets, alias, _ = self.query.trim_joins(targets, joins, path)\n   756\t        return [(OrderBy(transform_function(t, alias), descending=descending), False) for t in targets]\n   757\t\n   758\t    def _setup_joins(self, pieces, opts, alias):\n   759\t        \"\"\"\n   760\t        Helper method for get_order_by() and get_distinct().\n   761\t\n   762\t        get_ordering() and get_distinct() must produce same target columns on\n   763\t        same input, as the prefixes of get_ordering() and get_distinct() must\n   764\t        match. Executing SQL where this is not true is an error.\n   765\t        \"\"\"\n   766\t        alias = alias or self.query.get_initial_alias()\n   767\t        field, targets, opts, joins, path, transform_function = self.query.setup_joins(pieces, opts, alias)\n   768\t        alias = joins[-1]\n   769\t        return field, targets, alias, joins, path, opts, transform_function\n   770\t\n   771\t    def get_from_clause(self):\n   772\t        \"\"\"\n   773\t        Return a list of strings that are joined together to go after the\n   774\t        \"FROM\" part of the query, as well as a list any extra parameters that\n   775\t        need to be included. Subclasses, can override this to create a\n   776\t        from-clause via a \"select\".\n   777\t\n   778\t        This should only be called after any SQL construction methods that\n   779\t        might change the tables that are needed. This means the select columns,\n   780\t        ordering, and distinct must be done first.\n   781\t        \"\"\"\n   782\t        result = []\n   783\t        params = []\n   784\t        for alias in tuple(self.query.alias_map):\n   785\t            if not self.query.alias_refcount[alias]:\n   786\t                continue\n   787\t            try:\n   788\t                from_clause = self.query.alias_map[alias]\n   789\t            except KeyError:\n   790\t                # Extra tables can end up in self.tables, but not in the\n   791\t                # alias_map if they aren't in a join. That's OK. We skip them.\n   792\t                continue\n   793\t            clause_sql, clause_params = self.compile(from_clause)\n   794\t            result.append(clause_sql)\n   795\t            params.extend(clause_params)\n   796\t        for t in self.query.extra_tables:\n   797\t            alias, _ = self.query.table_alias(t)\n   798\t            # Only add the alias if it's not already present (the table_alias()\n   799\t            # call increments the refcount, so an alias refcount of one means\n   800\t            # this is the only reference).\n   801\t            if alias not in self.query.alias_map or self.query.alias_refcount[alias] == 1:\n   802\t                result.append(', %s' % self.quote_name_unless_alias(alias))\n   803\t        return result, params\n   804\t\n   805\t    def get_related_selections(self, select, opts=None, root_alias=None, cur_depth=1,\n   806\t                               requested=None, restricted=None):\n   807\t        \"\"\"\n   808\t        Fill in the information needed for a select_related query. The current\n   809\t        depth is measured as the number of connections away from the root model\n   810\t        (for example, cur_depth=1 means we are looking at models with direct\n   811\t        connections to the root model).\n   812\t        \"\"\"\n   813\t        def _get_field_choices():\n   814\t            direct_choices = (f.name for f in opts.fields if f.is_relation)\n   815\t            reverse_choices = (\n   816\t                f.field.related_query_name()\n   817\t                for f in opts.related_objects if f.field.unique\n   818\t            )\n   819\t            return chain(direct_choices, reverse_choices, self.query._filtered_relations)\n   820\t\n   821\t        related_klass_infos = []\n   822\t        if not restricted and cur_depth > self.query.max_depth:\n   823\t            # We've recursed far enough; bail out.\n   824\t            return related_klass_infos\n   825\t\n   826\t        if not opts:\n   827\t            opts = self.query.get_meta()\n   828\t            root_alias = self.query.get_initial_alias()\n   829\t        only_load = self.query.get_loaded_field_names()\n   830\t\n   831\t        # Setup for the case when only particular related fields should be\n   832\t        # included in the related selection.\n   833\t        fields_found = set()\n   834\t        if requested is None:\n   835\t            restricted = isinstance(self.query.select_related, dict)\n   836\t            if restricted:\n   837\t                requested = self.query.select_related\n   838\t\n   839\t        def get_related_klass_infos(klass_info, related_klass_infos):\n   840\t            klass_info['related_klass_infos'] = related_klass_infos\n   841\t\n   842\t        for f in opts.fields:\n   843\t            field_model = f.model._meta.concrete_model\n   844\t            fields_found.add(f.name)\n   845\t\n   846\t            if restricted:\n   847\t                next = requested.get(f.name, {})\n   848\t                if not f.is_relation:\n   849\t                    # If a non-related field is used like a relation,\n   850\t                    # or if a single non-relational field is given.\n   851\t                    if next or f.name in requested:\n   852\t                        raise FieldError(\n   853\t                            \"Non-relational field given in select_related: '%s'. \"\n   854\t                            \"Choices are: %s\" % (\n   855\t                                f.name,\n   856\t                                \", \".join(_get_field_choices()) or '(none)',\n   857\t                            )\n   858\t                        )\n   859\t            else:\n   860\t                next = False\n   861\t\n   862\t            if not select_related_descend(f, restricted, requested,\n   863\t                                          only_load.get(field_model)):\n   864\t                continue\n   865\t            klass_info = {\n   866\t                'model': f.remote_field.model,\n   867\t                'field': f,\n   868\t                'reverse': False,\n   869\t                'local_setter': f.set_cached_value,\n   870\t                'remote_setter': f.remote_field.set_cached_value if f.unique else lambda x, y: None,\n   871\t                'from_parent': False,\n   872\t            }\n   873\t            related_klass_infos.append(klass_info)\n   874\t            select_fields = []\n   875\t            _, _, _, joins, _, _ = self.query.setup_joins(\n   876\t                [f.name], opts, root_alias)\n   877\t            alias = joins[-1]\n   878\t            columns = self.get_default_columns(start_alias=alias, opts=f.remote_field.model._meta)\n   879\t            for col in columns:\n   880\t                select_fields.append(len(select))\n   881\t                select.append((col, None))\n   882\t            klass_info['select_fields'] = select_fields\n   883\t            next_klass_infos = self.get_related_selections(\n   884\t                select, f.remote_field.model._meta, alias, cur_depth + 1, next, restricted)\n   885\t            get_related_klass_infos(klass_info, next_klass_infos)\n   886\t\n   887\t        if restricted:\n   888\t            related_fields = [\n   889\t                (o.field, o.related_model)\n   890\t                for o in opts.related_objects\n   891\t                if o.field.unique and not o.many_to_many\n   892\t            ]\n   893\t            for f, model in related_fields:\n   894\t                if not select_related_descend(f, restricted, requested,\n   895\t                                              only_load.get(model), reverse=True):\n   896\t                    continue\n   897\t\n   898\t                related_field_name = f.related_query_name()\n   899\t                fields_found.add(related_field_name)\n   900\t\n   901\t                join_info = self.query.setup_joins([related_field_name], opts, root_alias)\n   902\t                alias = join_info.joins[-1]\n   903\t                from_parent = issubclass(model, opts.model) and model is not opts.model\n   904\t                klass_info = {\n   905\t                    'model': model,\n   906\t                    'field': f,\n   907\t                    'reverse': True,\n   908\t                    'local_setter': f.remote_field.set_cached_value,\n   909\t                    'remote_setter': f.set_cached_value,\n   910\t                    'from_parent': from_parent,\n   911\t                }\n   912\t                related_klass_infos.append(klass_info)\n   913\t                select_fields = []\n   914\t                columns = self.get_default_columns(\n   915\t                    start_alias=alias, opts=model._meta, from_parent=opts.model)\n   916\t                for col in columns:\n   917\t                    select_fields.append(len(select))\n   918\t                    select.append((col, None))\n   919\t                klass_info['select_fields'] = select_fields\n   920\t                next = requested.get(f.related_query_name(), {})\n   921\t                next_klass_infos = self.get_related_selections(\n   922\t                    select, model._meta, alias, cur_depth + 1,\n   923\t                    next, restricted)\n   924\t                get_related_klass_infos(klass_info, next_klass_infos)\n   925\t\n   926\t            def local_setter(obj, from_obj):\n   927\t                # Set a reverse fk object when relation is non-empty.\n   928\t                if from_obj:\n   929\t                    f.remote_field.set_cached_value(from_obj, obj)\n   930\t\n   931\t            def remote_setter(name, obj, from_obj):\n   932\t                setattr(from_obj, name, obj)\n   933\t\n   934\t            for name in list(requested):\n   935\t                # Filtered relations work only on the topmost level.\n   936\t                if cur_depth > 1:\n   937\t                    break\n   938\t                if name in self.query._filtered_relations:\n   939\t                    fields_found.add(name)\n   940\t                    f, _, join_opts, joins, _, _ = self.query.setup_joins([name], opts, root_alias)\n   941\t                    model = join_opts.model\n   942\t                    alias = joins[-1]\n   943\t                    from_parent = issubclass(model, opts.model) and model is not opts.model\n   944\t                    klass_info = {\n   945\t                        'model': model,\n   946\t                        'field': f,\n   947\t                        'reverse': True,\n   948\t                        'local_setter': local_setter,\n   949\t                        'remote_setter': partial(remote_setter, name),\n   950\t                        'from_parent': from_parent,\n   951\t                    }\n   952\t                    related_klass_infos.append(klass_info)\n   953\t                    select_fields = []\n   954\t                    columns = self.get_default_columns(\n   955\t                        start_alias=alias, opts=model._meta,\n   956\t                        from_parent=opts.model,\n   957\t                    )\n   958\t                    for col in columns:\n   959\t                        select_fields.append(len(select))\n   960\t                        select.append((col, None))\n   961\t                    klass_info['select_fields'] = select_fields\n   962\t                    next_requested = requested.get(name, {})\n   963\t                    next_klass_infos = self.get_related_selections(\n   964\t                        select, opts=model._meta, root_alias=alias,\n   965\t                        cur_depth=cur_depth + 1, requested=next_requested,\n   966\t                        restricted=restricted,\n   967\t                    )\n   968\t                    get_related_klass_infos(klass_info, next_klass_infos)\n   969\t            fields_not_found = set(requested).difference(fields_found)\n   970\t            if fields_not_found:\n   971\t                invalid_fields = (\"'%s'\" % s for s in fields_not_found)\n   972\t                raise FieldError(\n   973\t                    'Invalid field name(s) given in select_related: %s. '\n   974\t                    'Choices are: %s' % (\n   975\t                        ', '.join(invalid_fields),\n   976\t                        ', '.join(_get_field_choices()) or '(none)',\n   977\t                    )\n   978\t                )\n   979\t        return related_klass_infos\n   980\t\n   981\t    def get_select_for_update_of_arguments(self):\n   982\t        \"\"\"\n   983\t        Return a quoted list of arguments for the SELECT FOR UPDATE OF part of\n   984\t        the query.\n   985\t        \"\"\"\n   986\t        def _get_parent_klass_info(klass_info):\n   987\t            for parent_model, parent_link in klass_info['model']._meta.parents.items():\n   988\t                parent_list = parent_model._meta.get_parent_list()\n   989\t                yield {\n   990\t                    'model': parent_model,\n   991\t                    'field': parent_link,\n   992\t                    'reverse': False,\n   993\t                    'select_fields': [\n   994\t                        select_index\n   995\t                        for select_index in klass_info['select_fields']\n   996\t                        # Selected columns from a model or its parents.\n   997\t                        if (\n   998\t                            self.select[select_index][0].target.model == parent_model or\n   999\t                            self.select[select_index][0].target.model in parent_list\n  1000\t                        )\n  1001\t                    ],\n  1002\t                }\n  1003\t\n  1004\t        def _get_first_selected_col_from_model(klass_info):\n  1005\t            \"\"\"\n  1006\t            Find the first selected column from a model. If it doesn't exist,\n  1007\t            don't lock a model.\n  1008\t\n  1009\t            select_fields is filled recursively, so it also contains fields\n  1010\t            from the parent models.\n  1011\t            \"\"\"\n  1012\t            for select_index in klass_info['select_fields']:\n  1013\t                if self.select[select_index][0].target.model == klass_info['model']:\n  1014\t                    return self.select[select_index][0]\n  1015\t\n  1016\t        def _get_field_choices():\n  1017\t            \"\"\"Yield all allowed field paths in breadth-first search order.\"\"\"\n  1018\t            queue = collections.deque([(None, self.klass_info)])\n  1019\t            while queue:\n  1020\t                parent_path, klass_info = queue.popleft()\n  1021\t                if parent_path is None:\n  1022\t                    path = []\n  1023\t                    yield 'self'\n  1024\t                else:\n  1025\t                    field = klass_info['field']\n  1026\t                    if klass_info['reverse']:\n  1027\t                        field = field.remote_field\n  1028\t                    path = parent_path + [field.name]\n  1029\t                    yield LOOKUP_SEP.join(path)\n  1030\t                queue.extend(\n  1031\t                    (path, klass_info)\n  1032\t                    for klass_info in _get_parent_klass_info(klass_info)\n  1033\t                )\n  1034\t                queue.extend(\n  1035\t                    (path, klass_info)\n  1036\t                    for klass_info in klass_info.get('related_klass_infos', [])\n  1037\t                )\n  1038\t        result = []\n  1039\t        invalid_names = []\n  1040\t        for name in self.query.select_for_update_of:\n  1041\t            klass_info = self.klass_info\n  1042\t            if name == 'self':\n  1043\t                col = _get_first_selected_col_from_model(klass_info)\n  1044\t            else:\n  1045\t                for part in name.split(LOOKUP_SEP):\n  1046\t                    klass_infos = (\n  1047\t                        *klass_info.get('related_klass_infos', []),\n  1048\t                        *_get_parent_klass_info(klass_info),\n  1049\t                    )\n  1050\t                    for related_klass_info in klass_infos:\n  1051\t                        field = related_klass_info['field']\n  1052\t                        if related_klass_info['reverse']:\n  1053\t                            field = field.remote_field\n  1054\t                        if field.name == part:\n  1055\t                            klass_info = related_klass_info\n  1056\t                            break\n  1057\t                    else:\n  1058\t                        klass_info = None\n  1059\t                        break\n  1060\t                if klass_info is None:\n  1061\t                    invalid_names.append(name)\n  1062\t                    continue\n  1063\t                col = _get_first_selected_col_from_model(klass_info)\n  1064\t            if col is not None:\n  1065\t                if self.connection.features.select_for_update_of_column:\n  1066\t                    result.append(self.compile(col)[0])\n  1067\t                else:\n  1068\t                    result.append(self.quote_name_unless_alias(col.alias))\n  1069\t        if invalid_names:\n  1070\t            raise FieldError(\n  1071\t                'Invalid field name(s) given in select_for_update(of=(...)): %s. '\n  1072\t                'Only relational fields followed in the query are allowed. '\n  1073\t                'Choices are: %s.' % (\n  1074\t                    ', '.join(invalid_names),\n  1075\t                    ', '.join(_get_field_choices()),\n  1076\t                )\n  1077\t            )\n  1078\t        return result\n  1079\t\n  1080\t    def deferred_to_columns(self):\n  1081\t        \"\"\"\n  1082\t        Convert the self.deferred_loading data structure to mapping of table\n  1083\t        names to sets of column names which are to be loaded. Return the\n  1084\t        dictionary.\n  1085\t        \"\"\"\n  1086\t        columns = {}\n  1087\t        self.query.deferred_to_data(columns, self.query.get_loaded_field_names_cb)\n  1088\t        return columns\n  1089\t\n  1090\t    def get_converters(self, expressions):\n  1091\t        converters = {}\n  1092\t        for i, expression in enumerate(expressions):\n  1093\t            if expression:\n  1094\t                backend_converters = self.connection.ops.get_db_converters(expression)\n  1095\t                field_converters = expression.get_db_converters(self.connection)\n  1096\t                if backend_converters or field_converters:\n  1097\t                    converters[i] = (backend_converters + field_converters, expression)\n  1098\t        return converters\n  1099\t\n  1100\t    def apply_converters(self, rows, converters):\n  1101\t        connection = self.connection\n  1102\t        converters = list(converters.items())\n  1103\t        for row in map(list, rows):\n  1104\t            for pos, (convs, expression) in converters:\n  1105\t                value = row[pos]\n  1106\t                for converter in convs:\n  1107\t                    value = converter(value, expression, connection)\n  1108\t                row[pos] = value\n  1109\t            yield row\n  1110\t\n  1111\t    def results_iter(self, results=None, tuple_expected=False, chunked_fetch=False,\n  1112\t                     chunk_size=GET_ITERATOR_CHUNK_SIZE):\n  1113\t        \"\"\"Return an iterator over the results from executing this query.\"\"\"\n  1114\t        if results is None:\n  1115\t            results = self.execute_sql(MULTI, chunked_fetch=chunked_fetch, chunk_size=chunk_size)\n  1116\t        fields = [s[0] for s in self.select[0:self.col_count]]\n  1117\t        converters = self.get_converters(fields)\n  1118\t        rows = chain.from_iterable(results)\n  1119\t        if converters:\n  1120\t            rows = self.apply_converters(rows, converters)\n  1121\t            if tuple_expected:\n  1122\t                rows = map(tuple, rows)\n  1123\t        return rows\n  1124\t\n  1125\t    def has_results(self):\n  1126\t        \"\"\"\n  1127\t        Backends (e.g. NoSQL) can override this in order to use optimized\n  1128\t        versions of \"query has any results.\"\n  1129\t        \"\"\"\n  1130\t        # This is always executed on a query clone, so we can modify self.query\n  1131\t        self.query.add_extra({'a': 1}, None, None, None, None, None)\n  1132\t        self.query.set_extra_mask(['a'])\n  1133\t        return bool(self.execute_sql(SINGLE))\n  1134\t\n  1135\t    def execute_sql(self, result_type=MULTI, chunked_fetch=False, chunk_size=GET_ITERATOR_CHUNK_SIZE):\n  1136\t        \"\"\"\n  1137\t        Run the query against the database and return the result(s). The\n  1138\t        return value is a single data item if result_type is SINGLE, or an\n  1139\t        iterator over the results if the result_type is MULTI.\n  1140\t\n  1141\t        result_type is either MULTI (use fetchmany() to retrieve all rows),\n  1142\t        SINGLE (only retrieve a single row), or None. In this last case, the\n  1143\t        cursor is returned if any query is executed, since it's used by\n  1144\t        subclasses such as InsertQuery). It's possible, however, that no query\n  1145\t        is needed, as the filters describe an empty set. In that case, None is\n  1146\t        returned, to avoid any unnecessary database interaction.\n  1147\t        \"\"\"\n  1148\t        result_type = result_type or NO_RESULTS\n  1149\t        try:\n  1150\t            sql, params = self.as_sql()\n  1151\t            if not sql:\n  1152\t                raise EmptyResultSet\n  1153\t        except EmptyResultSet:\n  1154\t            if result_type == MULTI:\n  1155\t                return iter([])\n  1156\t            else:\n  1157\t                return\n  1158\t        if chunked_fetch:\n  1159\t            cursor = self.connection.chunked_cursor()\n  1160\t        else:\n  1161\t            cursor = self.connection.cursor()\n  1162\t        try:\n  1163\t            cursor.execute(sql, params)\n  1164\t        except Exception:\n  1165\t            # Might fail for server-side cursors (e.g. connection closed)\n  1166\t            cursor.close()\n  1167\t            raise\n  1168\t\n  1169\t        if result_type == CURSOR:\n  1170\t            # Give the caller the cursor to process and close.\n  1171\t            return cursor\n  1172\t        if result_type == SINGLE:\n  1173\t            try:\n  1174\t                val = cursor.fetchone()\n  1175\t                if val:\n  1176\t                    return val[0:self.col_count]\n  1177\t                return val\n  1178\t            finally:\n  1179\t                # done with the cursor\n  1180\t                cursor.close()\n  1181\t        if result_type == NO_RESULTS:\n  1182\t            cursor.close()\n  1183\t            return\n  1184\t\n  1185\t        result = cursor_iter(\n  1186\t            cursor, self.connection.features.empty_fetchmany_value,\n  1187\t            self.col_count if self.has_extra_select else None,\n  1188\t            chunk_size,\n  1189\t        )\n  1190\t        if not chunked_fetch or not self.connection.features.can_use_chunked_reads:\n  1191\t            try:\n  1192\t                # If we are using non-chunked reads, we return the same data\n  1193\t                # structure as normally, but ensure it is all read into memory\n  1194\t                # before going any further. Use chunked_fetch if requested,\n  1195\t                # unless the database doesn't support it.\n  1196\t                return list(result)\n  1197\t            finally:\n  1198\t                # done with the cursor\n  1199\t                cursor.close()\n  1200\t        return result\n  1201\t\n  1202\t    def as_subquery_condition(self, alias, columns, compiler):\n  1203\t        qn = compiler.quote_name_unless_alias\n  1204\t        qn2 = self.connection.ops.quote_name\n  1205\t\n  1206\t        for index, select_col in enumerate(self.query.select):\n  1207\t            lhs_sql, lhs_params = self.compile(select_col)\n  1208\t            rhs = '%s.%s' % (qn(alias), qn2(columns[index]))\n  1209\t            self.query.where.add(\n  1210\t                RawSQL('%s = %s' % (lhs_sql, rhs), lhs_params), 'AND')\n  1211\t\n  1212\t        sql, params = self.as_sql()\n  1213\t        return 'EXISTS (%s)' % sql, params\n  1214\t\n  1215\t    def explain_query(self):\n  1216\t        result = list(self.execute_sql())\n  1217\t        # Some backends return 1 item tuples with strings, and others return\n  1218\t        # tuples with integers and strings. Flatten them out into strings.\n  1219\t        for row in result[0]:\n  1220\t            if not isinstance(row, str):\n  1221\t                yield ' '.join(str(c) for c in row)\n  1222\t            else:\n  1223\t                yield row\n  1224\t\n  1225\t\n  1226\tclass SQLInsertCompiler(SQLCompiler):\n  1227\t    returning_fields = None\n  1228\t    returning_params = tuple()\n  1229\t\n  1230\t    def field_as_sql(self, field, val):\n  1231\t        \"\"\"\n  1232\t        Take a field and a value intended to be saved on that field, and\n  1233\t        return placeholder SQL and accompanying params. Check for raw values,\n  1234\t        expressions, and fields with get_placeholder() defined in that order.\n  1235\t\n  1236\t        When field is None, consider the value raw and use it as the\n  1237\t        placeholder, with no corresponding parameters returned.\n  1238\t        \"\"\"\n  1239\t        if field is None:\n  1240\t            # A field value of None means the value is raw.\n  1241\t            sql, params = val, []\n  1242\t        elif hasattr(val, 'as_sql'):\n  1243\t            # This is an expression, let's compile it.\n  1244\t            sql, params = self.compile(val)\n  1245\t        elif hasattr(field, 'get_placeholder'):\n  1246\t            # Some fields (e.g. geo fields) need special munging before\n  1247\t            # they can be inserted.\n  1248\t            sql, params = field.get_placeholder(val, self, self.connection), [val]\n  1249\t        else:\n  1250\t            # Return the common case for the placeholder\n  1251\t            sql, params = '%s', [val]\n  1252\t\n  1253\t        # The following hook is only used by Oracle Spatial, which sometimes\n  1254\t        # needs to yield 'NULL' and [] as its placeholder and params instead\n  1255\t        # of '%s' and [None]. The 'NULL' placeholder is produced earlier by\n  1256\t        # OracleOperations.get_geom_placeholder(). The following line removes\n  1257\t        # the corresponding None parameter. See ticket #10888.\n  1258\t        params = self.connection.ops.modify_insert_params(sql, params)\n  1259\t\n  1260\t        return sql, params\n  1261\t\n  1262\t    def prepare_value(self, field, value):\n  1263\t        \"\"\"\n  1264\t        Prepare a value to be used in a query by resolving it if it is an\n  1265\t        expression and otherwise calling the field's get_db_prep_save().\n  1266\t        \"\"\"\n  1267\t        if hasattr(value, 'resolve_expression'):\n  1268\t            value = value.resolve_expression(self.query, allow_joins=False, for_save=True)\n  1269\t            # Don't allow values containing Col expressions. They refer to\n  1270\t            # existing columns on a row, but in the case of insert the row\n  1271\t            # doesn't exist yet.\n  1272\t            if value.contains_column_references:\n  1273\t                raise ValueError(\n  1274\t                    'Failed to insert expression \"%s\" on %s. F() expressions '\n  1275\t                    'can only be used to update, not to insert.' % (value, field)\n  1276\t                )\n  1277\t            if value.contains_aggregate:\n  1278\t                raise FieldError(\n  1279\t                    'Aggregate functions are not allowed in this query '\n  1280\t                    '(%s=%r).' % (field.name, value)\n  1281\t                )\n  1282\t            if value.contains_over_clause:\n  1283\t                raise FieldError(\n  1284\t                    'Window expressions are not allowed in this query (%s=%r).'\n  1285\t                    % (field.name, value)\n  1286\t                )\n  1287\t        else:\n  1288\t            value = field.get_db_prep_save(value, connection=self.connection)\n  1289\t        return value\n  1290\t\n  1291\t    def pre_save_val(self, field, obj):\n  1292\t        \"\"\"\n  1293\t        Get the given field's value off the given obj. pre_save() is used for\n  1294\t        things like auto_now on DateTimeField. Skip it if this is a raw query.\n  1295\t        \"\"\"\n  1296\t        if self.query.raw:\n  1297\t            return getattr(obj, field.attname)\n  1298\t        return field.pre_save(obj, add=True)\n  1299\t\n  1300\t    def assemble_as_sql(self, fields, value_rows):\n  1301\t        \"\"\"\n  1302\t        Take a sequence of N fields and a sequence of M rows of values, and\n  1303\t        generate placeholder SQL and parameters for each field and value.\n  1304\t        Return a pair containing:\n  1305\t         * a sequence of M rows of N SQL placeholder strings, and\n  1306\t         * a sequence of M rows of corresponding parameter values.\n  1307\t\n  1308\t        Each placeholder string may contain any number of '%s' interpolation\n  1309\t        strings, and each parameter row will contain exactly as many params\n  1310\t        as the total number of '%s's in the corresponding placeholder row.\n  1311\t        \"\"\"\n  1312\t        if not value_rows:\n  1313\t            return [], []\n  1314\t\n  1315\t        # list of (sql, [params]) tuples for each object to be saved\n  1316\t        # Shape: [n_objs][n_fields][2]\n  1317\t        rows_of_fields_as_sql = (\n  1318\t            (self.field_as_sql(field, v) for field, v in zip(fields, row))\n  1319\t            for row in value_rows\n  1320\t        )\n  1321\t\n  1322\t        # tuple like ([sqls], [[params]s]) for each object to be saved\n  1323\t        # Shape: [n_objs][2][n_fields]\n  1324\t        sql_and_param_pair_rows = (zip(*row) for row in rows_of_fields_as_sql)\n  1325\t\n  1326\t        # Extract separate lists for placeholders and params.\n  1327\t        # Each of these has shape [n_objs][n_fields]\n  1328\t        placeholder_rows, param_rows = zip(*sql_and_param_pair_rows)\n  1329\t\n  1330\t        # Params for each field are still lists, and need to be flattened.\n  1331\t        param_rows = [[p for ps in row for p in ps] for row in param_rows]\n  1332\t\n  1333\t        return placeholder_rows, param_rows\n  1334\t\n  1335\t    def as_sql(self):\n  1336\t        # We don't need quote_name_unless_alias() here, since these are all\n  1337\t        # going to be column names (so we can avoid the extra overhead).\n  1338\t        qn = self.connection.ops.quote_name\n  1339\t        opts = self.query.get_meta()\n  1340\t        insert_statement = self.connection.ops.insert_statement(ignore_conflicts=self.query.ignore_conflicts)\n  1341\t        result = ['%s %s' % (insert_statement, qn(opts.db_table))]\n  1342\t        fields = self.query.fields or [opts.pk]\n  1343\t        result.append('(%s)' % ', '.join(qn(f.column) for f in fields))\n  1344\t\n  1345\t        if self.query.fields:\n  1346\t            value_rows = [\n  1347\t                [self.prepare_value(field, self.pre_save_val(field, obj)) for field in fields]\n  1348\t                for obj in self.query.objs\n  1349\t            ]\n  1350\t        else:\n  1351\t            # An empty object.\n  1352\t            value_rows = [[self.connection.ops.pk_default_value()] for _ in self.query.objs]\n  1353\t            fields = [None]\n  1354\t\n  1355\t        # Currently the backends just accept values when generating bulk\n  1356\t        # queries and generate their own placeholders. Doing that isn't\n  1357\t        # necessary and it should be possible to use placeholders and\n  1358\t        # expressions in bulk inserts too.\n  1359\t        can_bulk = (not self.returning_fields and self.connection.features.has_bulk_insert)\n  1360\t\n  1361\t        placeholder_rows, param_rows = self.assemble_as_sql(fields, value_rows)\n  1362\t\n  1363\t        ignore_conflicts_suffix_sql = self.connection.ops.ignore_conflicts_suffix_sql(\n  1364\t            ignore_conflicts=self.query.ignore_conflicts\n  1365\t        )\n  1366\t        if self.returning_fields and self.connection.features.can_return_columns_from_insert:\n  1367\t            if self.connection.features.can_return_rows_from_bulk_insert:\n  1368\t                result.append(self.connection.ops.bulk_insert_sql(fields, placeholder_rows))\n  1369\t                params = param_rows\n  1370\t            else:\n  1371\t                result.append(\"VALUES (%s)\" % \", \".join(placeholder_rows[0]))\n  1372\t                params = [param_rows[0]]\n  1373\t            if ignore_conflicts_suffix_sql:\n  1374\t                result.append(ignore_conflicts_suffix_sql)\n  1375\t            # Skip empty r_sql to allow subclasses to customize behavior for\n  1376\t            # 3rd party backends. Refs #19096.\n  1377\t            r_sql, self.returning_params = self.connection.ops.return_insert_columns(self.returning_fields)\n  1378\t            if r_sql:\n  1379\t                result.append(r_sql)\n  1380\t                params += [self.returning_params]\n  1381\t            return [(\" \".join(result), tuple(chain.from_iterable(params)))]\n  1382\t\n  1383\t        if can_bulk:\n  1384\t            result.append(self.connection.ops.bulk_insert_sql(fields, placeholder_rows))\n  1385\t            if ignore_conflicts_suffix_sql:\n  1386\t                result.append(ignore_conflicts_suffix_sql)\n  1387\t            return [(\" \".join(result), tuple(p for ps in param_rows for p in ps))]\n  1388\t        else:\n  1389\t            if ignore_conflicts_suffix_sql:\n  1390\t                result.append(ignore_conflicts_suffix_sql)\n  1391\t            return [\n  1392\t                (\" \".join(result + [\"VALUES (%s)\" % \", \".join(p)]), vals)\n  1393\t                for p, vals in zip(placeholder_rows, param_rows)\n  1394\t            ]\n  1395\t\n  1396\t    def execute_sql(self, returning_fields=None):\n  1397\t        assert not (\n  1398\t            returning_fields and len(self.query.objs) != 1 and\n  1399\t            not self.connection.features.can_return_rows_from_bulk_insert\n  1400\t        )\n  1401\t        self.returning_fields = returning_fields\n  1402\t        with self.connection.cursor() as cursor:\n  1403\t            for sql, params in self.as_sql():\n  1404\t                cursor.execute(sql, params)\n  1405\t            if not self.returning_fields:\n  1406\t                return []\n  1407\t            if self.connection.features.can_return_rows_from_bulk_insert and len(self.query.objs) > 1:\n  1408\t                return self.connection.ops.fetch_returned_insert_rows(cursor)\n  1409\t            if self.connection.features.can_return_columns_from_insert:\n  1410\t                assert len(self.query.objs) == 1\n  1411\t                return self.connection.ops.fetch_returned_insert_columns(cursor, self.returning_params)\n  1412\t            return [self.connection.ops.last_insert_id(\n  1413\t                cursor, self.query.get_meta().db_table, self.query.get_meta().pk.column\n  1414\t            )]\n  1415\t\n  1416\t\n  1417\tclass SQLDeleteCompiler(SQLCompiler):\n  1418\t    @cached_property\n  1419\t    def single_alias(self):\n  1420\t        return sum(self.query.alias_refcount[t] > 0 for t in self.query.alias_map) == 1\n  1421\t\n  1422\t    def _as_sql(self, query):\n  1423\t        result = [\n  1424\t            'DELETE FROM %s' % self.quote_name_unless_alias(query.base_table)\n  1425\t        ]\n  1426\t        where, params = self.compile(query.where)\n  1427\t        if where:\n  1428\t            result.append('WHERE %s' % where)\n  1429\t        return ' '.join(result), tuple(params)\n  1430\t\n  1431\t    def as_sql(self):\n  1432\t        \"\"\"\n  1433\t        Create the SQL for this query. Return the SQL string and list of\n  1434\t        parameters.\n  1435\t        \"\"\"\n  1436\t        if self.single_alias:\n  1437\t            return self._as_sql(self.query)\n  1438\t        innerq = self.query.clone()\n  1439\t        innerq.__class__ = Query\n  1440\t        innerq.clear_select_clause()\n  1441\t        pk = self.query.model._meta.pk\n  1442\t        innerq.select = [\n  1443\t            pk.get_col(self.query.get_initial_alias())\n  1444\t        ]\n  1445\t        outerq = Query(self.query.model)\n  1446\t        outerq.where = self.query.where_class()\n  1447\t        outerq.add_q(Q(pk__in=innerq))\n  1448\t        return self._as_sql(outerq)\n  1449\t\n  1450\t\n  1451\tclass SQLUpdateCompiler(SQLCompiler):\n  1452\t    def as_sql(self):\n  1453\t        \"\"\"\n  1454\t        Create the SQL for this query. Return the SQL string and list of\n  1455\t        parameters.\n  1456\t        \"\"\"\n  1457\t        self.pre_sql_setup()\n  1458\t        if not self.query.values:\n  1459\t            return '', ()\n  1460\t        qn = self.quote_name_unless_alias\n  1461\t        values, update_params = [], []\n  1462\t        for field, model, val in self.query.values:\n  1463\t            if hasattr(val, 'resolve_expression'):\n  1464\t                val = val.resolve_expression(self.query, allow_joins=False, for_save=True)\n  1465\t                if val.contains_aggregate:\n  1466\t                    raise FieldError(\n  1467\t                        'Aggregate functions are not allowed in this query '\n  1468\t                        '(%s=%r).' % (field.name, val)\n  1469\t                    )\n  1470\t                if val.contains_over_clause:\n  1471\t                    raise FieldError(\n  1472\t                        'Window expressions are not allowed in this query '\n  1473\t                        '(%s=%r).' % (field.name, val)\n  1474\t                    )\n  1475\t            elif hasattr(val, 'prepare_database_save'):\n  1476\t                if field.remote_field:\n  1477\t                    val = field.get_db_prep_save(\n  1478\t                        val.prepare_database_save(field),\n  1479\t                        connection=self.connection,\n  1480\t                    )\n  1481\t                else:\n  1482\t                    raise TypeError(\n  1483\t                        \"Tried to update field %s with a model instance, %r. \"\n  1484\t                        \"Use a value compatible with %s.\"\n  1485\t                        % (field, val, field.__class__.__name__)\n  1486\t                    )\n  1487\t            else:\n  1488\t                val = field.get_db_prep_save(val, connection=self.connection)\n  1489\t\n  1490\t            # Getting the placeholder for the field.\n  1491\t            if hasattr(field, 'get_placeholder'):\n  1492\t                placeholder = field.get_placeholder(val, self, self.connection)\n  1493\t            else:\n  1494\t                placeholder = '%s'\n  1495\t            name = field.column\n  1496\t            if hasattr(val, 'as_sql'):\n  1497\t                sql, params = self.compile(val)\n  1498\t                values.append('%s = %s' % (qn(name), placeholder % sql))\n  1499\t                update_params.extend(params)\n  1500\t            elif val is not None:\n  1501\t                values.append('%s = %s' % (qn(name), placeholder))\n  1502\t                update_params.append(val)\n  1503\t            else:\n  1504\t                values.append('%s = NULL' % qn(name))\n  1505\t        table = self.query.base_table\n  1506\t        result = [\n  1507\t            'UPDATE %s SET' % qn(table),\n  1508\t            ', '.join(values),\n  1509\t        ]\n  1510\t        where, params = self.compile(self.query.where)\n  1511\t        if where:\n  1512\t            result.append('WHERE %s' % where)\n  1513\t        return ' '.join(result), tuple(update_params + params)\n  1514\t\n  1515\t    def execute_sql(self, result_type):\n  1516\t        \"\"\"\n  1517\t        Execute the specified update. Return the number of rows affected by\n  1518\t        the primary update query. The \"primary update query\" is the first\n  1519\t        non-empty query that is executed. Row counts for any subsequent,\n  1520\t        related queries are not available.\n  1521\t        \"\"\"\n  1522\t        cursor = super().execute_sql(result_type)\n  1523\t        try:\n  1524\t            rows = cursor.rowcount if cursor else 0\n  1525\t            is_empty = cursor is None\n  1526\t        finally:\n  1527\t            if cursor:\n  1528\t                cursor.close()\n  1529\t        for query in self.query.get_related_updates():\n  1530\t            aux_rows = query.get_compiler(self.using).execute_sql(result_type)\n  1531\t            if is_empty and aux_rows:\n  1532\t                rows = aux_rows\n  1533\t                is_empty = False\n  1534\t        return rows\n  1535\t\n  1536\t    def pre_sql_setup(self):\n  1537\t        \"\"\"\n  1538\t        If the update depends on results from other tables, munge the \"where\"\n  1539\t        conditions to match the format required for (portable) SQL updates.\n  1540\t\n  1541\t        If multiple updates are required, pull out the id values to update at\n  1542\t        this point so that they don't change as a result of the progressive\n  1543\t        updates.\n  1544\t        \"\"\"\n  1545\t        refcounts_before = self.query.alias_refcount.copy()\n  1546\t        # Ensure base table is in the query\n  1547\t        self.query.get_initial_alias()\n  1548\t        count = self.query.count_active_tables()\n  1549\t        if not self.query.related_updates and count == 1:\n  1550\t            return\n  1551\t        query = self.query.chain(klass=Query)\n  1552\t        query.select_related = False\n  1553\t        query.clear_ordering(True)\n  1554\t        query.extra = {}\n  1555\t        query.select = []\n  1556\t        query.add_fields([query.get_meta().pk.name])\n  1557\t        super().pre_sql_setup()\n  1558\t\n  1559\t        must_pre_select = count > 1 and not self.connection.features.update_can_self_select\n  1560\t\n  1561\t        # Now we adjust the current query: reset the where clause and get rid\n  1562\t        # of all the tables we don't need (since they're in the sub-select).\n  1563\t        self.query.where = self.query.where_class()\n  1564\t        if self.query.related_updates or must_pre_select:\n  1565\t            # Either we're using the idents in multiple update queries (so\n  1566\t            # don't want them to change), or the db backend doesn't support\n  1567\t            # selecting from the updating table (e.g. MySQL).\n  1568\t            idents = []\n  1569\t            for rows in query.get_compiler(self.using).execute_sql(MULTI):\n  1570\t                idents.extend(r[0] for r in rows)\n  1571\t            self.query.add_filter(('pk__in', idents))\n  1572\t            self.query.related_ids = idents\n  1573\t        else:\n  1574\t            # The fast path. Filters and updates in one query.\n  1575\t            self.query.add_filter(('pk__in', query))\n  1576\t        self.query.reset_refcounts(refcounts_before)\n  1577\t\n  1578\t\n  1579\tclass SQLAggregateCompiler(SQLCompiler):\n  1580\t    def as_sql(self):\n  1581\t        \"\"\"\n  1582\t        Create the SQL for this query. Return the SQL string and list of\n  1583\t        parameters.\n  1584\t        \"\"\"\n  1585\t        sql, params = [], []\n  1586\t        for annotation in self.query.annotation_select.values():\n  1587\t            ann_sql, ann_params = self.compile(annotation)\n  1588\t            ann_sql, ann_params = annotation.select_format(self, ann_sql, ann_params)\n  1589\t            sql.append(ann_sql)\n  1590\t            params.extend(ann_params)\n  1591\t        self.col_count = len(self.query.annotation_select)\n  1592\t        sql = ', '.join(sql)\n  1593\t        params = tuple(params)\n  1594\t\n  1595\t        sql = 'SELECT %s FROM (%s) subquery' % (sql, self.query.subquery)\n  1596\t        params = params + self.query.sub_params\n  1597\t        return sql, params\n  1598\t\n  1599\t\n  1600\tdef cursor_iter(cursor, sentinel, col_count, itersize):\n  1601\t    \"\"\"\n  1602\t    Yield blocks of rows from a cursor and ensure the cursor is closed when\n  1603\t    done.\n  1604\t    \"\"\"\n  1605\t    try:\n  1606\t        for rows in iter((lambda: cursor.fetchmany(itersize)), sentinel):\n  1607\t            yield rows if col_count is None else [r[:col_count] for r in rows]\n  1608\t    finally:\n  1609\t        cursor.close()\n</document_content>\n</document>\n<document index=\"4\">\n<source>/testbed/test_app/models.py</source>\n<document_content>\n     1\tfrom django.db import models\n     2\t\n     3\tclass Parent(models.Model):\n     4\t    class Meta:\n     5\t        ordering = [\"-pk\"]\n     6\t\n     7\tclass Child(Parent):\n     8\t    pass</document_content>\n</document>\n<document index=\"5\">\n<source>/testbed/django/db/models/base.py</source>\n<document_content>\n     1\timport copy\n     2\timport inspect\n     3\timport warnings\n     4\tfrom functools import partialmethod\n     5\tfrom itertools import chain\n     6\t\n     7\tfrom django.apps import apps\n     8\tfrom django.conf import settings\n     9\tfrom django.core import checks\n    10\tfrom django.core.exceptions import (\n    11\t    NON_FIELD_ERRORS, FieldDoesNotExist, FieldError, MultipleObjectsReturned,\n    12\t    ObjectDoesNotExist, ValidationError,\n    13\t)\n    14\tfrom django.db import (\n    15\t    DEFAULT_DB_ALIAS, DJANGO_VERSION_PICKLE_KEY, DatabaseError, connection,\n    16\t    connections, router, transaction,\n    17\t)\n    18\tfrom django.db.models import (\n    19\t    NOT_PROVIDED, ExpressionWrapper, IntegerField, Max, Value,\n    20\t)\n    21\tfrom django.db.models.constants import LOOKUP_SEP\n    22\tfrom django.db.models.constraints import CheckConstraint\n    23\tfrom django.db.models.deletion import CASCADE, Collector\n    24\tfrom django.db.models.fields.related import (\n    25\t    ForeignObjectRel, OneToOneField, lazy_related_operation, resolve_relation,\n    26\t)\n    27\tfrom django.db.models.functions import Coalesce\n    28\tfrom django.db.models.manager import Manager\n    29\tfrom django.db.models.options import Options\n    30\tfrom django.db.models.query import Q\n    31\tfrom django.db.models.signals import (\n    32\t    class_prepared, post_init, post_save, pre_init, pre_save,\n    33\t)\n    34\tfrom django.db.models.utils import make_model_tuple\n    35\tfrom django.utils.encoding import force_str\n    36\tfrom django.utils.hashable import make_hashable\n    37\tfrom django.utils.text import capfirst, get_text_list\n    38\tfrom django.utils.translation import gettext_lazy as _\n    39\tfrom django.utils.version import get_version\n    40\t\n    41\t\n    42\tclass Deferred:\n    43\t    def __repr__(self):\n    44\t        return '<Deferred field>'\n    45\t\n    46\t    def __str__(self):\n    47\t        return '<Deferred field>'\n    48\t\n    49\t\n    50\tDEFERRED = Deferred()\n    51\t\n    52\t\n    53\tdef subclass_exception(name, bases, module, attached_to):\n    54\t    \"\"\"\n    55\t    Create exception subclass. Used by ModelBase below.\n    56\t\n    57\t    The exception is created in a way that allows it to be pickled, assuming\n    58\t    that the returned exception class will be added as an attribute to the\n    59\t    'attached_to' class.\n    60\t    \"\"\"\n    61\t    return type(name, bases, {\n    62\t        '__module__': module,\n    63\t        '__qualname__': '%s.%s' % (attached_to.__qualname__, name),\n    64\t    })\n    65\t\n    66\t\n    67\tdef _has_contribute_to_class(value):\n    68\t    # Only call contribute_to_class() if it's bound.\n    69\t    return not inspect.isclass(value) and hasattr(value, 'contribute_to_class')\n    70\t\n    71\t\n    72\tclass ModelBase(type):\n    73\t    \"\"\"Metaclass for all models.\"\"\"\n    74\t    def __new__(cls, name, bases, attrs, **kwargs):\n    75\t        super_new = super().__new__\n    76\t\n    77\t        # Also ensure initialization is only performed for subclasses of Model\n    78\t        # (excluding Model class itself).\n    79\t        parents = [b for b in bases if isinstance(b, ModelBase)]\n    80\t        if not parents:\n    81\t            return super_new(cls, name, bases, attrs)\n    82\t\n    83\t        # Create the class.\n    84\t        module = attrs.pop('__module__')\n    85\t        new_attrs = {'__module__': module}\n    86\t        classcell = attrs.pop('__classcell__', None)\n    87\t        if classcell is not None:\n    88\t            new_attrs['__classcell__'] = classcell\n    89\t        attr_meta = attrs.pop('Meta', None)\n    90\t        # Pass all attrs without a (Django-specific) contribute_to_class()\n    91\t        # method to type.__new__() so that they're properly initialized\n    92\t        # (i.e. __set_name__()).\n    93\t        contributable_attrs = {}\n    94\t        for obj_name, obj in list(attrs.items()):\n    95\t            if _has_contribute_to_class(obj):\n    96\t                contributable_attrs[obj_name] = obj\n    97\t            else:\n    98\t                new_attrs[obj_name] = obj\n    99\t        new_class = super_new(cls, name, bases, new_attrs, **kwargs)\n   100\t\n   101\t        abstract = getattr(attr_meta, 'abstract', False)\n   102\t        meta = attr_meta or getattr(new_class, 'Meta', None)\n   103\t        base_meta = getattr(new_class, '_meta', None)\n   104\t\n   105\t        app_label = None\n   106\t\n   107\t        # Look for an application configuration to attach the model to.\n   108\t        app_config = apps.get_containing_app_config(module)\n   109\t\n   110\t        if getattr(meta, 'app_label', None) is None:\n   111\t            if app_config is None:\n   112\t                if not abstract:\n   113\t                    raise RuntimeError(\n   114\t                        \"Model class %s.%s doesn't declare an explicit \"\n   115\t                        \"app_label and isn't in an application in \"\n   116\t                        \"INSTALLED_APPS.\" % (module, name)\n   117\t                    )\n   118\t\n   119\t            else:\n   120\t                app_label = app_config.label\n   121\t\n   122\t        new_class.add_to_class('_meta', Options(meta, app_label))\n   123\t        if not abstract:\n   124\t            new_class.add_to_class(\n   125\t                'DoesNotExist',\n   126\t                subclass_exception(\n   127\t                    'DoesNotExist',\n   128\t                    tuple(\n   129\t                        x.DoesNotExist for x in parents if hasattr(x, '_meta') and not x._meta.abstract\n   130\t                    ) or (ObjectDoesNotExist,),\n   131\t                    module,\n   132\t                    attached_to=new_class))\n   133\t            new_class.add_to_class(\n   134\t                'MultipleObjectsReturned',\n   135\t                subclass_exception(\n   136\t                    'MultipleObjectsReturned',\n   137\t                    tuple(\n   138\t                        x.MultipleObjectsReturned for x in parents if hasattr(x, '_meta') and not x._meta.abstract\n   139\t                    ) or (MultipleObjectsReturned,),\n   140\t                    module,\n   141\t                    attached_to=new_class))\n   142\t            if base_meta and not base_meta.abstract:\n   143\t                # Non-abstract child classes inherit some attributes from their\n   144\t                # non-abstract parent (unless an ABC comes before it in the\n   145\t                # method resolution order).\n   146\t                if not hasattr(meta, 'ordering'):\n   147\t                    # Ensure ordering is properly inherited\n   148\t                    new_class._meta.ordering = list(base_meta.ordering)\n   149\t                if not hasattr(meta, 'get_latest_by'):\n   150\t                    new_class._meta.get_latest_by = base_meta.get_latest_by\n   151\t\n   152\t        is_proxy = new_class._meta.proxy\n   153\t\n   154\t        # If the model is a proxy, ensure that the base class\n   155\t        # hasn't been swapped out.\n   156\t        if is_proxy and base_meta and base_meta.swapped:\n   157\t            raise TypeError(\"%s cannot proxy the swapped model '%s'.\" % (name, base_meta.swapped))\n   158\t\n   159\t        # Add remaining attributes (those with a contribute_to_class() method)\n   160\t        # to the class.\n   161\t        for obj_name, obj in contributable_attrs.items():\n   162\t            new_class.add_to_class(obj_name, obj)\n   163\t\n   164\t        # All the fields of any type declared on this model\n   165\t        new_fields = chain(\n   166\t            new_class._meta.local_fields,\n   167\t            new_class._meta.local_many_to_many,\n   168\t            new_class._meta.private_fields\n   169\t        )\n   170\t        field_names = {f.name for f in new_fields}\n   171\t\n   172\t        # Basic setup for proxy models.\n   173\t        if is_proxy:\n   174\t            base = None\n   175\t            for parent in [kls for kls in parents if hasattr(kls, '_meta')]:\n   176\t                if parent._meta.abstract:\n   177\t                    if parent._meta.fields:\n   178\t                        raise TypeError(\n   179\t                            \"Abstract base class containing model fields not \"\n   180\t                            \"permitted for proxy model '%s'.\" % name\n   181\t                        )\n   182\t                    else:\n   183\t                        continue\n   184\t                if base is None:\n   185\t                    base = parent\n   186\t                elif parent._meta.concrete_model is not base._meta.concrete_model:\n   187\t                    raise TypeError(\"Proxy model '%s' has more than one non-abstract model base class.\" % name)\n   188\t            if base is None:\n   189\t                raise TypeError(\"Proxy model '%s' has no non-abstract model base class.\" % name)\n   190\t            new_class._meta.setup_proxy(base)\n   191\t            new_class._meta.concrete_model = base._meta.concrete_model\n   192\t        else:\n   193\t            new_class._meta.concrete_model = new_class\n   194\t\n   195\t        # Collect the parent links for multi-table inheritance.\n   196\t        parent_links = {}\n   197\t        for base in reversed([new_class] + parents):\n   198\t            # Conceptually equivalent to `if base is Model`.\n   199\t            if not hasattr(base, '_meta'):\n   200\t                continue\n   201\t            # Skip concrete parent classes.\n   202\t            if base != new_class and not base._meta.abstract:\n   203\t                continue\n   204\t            # Locate OneToOneField instances.\n   205\t            for field in base._meta.local_fields:\n   206\t                if isinstance(field, OneToOneField) and field.remote_field.parent_link:\n   207\t                    related = resolve_relation(new_class, field.remote_field.model)\n   208\t                    parent_links[make_model_tuple(related)] = field\n   209\t\n   210\t        # Track fields inherited from base models.\n   211\t        inherited_attributes = set()\n   212\t        # Do the appropriate setup for any model parents.\n   213\t        for base in new_class.mro():\n   214\t            if base not in parents or not hasattr(base, '_meta'):\n   215\t                # Things without _meta aren't functional models, so they're\n   216\t                # uninteresting parents.\n   217\t                inherited_attributes.update(base.__dict__)\n   218\t                continue\n   219\t\n   220\t            parent_fields = base._meta.local_fields + base._meta.local_many_to_many\n   221\t            if not base._meta.abstract:\n   222\t                # Check for clashes between locally declared fields and those\n   223\t                # on the base classes.\n   224\t                for field in parent_fields:\n   225\t                    if field.name in field_names:\n   226\t                        raise FieldError(\n   227\t                            'Local field %r in class %r clashes with field of '\n   228\t                            'the same name from base class %r.' % (\n   229\t                                field.name,\n   230\t                                name,\n   231\t                                base.__name__,\n   232\t                            )\n   233\t                        )\n   234\t                    else:\n   235\t                        inherited_attributes.add(field.name)\n   236\t\n   237\t                # Concrete classes...\n   238\t                base = base._meta.concrete_model\n   239\t                base_key = make_model_tuple(base)\n   240\t                if base_key in parent_links:\n   241\t                    field = parent_links[base_key]\n   242\t                elif not is_proxy:\n   243\t                    attr_name = '%s_ptr' % base._meta.model_name\n   244\t                    field = OneToOneField(\n   245\t                        base,\n   246\t                        on_delete=CASCADE,\n   247\t                        name=attr_name,\n   248\t                        auto_created=True,\n   249\t                        parent_link=True,\n   250\t                    )\n   251\t\n   252\t                    if attr_name in field_names:\n   253\t                        raise FieldError(\n   254\t                            \"Auto-generated field '%s' in class %r for \"\n   255\t                            \"parent_link to base class %r clashes with \"\n   256\t                            \"declared field of the same name.\" % (\n   257\t                                attr_name,\n   258\t                                name,\n   259\t                                base.__name__,\n   260\t                            )\n   261\t                        )\n   262\t\n   263\t                    # Only add the ptr field if it's not already present;\n   264\t                    # e.g. migrations will already have it specified\n   265\t                    if not hasattr(new_class, attr_name):\n   266\t                        new_class.add_to_class(attr_name, field)\n   267\t                else:\n   268\t                    field = None\n   269\t                new_class._meta.parents[base] = field\n   270\t            else:\n   271\t                base_parents = base._meta.parents.copy()\n   272\t\n   273\t                # Add fields from abstract base class if it wasn't overridden.\n   274\t                for field in parent_fields:\n   275\t                    if (field.name not in field_names and\n   276\t                            field.name not in new_class.__dict__ and\n   277\t                            field.name not in inherited_attributes):\n   278\t                        new_field = copy.deepcopy(field)\n   279\t                        new_class.add_to_class(field.name, new_field)\n   280\t                        # Replace parent links defined on this base by the new\n   281\t                        # field. It will be appropriately resolved if required.\n   282\t                        if field.one_to_one:\n   283\t                            for parent, parent_link in base_parents.items():\n   284\t                                if field == parent_link:\n   285\t                                    base_parents[parent] = new_field\n   286\t\n   287\t                # Pass any non-abstract parent classes onto child.\n   288\t                new_class._meta.parents.update(base_parents)\n   289\t\n   290\t            # Inherit private fields (like GenericForeignKey) from the parent\n   291\t            # class\n   292\t            for field in base._meta.private_fields:\n   293\t                if field.name in field_names:\n   294\t                    if not base._meta.abstract:\n   295\t                        raise FieldError(\n   296\t                            'Local field %r in class %r clashes with field of '\n   297\t                            'the same name from base class %r.' % (\n   298\t                                field.name,\n   299\t                                name,\n   300\t                                base.__name__,\n   301\t                            )\n   302\t                        )\n   303\t                else:\n   304\t                    field = copy.deepcopy(field)\n   305\t                    if not base._meta.abstract:\n   306\t                        field.mti_inherited = True\n   307\t                    new_class.add_to_class(field.name, field)\n   308\t\n   309\t        # Copy indexes so that index names are unique when models extend an\n   310\t        # abstract model.\n   311\t        new_class._meta.indexes = [copy.deepcopy(idx) for idx in new_class._meta.indexes]\n   312\t\n   313\t        if abstract:\n   314\t            # Abstract base models can't be instantiated and don't appear in\n   315\t            # the list of models for an app. We do the final setup for them a\n   316\t            # little differently from normal models.\n   317\t            attr_meta.abstract = False\n   318\t            new_class.Meta = attr_meta\n   319\t            return new_class\n   320\t\n   321\t        new_class._prepare()\n   322\t        new_class._meta.apps.register_model(new_class._meta.app_label, new_class)\n   323\t        return new_class\n   324\t\n   325\t    def add_to_class(cls, name, value):\n   326\t        if _has_contribute_to_class(value):\n   327\t            value.contribute_to_class(cls, name)\n   328\t        else:\n   329\t            setattr(cls, name, value)\n   330\t\n   331\t    def _prepare(cls):\n   332\t        \"\"\"Create some methods once self._meta has been populated.\"\"\"\n   333\t        opts = cls._meta\n   334\t        opts._prepare(cls)\n   335\t\n   336\t        if opts.order_with_respect_to:\n   337\t            cls.get_next_in_order = partialmethod(cls._get_next_or_previous_in_order, is_next=True)\n   338\t            cls.get_previous_in_order = partialmethod(cls._get_next_or_previous_in_order, is_next=False)\n   339\t\n   340\t            # Defer creating accessors on the foreign class until it has been\n   341\t            # created and registered. If remote_field is None, we're ordering\n   342\t            # with respect to a GenericForeignKey and don't know what the\n   343\t            # foreign class is - we'll add those accessors later in\n   344\t            # contribute_to_class().\n   345\t            if opts.order_with_respect_to.remote_field:\n   346\t                wrt = opts.order_with_respect_to\n   347\t                remote = wrt.remote_field.model\n   348\t                lazy_related_operation(make_foreign_order_accessors, cls, remote)\n   349\t\n   350\t        # Give the class a docstring -- its definition.\n   351\t        if cls.__doc__ is None:\n   352\t            cls.__doc__ = \"%s(%s)\" % (cls.__name__, \", \".join(f.name for f in opts.fields))\n   353\t\n   354\t        get_absolute_url_override = settings.ABSOLUTE_URL_OVERRIDES.get(opts.label_lower)\n   355\t        if get_absolute_url_override:\n   356\t            setattr(cls, 'get_absolute_url', get_absolute_url_override)\n   357\t\n   358\t        if not opts.managers:\n   359\t            if any(f.name == 'objects' for f in opts.fields):\n   360\t                raise ValueError(\n   361\t                    \"Model %s must specify a custom Manager, because it has a \"\n   362\t                    \"field named 'objects'.\" % cls.__name__\n   363\t                )\n   364\t            manager = Manager()\n   365\t            manager.auto_created = True\n   366\t            cls.add_to_class('objects', manager)\n   367\t\n   368\t        # Set the name of _meta.indexes. This can't be done in\n   369\t        # Options.contribute_to_class() because fields haven't been added to\n   370\t        # the model at that point.\n   371\t        for index in cls._meta.indexes:\n   372\t            if not index.name:\n   373\t                index.set_name_with_model(cls)\n   374\t\n   375\t        class_prepared.send(sender=cls)\n   376\t\n   377\t    @property\n   378\t    def _base_manager(cls):\n   379\t        return cls._meta.base_manager\n   380\t\n   381\t    @property\n   382\t    def _default_manager(cls):\n   383\t        return cls._meta.default_manager\n   384\t\n   385\t\n   386\tclass ModelStateFieldsCacheDescriptor:\n   387\t    def __get__(self, instance, cls=None):\n   388\t        if instance is None:\n   389\t            return self\n   390\t        res = instance.fields_cache = {}\n   391\t        return res\n   392\t\n   393\t\n   394\tclass ModelState:\n   395\t    \"\"\"Store model instance state.\"\"\"\n   396\t    db = None\n   397\t    # If true, uniqueness validation checks will consider this a new, unsaved\n   398\t    # object. Necessary for correct validation of new instances of objects with\n   399\t    # explicit (non-auto) PKs. This impacts validation only; it has no effect\n   400\t    # on the actual save.\n   401\t    adding = True\n   402\t    fields_cache = ModelStateFieldsCacheDescriptor()\n   403\t\n   404\t\n   405\tclass Model(metaclass=ModelBase):\n   406\t\n   407\t    def __init__(self, *args, **kwargs):\n   408\t        # Alias some things as locals to avoid repeat global lookups\n   409\t        cls = self.__class__\n   410\t        opts = self._meta\n   411\t        _setattr = setattr\n   412\t        _DEFERRED = DEFERRED\n   413\t\n   414\t        pre_init.send(sender=cls, args=args, kwargs=kwargs)\n   415\t\n   416\t        # Set up the storage for instance state\n   417\t        self._state = ModelState()\n   418\t\n   419\t        # There is a rather weird disparity here; if kwargs, it's set, then args\n   420\t        # overrides it. It should be one or the other; don't duplicate the work\n   421\t        # The reason for the kwargs check is that standard iterator passes in by\n   422\t        # args, and instantiation for iteration is 33% faster.\n   423\t        if len(args) > len(opts.concrete_fields):\n   424\t            # Daft, but matches old exception sans the err msg.\n   425\t            raise IndexError(\"Number of args exceeds number of fields\")\n   426\t\n   427\t        if not kwargs:\n   428\t            fields_iter = iter(opts.concrete_fields)\n   429\t            # The ordering of the zip calls matter - zip throws StopIteration\n   430\t            # when an iter throws it. So if the first iter throws it, the second\n   431\t            # is *not* consumed. We rely on this, so don't change the order\n   432\t            # without changing the logic.\n   433\t            for val, field in zip(args, fields_iter):\n   434\t                if val is _DEFERRED:\n   435\t                    continue\n   436\t                _setattr(self, field.attname, val)\n   437\t        else:\n   438\t            # Slower, kwargs-ready version.\n   439\t            fields_iter = iter(opts.fields)\n   440\t            for val, field in zip(args, fields_iter):\n   441\t                if val is _DEFERRED:\n   442\t                    continue\n   443\t                _setattr(self, field.attname, val)\n   444\t                kwargs.pop(field.name, None)\n   445\t\n   446\t        # Now we're left with the unprocessed fields that *must* come from\n   447\t        # keywords, or default.\n   448\t\n   449\t        for field in fields_iter:\n   450\t            is_related_object = False\n   451\t            # Virtual field\n   452\t            if field.attname not in kwargs and field.column is None:\n   453\t                continue\n   454\t            if kwargs:\n   455\t                if isinstance(field.remote_field, ForeignObjectRel):\n   456\t                    try:\n   457\t                        # Assume object instance was passed in.\n   458\t                        rel_obj = kwargs.pop(field.name)\n   459\t                        is_related_object = True\n   460\t                    except KeyError:\n   461\t                        try:\n   462\t                            # Object instance wasn't passed in -- must be an ID.\n   463\t                            val = kwargs.pop(field.attname)\n   464\t                        except KeyError:\n   465\t                            val = field.get_default()\n   466\t                else:\n   467\t                    try:\n   468\t                        val = kwargs.pop(field.attname)\n   469\t                    except KeyError:\n   470\t                        # This is done with an exception rather than the\n   471\t                        # default argument on pop because we don't want\n   472\t                        # get_default() to be evaluated, and then not used.\n   473\t                        # Refs #12057.\n   474\t                        val = field.get_default()\n   475\t            else:\n   476\t                val = field.get_default()\n   477\t\n   478\t            if is_related_object:\n   479\t                # If we are passed a related instance, set it using the\n   480\t                # field.name instead of field.attname (e.g. \"user\" instead of\n   481\t                # \"user_id\") so that the object gets properly cached (and type\n   482\t                # checked) by the RelatedObjectDescriptor.\n   483\t                if rel_obj is not _DEFERRED:\n   484\t                    _setattr(self, field.name, rel_obj)\n   485\t            else:\n   486\t                if val is not _DEFERRED:\n   487\t                    _setattr(self, field.attname, val)\n   488\t\n   489\t        if kwargs:\n   490\t            property_names = opts._property_names\n   491\t            for prop in tuple(kwargs):\n   492\t                try:\n   493\t                    # Any remaining kwargs must correspond to properties or\n   494\t                    # virtual fields.\n   495\t                    if prop in property_names or opts.get_field(prop):\n   496\t                        if kwargs[prop] is not _DEFERRED:\n   497\t                            _setattr(self, prop, kwargs[prop])\n   498\t                        del kwargs[prop]\n   499\t                except (AttributeError, FieldDoesNotExist):\n   500\t                    pass\n   501\t            for kwarg in kwargs:\n   502\t                raise TypeError(\"%s() got an unexpected keyword argument '%s'\" % (cls.__name__, kwarg))\n   503\t        super().__init__()\n   504\t        post_init.send(sender=cls, instance=self)\n   505\t\n   506\t    @classmethod\n   507\t    def from_db(cls, db, field_names, values):\n   508\t        if len(values) != len(cls._meta.concrete_fields):\n   509\t            values_iter = iter(values)\n   510\t            values = [\n   511\t                next(values_iter) if f.attname in field_names else DEFERRED\n   512\t                for f in cls._meta.concrete_fields\n   513\t            ]\n   514\t        new = cls(*values)\n   515\t        new._state.adding = False\n   516\t        new._state.db = db\n   517\t        return new\n   518\t\n   519\t    def __repr__(self):\n   520\t        return '<%s: %s>' % (self.__class__.__name__, self)\n   521\t\n   522\t    def __str__(self):\n   523\t        return '%s object (%s)' % (self.__class__.__name__, self.pk)\n   524\t\n   525\t    def __eq__(self, other):\n   526\t        if not isinstance(other, Model):\n   527\t            return NotImplemented\n   528\t        if self._meta.concrete_model != other._meta.concrete_model:\n   529\t            return False\n   530\t        my_pk = self.pk\n   531\t        if my_pk is None:\n   532\t            return self is other\n   533\t        return my_pk == other.pk\n   534\t\n   535\t    def __hash__(self):\n   536\t        if self.pk is None:\n   537\t            raise TypeError(\"Model instances without primary key value are unhashable\")\n   538\t        return hash(self.pk)\n   539\t\n   540\t    def __reduce__(self):\n   541\t        data = self.__getstate__()\n   542\t        data[DJANGO_VERSION_PICKLE_KEY] = get_version()\n   543\t        class_id = self._meta.app_label, self._meta.object_name\n   544\t        return model_unpickle, (class_id,), data\n   545\t\n   546\t    def __getstate__(self):\n   547\t        \"\"\"Hook to allow choosing the attributes to pickle.\"\"\"\n   548\t        return self.__dict__\n   549\t\n   550\t    def __setstate__(self, state):\n   551\t        msg = None\n   552\t        pickled_version = state.get(DJANGO_VERSION_PICKLE_KEY)\n   553\t        if pickled_version:\n   554\t            current_version = get_version()\n   555\t            if current_version != pickled_version:\n   556\t                msg = (\n   557\t                    \"Pickled model instance's Django version %s does not match \"\n   558\t                    \"the current version %s.\" % (pickled_version, current_version)\n   559\t                )\n   560\t        else:\n   561\t            msg = \"Pickled model instance's Django version is not specified.\"\n   562\t\n   563\t        if msg:\n   564\t            warnings.warn(msg, RuntimeWarning, stacklevel=2)\n   565\t\n   566\t        self.__dict__.update(state)\n   567\t\n   568\t    def _get_pk_val(self, meta=None):\n   569\t        meta = meta or self._meta\n   570\t        return getattr(self, meta.pk.attname)\n   571\t\n   572\t    def _set_pk_val(self, value):\n   573\t        for parent_link in self._meta.parents.values():\n   574\t            if parent_link and parent_link != self._meta.pk:\n   575\t                setattr(self, parent_link.target_field.attname, value)\n   576\t        return setattr(self, self._meta.pk.attname, value)\n   577\t\n   578\t    pk = property(_get_pk_val, _set_pk_val)\n   579\t\n   580\t    def get_deferred_fields(self):\n   581\t        \"\"\"\n   582\t        Return a set containing names of deferred fields for this instance.\n   583\t        \"\"\"\n   584\t        return {\n   585\t            f.attname for f in self._meta.concrete_fields\n   586\t            if f.attname not in self.__dict__\n   587\t        }\n   588\t\n   589\t    def refresh_from_db(self, using=None, fields=None):\n   590\t        \"\"\"\n   591\t        Reload field values from the database.\n   592\t\n   593\t        By default, the reloading happens from the database this instance was\n   594\t        loaded from, or by the read router if this instance wasn't loaded from\n   595\t        any database. The using parameter will override the default.\n   596\t\n   597\t        Fields can be used to specify which fields to reload. The fields\n   598\t        should be an iterable of field attnames. If fields is None, then\n   599\t        all non-deferred fields are reloaded.\n   600\t\n   601\t        When accessing deferred fields of an instance, the deferred loading\n   602\t        of the field will call this method.\n   603\t        \"\"\"\n   604\t        if fields is None:\n   605\t            self._prefetched_objects_cache = {}\n   606\t        else:\n   607\t            prefetched_objects_cache = getattr(self, '_prefetched_objects_cache', ())\n   608\t            for field in fields:\n   609\t                if field in prefetched_objects_cache:\n   610\t                    del prefetched_objects_cache[field]\n   611\t                    fields.remove(field)\n   612\t            if not fields:\n   613\t                return\n   614\t            if any(LOOKUP_SEP in f for f in fields):\n   615\t                raise ValueError(\n   616\t                    'Found \"%s\" in fields argument. Relations and transforms '\n   617\t                    'are not allowed in fields.' % LOOKUP_SEP)\n   618\t\n   619\t        hints = {'instance': self}\n   620\t        db_instance_qs = self.__class__._base_manager.db_manager(using, hints=hints).filter(pk=self.pk)\n   621\t\n   622\t        # Use provided fields, if not set then reload all non-deferred fields.\n   623\t        deferred_fields = self.get_deferred_fields()\n   624\t        if fields is not None:\n   625\t            fields = list(fields)\n   626\t            db_instance_qs = db_instance_qs.only(*fields)\n   627\t        elif deferred_fields:\n   628\t            fields = [f.attname for f in self._meta.concrete_fields\n   629\t                      if f.attname not in deferred_fields]\n   630\t            db_instance_qs = db_instance_qs.only(*fields)\n   631\t\n   632\t        db_instance = db_instance_qs.get()\n   633\t        non_loaded_fields = db_instance.get_deferred_fields()\n   634\t        for field in self._meta.concrete_fields:\n   635\t            if field.attname in non_loaded_fields:\n   636\t                # This field wasn't refreshed - skip ahead.\n   637\t                continue\n   638\t            setattr(self, field.attname, getattr(db_instance, field.attname))\n   639\t            # Clear cached foreign keys.\n   640\t            if field.is_relation and field.is_cached(self):\n   641\t                field.delete_cached_value(self)\n   642\t\n   643\t        # Clear cached relations.\n   644\t        for field in self._meta.related_objects:\n   645\t            if field.is_cached(self):\n   646\t                field.delete_cached_value(self)\n   647\t\n   648\t        self._state.db = db_instance._state.db\n   649\t\n   650\t    def serializable_value(self, field_name):\n   651\t        \"\"\"\n   652\t        Return the value of the field name for this instance. If the field is\n   653\t        a foreign key, return the id value instead of the object. If there's\n   654\t        no Field object with this name on the model, return the model\n   655\t        attribute's value.\n   656\t\n   657\t        Used to serialize a field's value (in the serializer, or form output,\n   658\t        for example). Normally, you would just access the attribute directly\n   659\t        and not use this method.\n   660\t        \"\"\"\n   661\t        try:\n   662\t            field = self._meta.get_field(field_name)\n   663\t        except FieldDoesNotExist:\n   664\t            return getattr(self, field_name)\n   665\t        return getattr(self, field.attname)\n   666\t\n   667\t    def save(self, force_insert=False, force_update=False, using=None,\n   668\t             update_fields=None):\n   669\t        \"\"\"\n   670\t        Save the current instance. Override this in a subclass if you want to\n   671\t        control the saving process.\n   672\t\n   673\t        The 'force_insert' and 'force_update' parameters can be used to insist\n   674\t        that the \"save\" must be an SQL insert or update (or equivalent for\n   675\t        non-SQL backends), respectively. Normally, they should not be set.\n   676\t        \"\"\"\n   677\t        # Ensure that a model instance without a PK hasn't been assigned to\n   678\t        # a ForeignKey or OneToOneField on this model. If the field is\n   679\t        # nullable, allowing the save() would result in silent data loss.\n   680\t        for field in self._meta.concrete_fields:\n   681\t            # If the related field isn't cached, then an instance hasn't\n   682\t            # been assigned and there's no need to worry about this check.\n   683\t            if field.is_relation and field.is_cached(self):\n   684\t                obj = getattr(self, field.name, None)\n   685\t                if not obj:\n   686\t                    continue\n   687\t                # A pk may have been assigned manually to a model instance not\n   688\t                # saved to the database (or auto-generated in a case like\n   689\t                # UUIDField), but we allow the save to proceed and rely on the\n   690\t                # database to raise an IntegrityError if applicable. If\n   691\t                # constraints aren't supported by the database, there's the\n   692\t                # unavoidable risk of data corruption.\n   693\t                if obj.pk is None:\n   694\t                    # Remove the object from a related instance cache.\n   695\t                    if not field.remote_field.multiple:\n   696\t                        field.remote_field.delete_cached_value(obj)\n   697\t                    raise ValueError(\n   698\t                        \"save() prohibited to prevent data loss due to \"\n   699\t                        \"unsaved related object '%s'.\" % field.name\n   700\t                    )\n   701\t                elif getattr(self, field.attname) is None:\n   702\t                    # Use pk from related object if it has been saved after\n   703\t                    # an assignment.\n   704\t                    setattr(self, field.attname, obj.pk)\n   705\t                # If the relationship's pk/to_field was changed, clear the\n   706\t                # cached relationship.\n   707\t                if getattr(obj, field.target_field.attname) != getattr(self, field.attname):\n   708\t                    field.delete_cached_value(self)\n   709\t\n   710\t        using = using or router.db_for_write(self.__class__, instance=self)\n   711\t        if force_insert and (force_update or update_fields):\n   712\t            raise ValueError(\"Cannot force both insert and updating in model saving.\")\n   713\t\n   714\t        deferred_fields = self.get_deferred_fields()\n   715\t        if update_fields is not None:\n   716\t            # If update_fields is empty, skip the save. We do also check for\n   717\t            # no-op saves later on for inheritance cases. This bailout is\n   718\t            # still needed for skipping signal sending.\n   719\t            if not update_fields:\n   720\t                return\n   721\t\n   722\t            update_fields = frozenset(update_fields)\n   723\t            field_names = set()\n   724\t\n   725\t            for field in self._meta.fields:\n   726\t                if not field.primary_key:\n   727\t                    field_names.add(field.name)\n   728\t\n   729\t                    if field.name != field.attname:\n   730\t                        field_names.add(field.attname)\n   731\t\n   732\t            non_model_fields = update_fields.difference(field_names)\n   733\t\n   734\t            if non_model_fields:\n   735\t                raise ValueError(\"The following fields do not exist in this \"\n   736\t                                 \"model or are m2m fields: %s\"\n   737\t                                 % ', '.join(non_model_fields))\n   738\t\n   739\t        # If saving to the same database, and this model is deferred, then\n   740\t        # automatically do an \"update_fields\" save on the loaded fields.\n   741\t        elif not force_insert and deferred_fields and using == self._state.db:\n   742\t            field_names = set()\n   743\t            for field in self._meta.concrete_fields:\n   744\t                if not field.primary_key and not hasattr(field, 'through'):\n   745\t                    field_names.add(field.attname)\n   746\t            loaded_fields = field_names.difference(deferred_fields)\n   747\t            if loaded_fields:\n   748\t                update_fields = frozenset(loaded_fields)\n   749\t\n   750\t        self.save_base(using=using, force_insert=force_insert,\n   751\t                       force_update=force_update, update_fields=update_fields)\n   752\t    save.alters_data = True\n   753\t\n   754\t    def save_base(self, raw=False, force_insert=False,\n   755\t                  force_update=False, using=None, update_fields=None):\n   756\t        \"\"\"\n   757\t        Handle the parts of saving which should be done only once per save,\n   758\t        yet need to be done in raw saves, too. This includes some sanity\n   759\t        checks and signal sending.\n   760\t\n   761\t        The 'raw' argument is telling save_base not to save any parent\n   762\t        models and not to do any changes to the values before save. This\n   763\t        is used by fixture loading.\n   764\t        \"\"\"\n   765\t        using = using or router.db_for_write(self.__class__, instance=self)\n   766\t        assert not (force_insert and (force_update or update_fields))\n   767\t        assert update_fields is None or update_fields\n   768\t        cls = origin = self.__class__\n   769\t        # Skip proxies, but keep the origin as the proxy model.\n   770\t        if cls._meta.proxy:\n   771\t            cls = cls._meta.concrete_model\n   772\t        meta = cls._meta\n   773\t        if not meta.auto_created:\n   774\t            pre_save.send(\n   775\t                sender=origin, instance=self, raw=raw, using=using,\n   776\t                update_fields=update_fields,\n   777\t            )\n   778\t        # A transaction isn't needed if one query is issued.\n   779\t        if meta.parents:\n   780\t            context_manager = transaction.atomic(using=using, savepoint=False)\n   781\t        else:\n   782\t            context_manager = transaction.mark_for_rollback_on_error(using=using)\n   783\t        with context_manager:\n   784\t            parent_inserted = False\n   785\t            if not raw:\n   786\t                parent_inserted = self._save_parents(cls, using, update_fields)\n   787\t            updated = self._save_table(\n   788\t                raw, cls, force_insert or parent_inserted,\n   789\t                force_update, using, update_fields,\n   790\t            )\n   791\t        # Store the database on which the object was saved\n   792\t        self._state.db = using\n   793\t        # Once saved, this is no longer a to-be-added instance.\n   794\t        self._state.adding = False\n   795\t\n   796\t        # Signal that the save is complete\n   797\t        if not meta.auto_created:\n   798\t            post_save.send(\n   799\t                sender=origin, instance=self, created=(not updated),\n   800\t                update_fields=update_fields, raw=raw, using=using,\n   801\t            )\n   802\t\n   803\t    save_base.alters_data = True\n   804\t\n   805\t    def _save_parents(self, cls, using, update_fields):\n   806\t        \"\"\"Save all the parents of cls using values from self.\"\"\"\n   807\t        meta = cls._meta\n   808\t        inserted = False\n   809\t        for parent, field in meta.parents.items():\n   810\t            # Make sure the link fields are synced between parent and self.\n   811\t            if (field and getattr(self, parent._meta.pk.attname) is None and\n   812\t                    getattr(self, field.attname) is not None):\n   813\t                setattr(self, parent._meta.pk.attname, getattr(self, field.attname))\n   814\t            parent_inserted = self._save_parents(cls=parent, using=using, update_fields=update_fields)\n   815\t            updated = self._save_table(\n   816\t                cls=parent, using=using, update_fields=update_fields,\n   817\t                force_insert=parent_inserted,\n   818\t            )\n   819\t            if not updated:\n   820\t                inserted = True\n   821\t            # Set the parent's PK value to self.\n   822\t            if field:\n   823\t                setattr(self, field.attname, self._get_pk_val(parent._meta))\n   824\t                # Since we didn't have an instance of the parent handy set\n   825\t                # attname directly, bypassing the descriptor. Invalidate\n   826\t                # the related object cache, in case it's been accidentally\n   827\t                # populated. A fresh instance will be re-built from the\n   828\t                # database if necessary.\n   829\t                if field.is_cached(self):\n   830\t                    field.delete_cached_value(self)\n   831\t        return inserted\n   832\t\n   833\t    def _save_table(self, raw=False, cls=None, force_insert=False,\n   834\t                    force_update=False, using=None, update_fields=None):\n   835\t        \"\"\"\n   836\t        Do the heavy-lifting involved in saving. Update or insert the data\n   837\t        for a single table.\n   838\t        \"\"\"\n   839\t        meta = cls._meta\n   840\t        non_pks = [f for f in meta.local_concrete_fields if not f.primary_key]\n   841\t\n   842\t        if update_fields:\n   843\t            non_pks = [f for f in non_pks\n   844\t                       if f.name in update_fields or f.attname in update_fields]\n   845\t\n   846\t        pk_val = self._get_pk_val(meta)\n   847\t        if pk_val is None:\n   848\t            pk_val = meta.pk.get_pk_value_on_save(self)\n   849\t            setattr(self, meta.pk.attname, pk_val)\n   850\t        pk_set = pk_val is not None\n   851\t        if not pk_set and (force_update or update_fields):\n   852\t            raise ValueError(\"Cannot force an update in save() with no primary key.\")\n   853\t        updated = False\n   854\t        # Skip an UPDATE when adding an instance and primary key has a default.\n   855\t        if (\n   856\t            not raw and\n   857\t            not force_insert and\n   858\t            self._state.adding and\n   859\t            meta.pk.default and\n   860\t            meta.pk.default is not NOT_PROVIDED\n   861\t        ):\n   862\t            force_insert = True\n   863\t        # If possible, try an UPDATE. If that doesn't update anything, do an INSERT.\n   864\t        if pk_set and not force_insert:\n   865\t            base_qs = cls._base_manager.using(using)\n   866\t            values = [(f, None, (getattr(self, f.attname) if raw else f.pre_save(self, False)))\n   867\t                      for f in non_pks]\n   868\t            forced_update = update_fields or force_update\n   869\t            updated = self._do_update(base_qs, using, pk_val, values, update_fields,\n   870\t                                      forced_update)\n   871\t            if force_update and not updated:\n   872\t                raise DatabaseError(\"Forced update did not affect any rows.\")\n   873\t            if update_fields and not updated:\n   874\t                raise DatabaseError(\"Save with update_fields did not affect any rows.\")\n   875\t        if not updated:\n   876\t            if meta.order_with_respect_to:\n   877\t                # If this is a model with an order_with_respect_to\n   878\t                # autopopulate the _order field\n   879\t                field = meta.order_with_respect_to\n   880\t                filter_args = field.get_filter_kwargs_for_object(self)\n   881\t                self._order = cls._base_manager.using(using).filter(**filter_args).aggregate(\n   882\t                    _order__max=Coalesce(\n   883\t                        ExpressionWrapper(Max('_order') + Value(1), output_field=IntegerField()),\n   884\t                        Value(0),\n   885\t                    ),\n   886\t                )['_order__max']\n   887\t            fields = meta.local_concrete_fields\n   888\t            if not pk_set:\n   889\t                fields = [f for f in fields if f is not meta.auto_field]\n   890\t\n   891\t            returning_fields = meta.db_returning_fields\n   892\t            results = self._do_insert(cls._base_manager, using, fields, returning_fields, raw)\n   893\t            for result, field in zip(results, returning_fields):\n   894\t                setattr(self, field.attname, result)\n   895\t        return updated\n   896\t\n   897\t    def _do_update(self, base_qs, using, pk_val, values, update_fields, forced_update):\n   898\t        \"\"\"\n   899\t        Try to update the model. Return True if the model was updated (if an\n   900\t        update query was done and a matching row was found in the DB).\n   901\t        \"\"\"\n   902\t        filtered = base_qs.filter(pk=pk_val)\n   903\t        if not values:\n   904\t            # We can end up here when saving a model in inheritance chain where\n   905\t            # update_fields doesn't target any field in current model. In that\n   906\t            # case we just say the update succeeded. Another case ending up here\n   907\t            # is a model with just PK - in that case check that the PK still\n   908\t            # exists.\n   909\t            return update_fields is not None or filtered.exists()\n   910\t        if self._meta.select_on_save and not forced_update:\n   911\t            return (\n   912\t                filtered.exists() and\n   913\t                # It may happen that the object is deleted from the DB right after\n   914\t                # this check, causing the subsequent UPDATE to return zero matching\n   915\t                # rows. The same result can occur in some rare cases when the\n   916\t                # database returns zero despite the UPDATE being executed\n   917\t                # successfully (a row is matched and updated). In order to\n   918\t                # distinguish these two cases, the object's existence in the\n   919\t                # database is again checked for if the UPDATE query returns 0.\n   920\t                (filtered._update(values) > 0 or filtered.exists())\n   921\t            )\n   922\t        return filtered._update(values) > 0\n   923\t\n   924\t    def _do_insert(self, manager, using, fields, returning_fields, raw):\n   925\t        \"\"\"\n   926\t        Do an INSERT. If returning_fields is defined then this method should\n   927\t        return the newly created data for the model.\n   928\t        \"\"\"\n   929\t        return manager._insert(\n   930\t            [self], fields=fields, returning_fields=returning_fields,\n   931\t            using=using, raw=raw,\n   932\t        )\n   933\t\n   934\t    def delete(self, using=None, keep_parents=False):\n   935\t        using = using or router.db_for_write(self.__class__, instance=self)\n   936\t        assert self.pk is not None, (\n   937\t            \"%s object can't be deleted because its %s attribute is set to None.\" %\n   938\t            (self._meta.object_name, self._meta.pk.attname)\n   939\t        )\n   940\t\n   941\t        collector = Collector(using=using)\n   942\t        collector.collect([self], keep_parents=keep_parents)\n   943\t        return collector.delete()\n   944\t\n   945\t    delete.alters_data = True\n   946\t\n   947\t    def _get_FIELD_display(self, field):\n   948\t        value = getattr(self, field.attname)\n   949\t        choices_dict = dict(make_hashable(field.flatchoices))\n   950\t        # force_str() to coerce lazy strings.\n   951\t        return force_str(choices_dict.get(make_hashable(value), value), strings_only=True)\n   952\t\n   953\t    def _get_next_or_previous_by_FIELD(self, field, is_next, **kwargs):\n   954\t        if not self.pk:\n   955\t            raise ValueError(\"get_next/get_previous cannot be used on unsaved objects.\")\n   956\t        op = 'gt' if is_next else 'lt'\n   957\t        order = '' if is_next else '-'\n   958\t        param = getattr(self, field.attname)\n   959\t        q = Q(**{'%s__%s' % (field.name, op): param})\n   960\t        q = q | Q(**{field.name: param, 'pk__%s' % op: self.pk})\n   961\t        qs = self.__class__._default_manager.using(self._state.db).filter(**kwargs).filter(q).order_by(\n   962\t            '%s%s' % (order, field.name), '%spk' % order\n   963\t        )\n   964\t        try:\n   965\t            return qs[0]\n   966\t        except IndexError:\n   967\t            raise self.DoesNotExist(\"%s matching query does not exist.\" % self.__class__._meta.object_name)\n   968\t\n   969\t    def _get_next_or_previous_in_order(self, is_next):\n   970\t        cachename = \"__%s_order_cache\" % is_next\n   971\t        if not hasattr(self, cachename):\n   972\t            op = 'gt' if is_next else 'lt'\n   973\t            order = '_order' if is_next else '-_order'\n   974\t            order_field = self._meta.order_with_respect_to\n   975\t            filter_args = order_field.get_filter_kwargs_for_object(self)\n   976\t            obj = self.__class__._default_manager.filter(**filter_args).filter(**{\n   977\t                '_order__%s' % op: self.__class__._default_manager.values('_order').filter(**{\n   978\t                    self._meta.pk.name: self.pk\n   979\t                })\n   980\t            }).order_by(order)[:1].get()\n   981\t            setattr(self, cachename, obj)\n   982\t        return getattr(self, cachename)\n   983\t\n   984\t    def prepare_database_save(self, field):\n   985\t        if self.pk is None:\n   986\t            raise ValueError(\"Unsaved model instance %r cannot be used in an ORM query.\" % self)\n   987\t        return getattr(self, field.remote_field.get_related_field().attname)\n   988\t\n   989\t    def clean(self):\n   990\t        \"\"\"\n   991\t        Hook for doing any extra model-wide validation after clean() has been\n   992\t        called on every field by self.clean_fields. Any ValidationError raised\n   993\t        by this method will not be associated with a particular field; it will\n   994\t        have a special-case association with the field defined by NON_FIELD_ERRORS.\n   995\t        \"\"\"\n   996\t        pass\n   997\t\n   998\t    def validate_unique(self, exclude=None):\n   999\t        \"\"\"\n  1000\t        Check unique constraints on the model and raise ValidationError if any\n  1001\t        failed.\n  1002\t        \"\"\"\n  1003\t        unique_checks, date_checks = self._get_unique_checks(exclude=exclude)\n  1004\t\n  1005\t        errors = self._perform_unique_checks(unique_checks)\n  1006\t        date_errors = self._perform_date_checks(date_checks)\n  1007\t\n  1008\t        for k, v in date_errors.items():\n  1009\t            errors.setdefault(k, []).extend(v)\n  1010\t\n  1011\t        if errors:\n  1012\t            raise ValidationError(errors)\n  1013\t\n  1014\t    def _get_unique_checks(self, exclude=None):\n  1015\t        \"\"\"\n  1016\t        Return a list of checks to perform. Since validate_unique() could be\n  1017\t        called from a ModelForm, some fields may have been excluded; we can't\n  1018\t        perform a unique check on a model that is missing fields involved\n  1019\t        in that check. Fields that did not validate should also be excluded,\n  1020\t        but they need to be passed in via the exclude argument.\n  1021\t        \"\"\"\n  1022\t        if exclude is None:\n  1023\t            exclude = []\n  1024\t        unique_checks = []\n  1025\t\n  1026\t        unique_togethers = [(self.__class__, self._meta.unique_together)]\n  1027\t        constraints = [(self.__class__, self._meta.total_unique_constraints)]\n  1028\t        for parent_class in self._meta.get_parent_list():\n  1029\t            if parent_class._meta.unique_together:\n  1030\t                unique_togethers.append((parent_class, parent_class._meta.unique_together))\n  1031\t            if parent_class._meta.total_unique_constraints:\n  1032\t                constraints.append(\n  1033\t                    (parent_class, parent_class._meta.total_unique_constraints)\n  1034\t                )\n  1035\t\n  1036\t        for model_class, unique_together in unique_togethers:\n  1037\t            for check in unique_together:\n  1038\t                if not any(name in exclude for name in check):\n  1039\t                    # Add the check if the field isn't excluded.\n  1040\t                    unique_checks.append((model_class, tuple(check)))\n  1041\t\n  1042\t        for model_class, model_constraints in constraints:\n  1043\t            for constraint in model_constraints:\n  1044\t                if not any(name in exclude for name in constraint.fields):\n  1045\t                    unique_checks.append((model_class, constraint.fields))\n  1046\t\n  1047\t        # These are checks for the unique_for_<date/year/month>.\n  1048\t        date_checks = []\n  1049\t\n  1050\t        # Gather a list of checks for fields declared as unique and add them to\n  1051\t        # the list of checks.\n  1052\t\n  1053\t        fields_with_class = [(self.__class__, self._meta.local_fields)]\n  1054\t        for parent_class in self._meta.get_parent_list():\n  1055\t            fields_with_class.append((parent_class, parent_class._meta.local_fields))\n  1056\t\n  1057\t        for model_class, fields in fields_with_class:\n  1058\t            for f in fields:\n  1059\t                name = f.name\n  1060\t                if name in exclude:\n  1061\t                    continue\n  1062\t                if f.unique:\n  1063\t                    unique_checks.append((model_class, (name,)))\n  1064\t                if f.unique_for_date and f.unique_for_date not in exclude:\n  1065\t                    date_checks.append((model_class, 'date', name, f.unique_for_date))\n  1066\t                if f.unique_for_year and f.unique_for_year not in exclude:\n  1067\t                    date_checks.append((model_class, 'year', name, f.unique_for_year))\n  1068\t                if f.unique_for_month and f.unique_for_month not in exclude:\n  1069\t                    date_checks.append((model_class, 'month', name, f.unique_for_month))\n  1070\t        return unique_checks, date_checks\n  1071\t\n  1072\t    def _perform_unique_checks(self, unique_checks):\n  1073\t        errors = {}\n  1074\t\n  1075\t        for model_class, unique_check in unique_checks:\n  1076\t            # Try to look up an existing object with the same values as this\n  1077\t            # object's values for all the unique field.\n  1078\t\n  1079\t            lookup_kwargs = {}\n  1080\t            for field_name in unique_check:\n  1081\t                f = self._meta.get_field(field_name)\n  1082\t                lookup_value = getattr(self, f.attname)\n  1083\t                # TODO: Handle multiple backends with different feature flags.\n  1084\t                if (lookup_value is None or\n  1085\t                        (lookup_value == '' and connection.features.interprets_empty_strings_as_nulls)):\n  1086\t                    # no value, skip the lookup\n  1087\t                    continue\n  1088\t                if f.primary_key and not self._state.adding:\n  1089\t                    # no need to check for unique primary key when editing\n  1090\t                    continue\n  1091\t                lookup_kwargs[str(field_name)] = lookup_value\n  1092\t\n  1093\t            # some fields were skipped, no reason to do the check\n  1094\t            if len(unique_check) != len(lookup_kwargs):\n  1095\t                continue\n  1096\t\n  1097\t            qs = model_class._default_manager.filter(**lookup_kwargs)\n  1098\t\n  1099\t            # Exclude the current object from the query if we are editing an\n  1100\t            # instance (as opposed to creating a new one)\n  1101\t            # Note that we need to use the pk as defined by model_class, not\n  1102\t            # self.pk. These can be different fields because model inheritance\n  1103\t            # allows single model to have effectively multiple primary keys.\n  1104\t            # Refs #17615.\n  1105\t            model_class_pk = self._get_pk_val(model_class._meta)\n  1106\t            if not self._state.adding and model_class_pk is not None:\n  1107\t                qs = qs.exclude(pk=model_class_pk)\n  1108\t            if qs.exists():\n  1109\t                if len(unique_check) == 1:\n  1110\t                    key = unique_check[0]\n  1111\t                else:\n  1112\t                    key = NON_FIELD_ERRORS\n  1113\t                errors.setdefault(key, []).append(self.unique_error_message(model_class, unique_check))\n  1114\t\n  1115\t        return errors\n  1116\t\n  1117\t    def _perform_date_checks(self, date_checks):\n  1118\t        errors = {}\n  1119\t        for model_class, lookup_type, field, unique_for in date_checks:\n  1120\t            lookup_kwargs = {}\n  1121\t            # there's a ticket to add a date lookup, we can remove this special\n  1122\t            # case if that makes it's way in\n  1123\t            date = getattr(self, unique_for)\n  1124\t            if date is None:\n  1125\t                continue\n  1126\t            if lookup_type == 'date':\n  1127\t                lookup_kwargs['%s__day' % unique_for] = date.day\n  1128\t                lookup_kwargs['%s__month' % unique_for] = date.month\n  1129\t                lookup_kwargs['%s__year' % unique_for] = date.year\n  1130\t            else:\n  1131\t                lookup_kwargs['%s__%s' % (unique_for, lookup_type)] = getattr(date, lookup_type)\n  1132\t            lookup_kwargs[field] = getattr(self, field)\n  1133\t\n  1134\t            qs = model_class._default_manager.filter(**lookup_kwargs)\n  1135\t            # Exclude the current object from the query if we are editing an\n  1136\t            # instance (as opposed to creating a new one)\n  1137\t            if not self._state.adding and self.pk is not None:\n  1138\t                qs = qs.exclude(pk=self.pk)\n  1139\t\n  1140\t            if qs.exists():\n  1141\t                errors.setdefault(field, []).append(\n  1142\t                    self.date_error_message(lookup_type, field, unique_for)\n  1143\t                )\n  1144\t        return errors\n  1145\t\n  1146\t    def date_error_message(self, lookup_type, field_name, unique_for):\n  1147\t        opts = self._meta\n  1148\t        field = opts.get_field(field_name)\n  1149\t        return ValidationError(\n  1150\t            message=field.error_messages['unique_for_date'],\n  1151\t            code='unique_for_date',\n  1152\t            params={\n  1153\t                'model': self,\n  1154\t                'model_name': capfirst(opts.verbose_name),\n  1155\t                'lookup_type': lookup_type,\n  1156\t                'field': field_name,\n  1157\t                'field_label': capfirst(field.verbose_name),\n  1158\t                'date_field': unique_for,\n  1159\t                'date_field_label': capfirst(opts.get_field(unique_for).verbose_name),\n  1160\t            }\n  1161\t        )\n  1162\t\n  1163\t    def unique_error_message(self, model_class, unique_check):\n  1164\t        opts = model_class._meta\n  1165\t\n  1166\t        params = {\n  1167\t            'model': self,\n  1168\t            'model_class': model_class,\n  1169\t            'model_name': capfirst(opts.verbose_name),\n  1170\t            'unique_check': unique_check,\n  1171\t        }\n  1172\t\n  1173\t        # A unique field\n  1174\t        if len(unique_check) == 1:\n  1175\t            field = opts.get_field(unique_check[0])\n  1176\t            params['field_label'] = capfirst(field.verbose_name)\n  1177\t            return ValidationError(\n  1178\t                message=field.error_messages['unique'],\n  1179\t                code='unique',\n  1180\t                params=params,\n  1181\t            )\n  1182\t\n  1183\t        # unique_together\n  1184\t        else:\n  1185\t            field_labels = [capfirst(opts.get_field(f).verbose_name) for f in unique_check]\n  1186\t            params['field_labels'] = get_text_list(field_labels, _('and'))\n  1187\t            return ValidationError(\n  1188\t                message=_(\"%(model_name)s with this %(field_labels)s already exists.\"),\n  1189\t                code='unique_together',\n  1190\t                params=params,\n  1191\t            )\n  1192\t\n  1193\t    def full_clean(self, exclude=None, validate_unique=True):\n  1194\t        \"\"\"\n  1195\t        Call clean_fields(), clean(), and validate_unique() on the model.\n  1196\t        Raise a ValidationError for any errors that occur.\n  1197\t        \"\"\"\n  1198\t        errors = {}\n  1199\t        if exclude is None:\n  1200\t            exclude = []\n  1201\t        else:\n  1202\t            exclude = list(exclude)\n  1203\t\n  1204\t        try:\n  1205\t            self.clean_fields(exclude=exclude)\n  1206\t        except ValidationError as e:\n  1207\t            errors = e.update_error_dict(errors)\n  1208\t\n  1209\t        # Form.clean() is run even if other validation fails, so do the\n  1210\t        # same with Model.clean() for consistency.\n  1211\t        try:\n  1212\t            self.clean()\n  1213\t        except ValidationError as e:\n  1214\t            errors = e.update_error_dict(errors)\n  1215\t\n  1216\t        # Run unique checks, but only for fields that passed validation.\n  1217\t        if validate_unique:\n  1218\t            for name in errors:\n  1219\t                if name != NON_FIELD_ERRORS and name not in exclude:\n  1220\t                    exclude.append(name)\n  1221\t            try:\n  1222\t                self.validate_unique(exclude=exclude)\n  1223\t            except ValidationError as e:\n  1224\t                errors = e.update_error_dict(errors)\n  1225\t\n  1226\t        if errors:\n  1227\t            raise ValidationError(errors)\n  1228\t\n  1229\t    def clean_fields(self, exclude=None):\n  1230\t        \"\"\"\n  1231\t        Clean all fields and raise a ValidationError containing a dict\n  1232\t        of all validation errors if any occur.\n  1233\t        \"\"\"\n  1234\t        if exclude is None:\n  1235\t            exclude = []\n  1236\t\n  1237\t        errors = {}\n  1238\t        for f in self._meta.fields:\n  1239\t            if f.name in exclude:\n  1240\t                continue\n  1241\t            # Skip validation for empty fields with blank=True. The developer\n  1242\t            # is responsible for making sure they have a valid value.\n  1243\t            raw_value = getattr(self, f.attname)\n  1244\t            if f.blank and raw_value in f.empty_values:\n  1245\t                continue\n  1246\t            try:\n  1247\t                setattr(self, f.attname, f.clean(raw_value, self))\n  1248\t            except ValidationError as e:\n  1249\t                errors[f.name] = e.error_list\n  1250\t\n  1251\t        if errors:\n  1252\t            raise ValidationError(errors)\n  1253\t\n  1254\t    @classmethod\n  1255\t    def check(cls, **kwargs):\n  1256\t        errors = [*cls._check_swappable(), *cls._check_model(), *cls._check_managers(**kwargs)]\n  1257\t        if not cls._meta.swapped:\n  1258\t            databases = kwargs.get('databases') or []\n  1259\t            errors += [\n  1260\t                *cls._check_fields(**kwargs),\n  1261\t                *cls._check_m2m_through_same_relationship(),\n  1262\t                *cls._check_long_column_names(databases),\n  1263\t            ]\n  1264\t            clash_errors = (\n  1265\t                *cls._check_id_field(),\n  1266\t                *cls._check_field_name_clashes(),\n  1267\t                *cls._check_model_name_db_lookup_clashes(),\n  1268\t                *cls._check_property_name_related_field_accessor_clashes(),\n  1269\t                *cls._check_single_primary_key(),\n  1270\t            )\n  1271\t            errors.extend(clash_errors)\n  1272\t            # If there are field name clashes, hide consequent column name\n  1273\t            # clashes.\n  1274\t            if not clash_errors:\n  1275\t                errors.extend(cls._check_column_name_clashes())\n  1276\t            errors += [\n  1277\t                *cls._check_index_together(),\n  1278\t                *cls._check_unique_together(),\n  1279\t                *cls._check_indexes(),\n  1280\t                *cls._check_ordering(),\n  1281\t                *cls._check_constraints(databases),\n  1282\t            ]\n  1283\t\n  1284\t        return errors\n  1285\t\n  1286\t    @classmethod\n  1287\t    def _check_swappable(cls):\n  1288\t        \"\"\"Check if the swapped model exists.\"\"\"\n  1289\t        errors = []\n  1290\t        if cls._meta.swapped:\n  1291\t            try:\n  1292\t                apps.get_model(cls._meta.swapped)\n  1293\t            except ValueError:\n  1294\t                errors.append(\n  1295\t                    checks.Error(\n  1296\t                        \"'%s' is not of the form 'app_label.app_name'.\" % cls._meta.swappable,\n  1297\t                        id='models.E001',\n  1298\t                    )\n  1299\t                )\n  1300\t            except LookupError:\n  1301\t                app_label, model_name = cls._meta.swapped.split('.')\n  1302\t                errors.append(\n  1303\t                    checks.Error(\n  1304\t                        \"'%s' references '%s.%s', which has not been \"\n  1305\t                        \"installed, or is abstract.\" % (\n  1306\t                            cls._meta.swappable, app_label, model_name\n  1307\t                        ),\n  1308\t                        id='models.E002',\n  1309\t                    )\n  1310\t                )\n  1311\t        return errors\n  1312\t\n  1313\t    @classmethod\n  1314\t    def _check_model(cls):\n  1315\t        errors = []\n  1316\t        if cls._meta.proxy:\n  1317\t            if cls._meta.local_fields or cls._meta.local_many_to_many:\n  1318\t                errors.append(\n  1319\t                    checks.Error(\n  1320\t                        \"Proxy model '%s' contains model fields.\" % cls.__name__,\n  1321\t                        id='models.E017',\n  1322\t                    )\n  1323\t                )\n  1324\t        return errors\n  1325\t\n  1326\t    @classmethod\n  1327\t    def _check_managers(cls, **kwargs):\n  1328\t        \"\"\"Perform all manager checks.\"\"\"\n  1329\t        errors = []\n  1330\t        for manager in cls._meta.managers:\n  1331\t            errors.extend(manager.check(**kwargs))\n  1332\t        return errors\n  1333\t\n  1334\t    @classmethod\n  1335\t    def _check_fields(cls, **kwargs):\n  1336\t        \"\"\"Perform all field checks.\"\"\"\n  1337\t        errors = []\n  1338\t        for field in cls._meta.local_fields:\n  1339\t            errors.extend(field.check(**kwargs))\n  1340\t        for field in cls._meta.local_many_to_many:\n  1341\t            errors.extend(field.check(from_model=cls, **kwargs))\n  1342\t        return errors\n  1343\t\n  1344\t    @classmethod\n  1345\t    def _check_m2m_through_same_relationship(cls):\n  1346\t        \"\"\" Check if no relationship model is used by more than one m2m field.\n  1347\t        \"\"\"\n  1348\t\n  1349\t        errors = []\n  1350\t        seen_intermediary_signatures = []\n  1351\t\n  1352\t        fields = cls._meta.local_many_to_many\n  1353\t\n  1354\t        # Skip when the target model wasn't found.\n  1355\t        fields = (f for f in fields if isinstance(f.remote_field.model, ModelBase))\n  1356\t\n  1357\t        # Skip when the relationship model wasn't found.\n  1358\t        fields = (f for f in fields if isinstance(f.remote_field.through, ModelBase))\n  1359\t\n  1360\t        for f in fields:\n  1361\t            signature = (f.remote_field.model, cls, f.remote_field.through, f.remote_field.through_fields)\n  1362\t            if signature in seen_intermediary_signatures:\n  1363\t                errors.append(\n  1364\t                    checks.Error(\n  1365\t                        \"The model has two identical many-to-many relations \"\n  1366\t                        \"through the intermediate model '%s'.\" %\n  1367\t                        f.remote_field.through._meta.label,\n  1368\t                        obj=cls,\n  1369\t                        id='models.E003',\n  1370\t                    )\n  1371\t                )\n  1372\t            else:\n  1373\t                seen_intermediary_signatures.append(signature)\n  1374\t        return errors\n  1375\t\n  1376\t    @classmethod\n  1377\t    def _check_id_field(cls):\n  1378\t        \"\"\"Check if `id` field is a primary key.\"\"\"\n  1379\t        fields = [f for f in cls._meta.local_fields if f.name == 'id' and f != cls._meta.pk]\n  1380\t        # fields is empty or consists of the invalid \"id\" field\n  1381\t        if fields and not fields[0].primary_key and cls._meta.pk.name == 'id':\n  1382\t            return [\n  1383\t                checks.Error(\n  1384\t                    \"'id' can only be used as a field name if the field also \"\n  1385\t                    \"sets 'primary_key=True'.\",\n  1386\t                    obj=cls,\n  1387\t                    id='models.E004',\n  1388\t                )\n  1389\t            ]\n  1390\t        else:\n  1391\t            return []\n  1392\t\n  1393\t    @classmethod\n  1394\t    def _check_field_name_clashes(cls):\n  1395\t        \"\"\"Forbid field shadowing in multi-table inheritance.\"\"\"\n  1396\t        errors = []\n  1397\t        used_fields = {}  # name or attname -> field\n  1398\t\n  1399\t        # Check that multi-inheritance doesn't cause field name shadowing.\n  1400\t        for parent in cls._meta.get_parent_list():\n  1401\t            for f in parent._meta.local_fields:\n  1402\t                clash = used_fields.get(f.name) or used_fields.get(f.attname) or None\n  1403\t                if clash:\n  1404\t                    errors.append(\n  1405\t                        checks.Error(\n  1406\t                            \"The field '%s' from parent model \"\n  1407\t                            \"'%s' clashes with the field '%s' \"\n  1408\t                            \"from parent model '%s'.\" % (\n  1409\t                                clash.name, clash.model._meta,\n  1410\t                                f.name, f.model._meta\n  1411\t                            ),\n  1412\t                            obj=cls,\n  1413\t                            id='models.E005',\n  1414\t                        )\n  1415\t                    )\n  1416\t                used_fields[f.name] = f\n  1417\t                used_fields[f.attname] = f\n  1418\t\n  1419\t        # Check that fields defined in the model don't clash with fields from\n  1420\t        # parents, including auto-generated fields like multi-table inheritance\n  1421\t        # child accessors.\n  1422\t        for parent in cls._meta.get_parent_list():\n  1423\t            for f in parent._meta.get_fields():\n  1424\t                if f not in used_fields:\n  1425\t                    used_fields[f.name] = f\n  1426\t\n  1427\t        for f in cls._meta.local_fields:\n  1428\t            clash = used_fields.get(f.name) or used_fields.get(f.attname) or None\n  1429\t            # Note that we may detect clash between user-defined non-unique\n  1430\t            # field \"id\" and automatically added unique field \"id\", both\n  1431\t            # defined at the same model. This special case is considered in\n  1432\t            # _check_id_field and here we ignore it.\n  1433\t            id_conflict = f.name == \"id\" and clash and clash.name == \"id\" and clash.model == cls\n  1434\t            if clash and not id_conflict:\n  1435\t                errors.append(\n  1436\t                    checks.Error(\n  1437\t                        \"The field '%s' clashes with the field '%s' \"\n  1438\t                        \"from model '%s'.\" % (\n  1439\t                            f.name, clash.name, clash.model._meta\n  1440\t                        ),\n  1441\t                        obj=f,\n  1442\t                        id='models.E006',\n  1443\t                    )\n  1444\t                )\n  1445\t            used_fields[f.name] = f\n  1446\t            used_fields[f.attname] = f\n  1447\t\n  1448\t        return errors\n  1449\t\n  1450\t    @classmethod\n  1451\t    def _check_column_name_clashes(cls):\n  1452\t        # Store a list of column names which have already been used by other fields.\n  1453\t        used_column_names = []\n  1454\t        errors = []\n  1455\t\n  1456\t        for f in cls._meta.local_fields:\n  1457\t            _, column_name = f.get_attname_column()\n  1458\t\n  1459\t            # Ensure the column name is not already in use.\n  1460\t            if column_name and column_name in used_column_names:\n  1461\t                errors.append(\n  1462\t                    checks.Error(\n  1463\t                        \"Field '%s' has column name '%s' that is used by \"\n  1464\t                        \"another field.\" % (f.name, column_name),\n  1465\t                        hint=\"Specify a 'db_column' for the field.\",\n  1466\t                        obj=cls,\n  1467\t                        id='models.E007'\n  1468\t                    )\n  1469\t                )\n  1470\t            else:\n  1471\t                used_column_names.append(column_name)\n  1472\t\n  1473\t        return errors\n  1474\t\n  1475\t    @classmethod\n  1476\t    def _check_model_name_db_lookup_clashes(cls):\n  1477\t        errors = []\n  1478\t        model_name = cls.__name__\n  1479\t        if model_name.startswith('_') or model_name.endswith('_'):\n  1480\t            errors.append(\n  1481\t                checks.Error(\n  1482\t                    \"The model name '%s' cannot start or end with an underscore \"\n  1483\t                    \"as it collides with the query lookup syntax.\" % model_name,\n  1484\t                    obj=cls,\n  1485\t                    id='models.E023'\n  1486\t                )\n  1487\t            )\n  1488\t        elif LOOKUP_SEP in model_name:\n  1489\t            errors.append(\n  1490\t                checks.Error(\n  1491\t                    \"The model name '%s' cannot contain double underscores as \"\n  1492\t                    \"it collides with the query lookup syntax.\" % model_name,\n  1493\t                    obj=cls,\n  1494\t                    id='models.E024'\n  1495\t                )\n  1496\t            )\n  1497\t        return errors\n  1498\t\n  1499\t    @classmethod\n  1500\t    def _check_property_name_related_field_accessor_clashes(cls):\n  1501\t        errors = []\n  1502\t        property_names = cls._meta._property_names\n  1503\t        related_field_accessors = (\n  1504\t            f.get_attname() for f in cls._meta._get_fields(reverse=False)\n  1505\t            if f.is_relation and f.related_model is not None\n  1506\t        )\n  1507\t        for accessor in related_field_accessors:\n  1508\t            if accessor in property_names:\n  1509\t                errors.append(\n  1510\t                    checks.Error(\n  1511\t                        \"The property '%s' clashes with a related field \"\n  1512\t                        \"accessor.\" % accessor,\n  1513\t                        obj=cls,\n  1514\t                        id='models.E025',\n  1515\t                    )\n  1516\t                )\n  1517\t        return errors\n  1518\t\n  1519\t    @classmethod\n  1520\t    def _check_single_primary_key(cls):\n  1521\t        errors = []\n  1522\t        if sum(1 for f in cls._meta.local_fields if f.primary_key) > 1:\n  1523\t            errors.append(\n  1524\t                checks.Error(\n  1525\t                    \"The model cannot have more than one field with \"\n  1526\t                    \"'primary_key=True'.\",\n  1527\t                    obj=cls,\n  1528\t                    id='models.E026',\n  1529\t                )\n  1530\t            )\n  1531\t        return errors\n  1532\t\n  1533\t    @classmethod\n  1534\t    def _check_index_together(cls):\n  1535\t        \"\"\"Check the value of \"index_together\" option.\"\"\"\n  1536\t        if not isinstance(cls._meta.index_together, (tuple, list)):\n  1537\t            return [\n  1538\t                checks.Error(\n  1539\t                    \"'index_together' must be a list or tuple.\",\n  1540\t                    obj=cls,\n  1541\t                    id='models.E008',\n  1542\t                )\n  1543\t            ]\n  1544\t\n  1545\t        elif any(not isinstance(fields, (tuple, list)) for fields in cls._meta.index_together):\n  1546\t            return [\n  1547\t                checks.Error(\n  1548\t                    \"All 'index_together' elements must be lists or tuples.\",\n  1549\t                    obj=cls,\n  1550\t                    id='models.E009',\n  1551\t                )\n  1552\t            ]\n  1553\t\n  1554\t        else:\n  1555\t            errors = []\n  1556\t            for fields in cls._meta.index_together:\n  1557\t                errors.extend(cls._check_local_fields(fields, \"index_together\"))\n  1558\t            return errors\n  1559\t\n  1560\t    @classmethod\n  1561\t    def _check_unique_together(cls):\n  1562\t        \"\"\"Check the value of \"unique_together\" option.\"\"\"\n  1563\t        if not isinstance(cls._meta.unique_together, (tuple, list)):\n  1564\t            return [\n  1565\t                checks.Error(\n  1566\t                    \"'unique_together' must be a list or tuple.\",\n  1567\t                    obj=cls,\n  1568\t                    id='models.E010',\n  1569\t                )\n  1570\t            ]\n  1571\t\n  1572\t        elif any(not isinstance(fields, (tuple, list)) for fields in cls._meta.unique_together):\n  1573\t            return [\n  1574\t                checks.Error(\n  1575\t                    \"All 'unique_together' elements must be lists or tuples.\",\n  1576\t                    obj=cls,\n  1577\t                    id='models.E011',\n  1578\t                )\n  1579\t            ]\n  1580\t\n  1581\t        else:\n  1582\t            errors = []\n  1583\t            for fields in cls._meta.unique_together:\n  1584\t                errors.extend(cls._check_local_fields(fields, \"unique_together\"))\n  1585\t            return errors\n  1586\t\n  1587\t    @classmethod\n  1588\t    def _check_indexes(cls):\n  1589\t        \"\"\"Check the fields and names of indexes.\"\"\"\n  1590\t        errors = []\n  1591\t        for index in cls._meta.indexes:\n  1592\t            # Index name can't start with an underscore or a number, restricted\n  1593\t            # for cross-database compatibility with Oracle.\n  1594\t            if index.name[0] == '_' or index.name[0].isdigit():\n  1595\t                errors.append(\n  1596\t                    checks.Error(\n  1597\t                        \"The index name '%s' cannot start with an underscore \"\n  1598\t                        \"or a number.\" % index.name,\n  1599\t                        obj=cls,\n  1600\t                        id='models.E033',\n  1601\t                    ),\n  1602\t                )\n  1603\t            if len(index.name) > index.max_name_length:\n  1604\t                errors.append(\n  1605\t                    checks.Error(\n  1606\t                        \"The index name '%s' cannot be longer than %d \"\n  1607\t                        \"characters.\" % (index.name, index.max_name_length),\n  1608\t                        obj=cls,\n  1609\t                        id='models.E034',\n  1610\t                    ),\n  1611\t                )\n  1612\t        fields = [field for index in cls._meta.indexes for field, _ in index.fields_orders]\n  1613\t        errors.extend(cls._check_local_fields(fields, 'indexes'))\n  1614\t        return errors\n  1615\t\n  1616\t    @classmethod\n  1617\t    def _check_local_fields(cls, fields, option):\n  1618\t        from django.db import models\n  1619\t\n  1620\t        # In order to avoid hitting the relation tree prematurely, we use our\n  1621\t        # own fields_map instead of using get_field()\n  1622\t        forward_fields_map = {}\n  1623\t        for field in cls._meta._get_fields(reverse=False):\n  1624\t            forward_fields_map[field.name] = field\n  1625\t            if hasattr(field, 'attname'):\n  1626\t                forward_fields_map[field.attname] = field\n  1627\t\n  1628\t        errors = []\n  1629\t        for field_name in fields:\n  1630\t            try:\n  1631\t                field = forward_fields_map[field_name]\n  1632\t            except KeyError:\n  1633\t                errors.append(\n  1634\t                    checks.Error(\n  1635\t                        \"'%s' refers to the nonexistent field '%s'.\" % (\n  1636\t                            option, field_name,\n  1637\t                        ),\n  1638\t                        obj=cls,\n  1639\t                        id='models.E012',\n  1640\t                    )\n  1641\t                )\n  1642\t            else:\n  1643\t                if isinstance(field.remote_field, models.ManyToManyRel):\n  1644\t                    errors.append(\n  1645\t                        checks.Error(\n  1646\t                            \"'%s' refers to a ManyToManyField '%s', but \"\n  1647\t                            \"ManyToManyFields are not permitted in '%s'.\" % (\n  1648\t                                option, field_name, option,\n  1649\t                            ),\n  1650\t                            obj=cls,\n  1651\t                            id='models.E013',\n  1652\t                        )\n  1653\t                    )\n  1654\t                elif field not in cls._meta.local_fields:\n  1655\t                    errors.append(\n  1656\t                        checks.Error(\n  1657\t                            \"'%s' refers to field '%s' which is not local to model '%s'.\"\n  1658\t                            % (option, field_name, cls._meta.object_name),\n  1659\t                            hint=\"This issue may be caused by multi-table inheritance.\",\n  1660\t                            obj=cls,\n  1661\t                            id='models.E016',\n  1662\t                        )\n  1663\t                    )\n  1664\t        return errors\n  1665\t\n  1666\t    @classmethod\n  1667\t    def _check_ordering(cls):\n  1668\t        \"\"\"\n  1669\t        Check \"ordering\" option -- is it a list of strings and do all fields\n  1670\t        exist?\n  1671\t        \"\"\"\n  1672\t        if cls._meta._ordering_clash:\n  1673\t            return [\n  1674\t                checks.Error(\n  1675\t                    \"'ordering' and 'order_with_respect_to' cannot be used together.\",\n  1676\t                    obj=cls,\n  1677\t                    id='models.E021',\n  1678\t                ),\n  1679\t            ]\n  1680\t\n  1681\t        if cls._meta.order_with_respect_to or not cls._meta.ordering:\n  1682\t            return []\n  1683\t\n  1684\t        if not isinstance(cls._meta.ordering, (list, tuple)):\n  1685\t            return [\n  1686\t                checks.Error(\n  1687\t                    \"'ordering' must be a tuple or list (even if you want to order by only one field).\",\n  1688\t                    obj=cls,\n  1689\t                    id='models.E014',\n  1690\t                )\n  1691\t            ]\n  1692\t\n  1693\t        errors = []\n  1694\t        fields = cls._meta.ordering\n  1695\t\n  1696\t        # Skip expressions and '?' fields.\n  1697\t        fields = (f for f in fields if isinstance(f, str) and f != '?')\n  1698\t\n  1699\t        # Convert \"-field\" to \"field\".\n  1700\t        fields = ((f[1:] if f.startswith('-') else f) for f in fields)\n  1701\t\n  1702\t        # Separate related fields and non-related fields.\n  1703\t        _fields = []\n  1704\t        related_fields = []\n  1705\t        for f in fields:\n  1706\t            if LOOKUP_SEP in f:\n  1707\t                related_fields.append(f)\n  1708\t            else:\n  1709\t                _fields.append(f)\n  1710\t        fields = _fields\n  1711\t\n  1712\t        # Check related fields.\n  1713\t        for field in related_fields:\n  1714\t            _cls = cls\n  1715\t            fld = None\n  1716\t            for part in field.split(LOOKUP_SEP):\n  1717\t                try:\n  1718\t                    # pk is an alias that won't be found by opts.get_field.\n  1719\t                    if part == 'pk':\n  1720\t                        fld = _cls._meta.pk\n  1721\t                    else:\n  1722\t                        fld = _cls._meta.get_field(part)\n  1723\t                    if fld.is_relation:\n  1724\t                        _cls = fld.get_path_info()[-1].to_opts.model\n  1725\t                    else:\n  1726\t                        _cls = None\n  1727\t                except (FieldDoesNotExist, AttributeError):\n  1728\t                    if fld is None or fld.get_transform(part) is None:\n  1729\t                        errors.append(\n  1730\t                            checks.Error(\n  1731\t                                \"'ordering' refers to the nonexistent field, \"\n  1732\t                                \"related field, or lookup '%s'.\" % field,\n  1733\t                                obj=cls,\n  1734\t                                id='models.E015',\n  1735\t                            )\n  1736\t                        )\n  1737\t\n  1738\t        # Skip ordering on pk. This is always a valid order_by field\n  1739\t        # but is an alias and therefore won't be found by opts.get_field.\n  1740\t        fields = {f for f in fields if f != 'pk'}\n  1741\t\n  1742\t        # Check for invalid or nonexistent fields in ordering.\n  1743\t        invalid_fields = []\n  1744\t\n  1745\t        # Any field name that is not present in field_names does not exist.\n  1746\t        # Also, ordering by m2m fields is not allowed.\n  1747\t        opts = cls._meta\n  1748\t        valid_fields = set(chain.from_iterable(\n  1749\t            (f.name, f.attname) if not (f.auto_created and not f.concrete) else (f.field.related_query_name(),)\n  1750\t            for f in chain(opts.fields, opts.related_objects)\n  1751\t        ))\n  1752\t\n  1753\t        invalid_fields.extend(fields - valid_fields)\n  1754\t\n  1755\t        for invalid_field in invalid_fields:\n  1756\t            errors.append(\n  1757\t                checks.Error(\n  1758\t                    \"'ordering' refers to the nonexistent field, related \"\n  1759\t                    \"field, or lookup '%s'.\" % invalid_field,\n  1760\t                    obj=cls,\n  1761\t                    id='models.E015',\n  1762\t                )\n  1763\t            )\n  1764\t        return errors\n  1765\t\n  1766\t    @classmethod\n  1767\t    def _check_long_column_names(cls, databases):\n  1768\t        \"\"\"\n  1769\t        Check that any auto-generated column names are shorter than the limits\n  1770\t        for each database in which the model will be created.\n  1771\t        \"\"\"\n  1772\t        if not databases:\n  1773\t            return []\n  1774\t        errors = []\n  1775\t        allowed_len = None\n  1776\t        db_alias = None\n  1777\t\n  1778\t        # Find the minimum max allowed length among all specified db_aliases.\n  1779\t        for db in databases:\n  1780\t            # skip databases where the model won't be created\n  1781\t            if not router.allow_migrate_model(db, cls):\n  1782\t                continue\n  1783\t            connection = connections[db]\n  1784\t            max_name_length = connection.ops.max_name_length()\n  1785\t            if max_name_length is None or connection.features.truncates_names:\n  1786\t                continue\n  1787\t            else:\n  1788\t                if allowed_len is None:\n  1789\t                    allowed_len = max_name_length\n  1790\t                    db_alias = db\n  1791\t                elif max_name_length < allowed_len:\n  1792\t                    allowed_len = max_name_length\n  1793\t                    db_alias = db\n  1794\t\n  1795\t        if allowed_len is None:\n  1796\t            return errors\n  1797\t\n  1798\t        for f in cls._meta.local_fields:\n  1799\t            _, column_name = f.get_attname_column()\n  1800\t\n  1801\t            # Check if auto-generated name for the field is too long\n  1802\t            # for the database.\n  1803\t            if f.db_column is None and column_name is not None and len(column_name) > allowed_len:\n  1804\t                errors.append(\n  1805\t                    checks.Error(\n  1806\t                        'Autogenerated column name too long for field \"%s\". '\n  1807\t                        'Maximum length is \"%s\" for database \"%s\".'\n  1808\t                        % (column_name, allowed_len, db_alias),\n  1809\t                        hint=\"Set the column name manually using 'db_column'.\",\n  1810\t                        obj=cls,\n  1811\t                        id='models.E018',\n  1812\t                    )\n  1813\t                )\n  1814\t\n  1815\t        for f in cls._meta.local_many_to_many:\n  1816\t            # Skip nonexistent models.\n  1817\t            if isinstance(f.remote_field.through, str):\n  1818\t                continue\n  1819\t\n  1820\t            # Check if auto-generated name for the M2M field is too long\n  1821\t            # for the database.\n  1822\t            for m2m in f.remote_field.through._meta.local_fields:\n  1823\t                _, rel_name = m2m.get_attname_column()\n  1824\t                if m2m.db_column is None and rel_name is not None and len(rel_name) > allowed_len:\n  1825\t                    errors.append(\n  1826\t                        checks.Error(\n  1827\t                            'Autogenerated column name too long for M2M field '\n  1828\t                            '\"%s\". Maximum length is \"%s\" for database \"%s\".'\n  1829\t                            % (rel_name, allowed_len, db_alias),\n  1830\t                            hint=(\n  1831\t                                \"Use 'through' to create a separate model for \"\n  1832\t                                \"M2M and then set column_name using 'db_column'.\"\n  1833\t                            ),\n  1834\t                            obj=cls,\n  1835\t                            id='models.E019',\n  1836\t                        )\n  1837\t                    )\n  1838\t\n  1839\t        return errors\n  1840\t\n  1841\t    @classmethod\n  1842\t    def _check_constraints(cls, databases):\n  1843\t        errors = []\n  1844\t        for db in databases:\n  1845\t            if not router.allow_migrate_model(db, cls):\n  1846\t                continue\n  1847\t            connection = connections[db]\n  1848\t            if (\n  1849\t                connection.features.supports_table_check_constraints or\n  1850\t                'supports_table_check_constraints' in cls._meta.required_db_features\n  1851\t            ):\n  1852\t                continue\n  1853\t            if any(isinstance(constraint, CheckConstraint) for constraint in cls._meta.constraints):\n  1854\t                errors.append(\n  1855\t                    checks.Warning(\n  1856\t                        '%s does not support check constraints.' % connection.display_name,\n  1857\t                        hint=(\n  1858\t                            \"A constraint won't be created. Silence this \"\n  1859\t                            \"warning if you don't care about it.\"\n  1860\t                        ),\n  1861\t                        obj=cls,\n  1862\t                        id='models.W027',\n  1863\t                    )\n  1864\t                )\n  1865\t        return errors\n  1866\t\n  1867\t\n  1868\t############################################\n  1869\t# HELPER FUNCTIONS (CURRIED MODEL METHODS) #\n  1870\t############################################\n  1871\t\n  1872\t# ORDERING METHODS #########################\n  1873\t\n  1874\tdef method_set_order(self, ordered_obj, id_list, using=None):\n  1875\t    if using is None:\n  1876\t        using = DEFAULT_DB_ALIAS\n  1877\t    order_wrt = ordered_obj._meta.order_with_respect_to\n  1878\t    filter_args = order_wrt.get_forward_related_filter(self)\n  1879\t    ordered_obj.objects.db_manager(using).filter(**filter_args).bulk_update([\n  1880\t        ordered_obj(pk=pk, _order=order) for order, pk in enumerate(id_list)\n  1881\t    ], ['_order'])\n  1882\t\n  1883\t\n  1884\tdef method_get_order(self, ordered_obj):\n  1885\t    order_wrt = ordered_obj._meta.order_with_respect_to\n  1886\t    filter_args = order_wrt.get_forward_related_filter(self)\n  1887\t    pk_name = ordered_obj._meta.pk.name\n  1888\t    return ordered_obj.objects.filter(**filter_args).values_list(pk_name, flat=True)\n  1889\t\n  1890\t\n  1891\tdef make_foreign_order_accessors(model, related_model):\n  1892\t    setattr(\n  1893\t        related_model,\n  1894\t        'get_%s_order' % model.__name__.lower(),\n  1895\t        partialmethod(method_get_order, model)\n  1896\t    )\n  1897\t    setattr(\n  1898\t        related_model,\n  1899\t        'set_%s_order' % model.__name__.lower(),\n  1900\t        partialmethod(method_set_order, model)\n  1901\t    )\n  1902\t\n  1903\t########\n  1904\t# MISC #\n  1905\t########\n  1906\t\n  1907\t\n  1908\tdef model_unpickle(model_id):\n  1909\t    \"\"\"Used to unpickle Model subclasses with deferred fields.\"\"\"\n  1910\t    if isinstance(model_id, tuple):\n  1911\t        model = apps.get_model(*model_id)\n  1912\t    else:\n  1913\t        # Backwards compat - the model was cached directly in earlier versions.\n  1914\t        model = model_id\n  1915\t    return model.__new__(model)\n  1916\t\n  1917\t\n  1918\tmodel_unpickle.__safe_for_unpickle__ = True\n</document_content>\n</document>\n</documents></ToolResult>"}, {"role": "user", "content": "<ToolResult>\nCommand executed: `PYTHONPATH=/testbed DJANGO_SETTINGS_MODULE=test_settings django-admin test test_app.test_ordering -v 2`\n<output>Operations to perform:\n  Synchronize unmigrated apps: test_app\n  Apply all migrations: auth, contenttypes\nSynchronizing apps without migrations:\n  Creating tables...\n    Creating table test_app_parent\n    Creating table test_app_child\n    Running deferred SQL...\nRunning migrations:\n  Applying contenttypes.0001_initial... OK\n  Applying contenttypes.0002_remove_content_type_name... OK\n  Applying auth.0001_initial... OK\n  Applying auth.0002_alter_permission_name_max_length... OK\n  Applying auth.0003_alter_user_email_max_length... OK\n  Applying auth.0004_alter_user_username_opts... OK\n  Applying auth.0005_alter_user_last_login_null... OK\n  Applying auth.0006_require_contenttypes_0002... OK\n  Applying auth.0007_alter_validators_add_error_messages... OK\n  Applying auth.0008_alter_user_username_max_length... OK\n  Applying auth.0009_alter_user_last_name_max_length... OK\n  Applying auth.0010_alter_group_name_max_length... OK\n  Applying auth.0011_update_proxy_permissions... OK\n  Applying auth.0012_alter_user_first_name_max_length... OK\nSystem check identified no issues (0 silenced).\nCreating test database for alias 'default' ('file:memorydb_default?mode=memory&cache=shared')...\ntest_child_ordering (test_app.test_ordering.OrderingInheritanceTest) ... ok\ntest_child_query_ordering (test_app.test_ordering.OrderingInheritanceTest) ... ok\ntest_parent_ordering (test_app.test_ordering.OrderingInheritanceTest) ... FAIL\n\n======================================================================\nFAIL: test_parent_ordering (test_app.test_ordering.OrderingInheritanceTest)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"/testbed/test_app/test_ordering.py\", line 16, in test_parent_ordering\n    self.assertEqual(pks, sorted(pks, reverse=True))\nAssertionError: Lists differ: [1, 2, 3, 4, 5, 6] != [6, 5, 4, 3, 2, 1]\n\nFirst differing element 0:\n1\n6\n\n- [1, 2, 3, 4, 5, 6]\n+ [6, 5, 4, 3, 2, 1]\n\n----------------------------------------------------------------------\nRan 3 tests in 0.005s\n\nFAILED (failures=1)\nDestroying test database for alias 'default' ('file:memorydb_default?mode=memory&cache=shared')...</output></ToolResult>"}, {"role": "assistant", "content": "Let me analyze the current situation and continue working on the fix.\n\n<OBSERVE>\n1. The test results show that two tests pass but one fails:\n   - test_child_ordering: PASS\n   - test_child_query_ordering: PASS\n   - test_parent_ordering: FAIL\n2. The failure in test_parent_ordering shows that the ordering is incorrect:\n   - Got: [1, 2, 3, 4, 5, 6]\n   - Expected: [6, 5, 4, 3, 2, 1]\n3. This indicates that the descending ordering (-pk) is not being properly applied for the Parent model.\n</OBSERVE>\n\n<REASON>\n1. The issue appears to be in how the ordering is handled in the SQLCompiler.get_order_by() method.\n2. Looking at the code, I notice that when handling meta ordering, we need to ensure that:\n   - The descending flag is properly preserved for both parent and child models\n   - The ordering direction is correctly applied when using standard_ordering\n3. The current fix works for the Child model but not for the Parent model, suggesting we need to adjust how the ordering is handled for base models.\n</REASON>\n\n<FIX>\nLet's modify the get_order_by() method in SQLCompiler to properly handle the ordering direction for both parent and child models:\n\n1. We need to ensure that when meta.ordering contains a descending field (starting with '-'), the ordering direction is properly preserved.\n2. We should handle the standard_ordering flag correctly in conjunction with descending meta ordering.\n</FIX>\n\n<ACTION>\nLet's modify the SQLCompiler.get_order_by() method:", "tool_calls": [{"id": "toolu_017yVWm9Vorfg6Q2UbE559o3", "type": "function", "function": {"name": "edit_file_and_run", "arguments": "{\"command\": \"str_replace\", \"path\": \"/testbed/django/db/models/sql/compiler.py\", \"old_str\": \"            # Check if we need to reverse the ordering based on meta ordering\\n            meta = self.query.model._meta\\n            if meta.ordering and isinstance(meta.ordering[0], str):\\n                if meta.ordering[0].startswith('-'):\\n                    # For descending meta ordering, we need to swap the direction\\n                    if not self.query.standard_ordering:\\n                        asc, desc = ORDER_DIR['ASC']\\n                    else:\\n                        asc, desc = ORDER_DIR['DESC']\", \"new_str\": \"            # Check if we need to reverse the ordering based on meta ordering\\n            meta = self.query.model._meta\\n            if meta.ordering and isinstance(meta.ordering[0], str):\\n                if meta.ordering[0].startswith('-'):\\n                    # For descending meta ordering, we need to swap the direction\\n                    if self.query.standard_ordering:\\n                        asc, desc = ORDER_DIR['DESC']\\n                    else:\\n                        asc, desc = ORDER_DIR['ASC']\", \"bash_command\": \"PYTHONPATH=/testbed DJANGO_SETTINGS_MODULE=test_settings django-admin test test_app.test_ordering -v 2\"}"}}]}, {"role": "tool", "tool_call_id": "toolu_017yVWm9Vorfg6Q2UbE559o3", "name": "edit_file_and_run", "content": "<ToolResult>Replace string failed: The string '{old_str}' was not found in the file.\nBash command failed:\n\nCommand executed: `PYTHONPATH=/testbed DJANGO_SETTINGS_MODULE=test_settings django-admin test test_app.test_ordering -v 2`\n<output>Operations to perform:\n  Synchronize unmigrated apps: test_app\n  Apply all migrations: auth, contenttypes\nSynchronizing apps without migrations:\n  Creating tables...\n    Creating table test_app_parent\n    Creating table test_app_child\n    Running deferred SQL...\nRunning migrations:\n  Applying contenttypes.0001_initial... OK\n  Applying contenttypes.0002_remove_content_type_name... OK\n  Applying auth.0001_initial... OK\n  Applying auth.0002_alter_permission_name_max_length... OK\n  Applying auth.0003_alter_user_email_max_length... OK\n  Applying auth.0004_alter_user_username_opts... OK\n  Applying auth.0005_alter_user_last_login_null... OK\n  Applying auth.0006_require_contenttypes_0002... OK\n  Applying auth.0007_alter_validators_add_error_messages... OK\n  Applying auth.0008_alter_user_username_max_length... OK\n  Applying auth.0009_alter_user_last_name_max_length... OK\n  Applying auth.0010_alter_group_name_max_length... OK\n  Applying auth.0011_update_proxy_permissions... OK\n  Applying auth.0012_alter_user_first_name_max_length... OK\nSystem check identified no issues (0 silenced).\nCreating test database for alias 'default' ('file:memorydb_default?mode=memory&cache=shared')...\ntest_child_ordering (test_app.test_ordering.OrderingInheritanceTest) ... ok\ntest_child_query_ordering (test_app.test_ordering.OrderingInheritanceTest) ... ok\ntest_parent_ordering (test_app.test_ordering.OrderingInheritanceTest) ... FAIL\n\n======================================================================\nFAIL: test_parent_ordering (test_app.test_ordering.OrderingInheritanceTest)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"/testbed/test_app/test_ordering.py\", line 16, in test_parent_ordering\n    self.assertEqual(pks, sorted(pks, reverse=True))\nAssertionError: Lists differ: [1, 2, 3, 4, 5, 6] != [6, 5, 4, 3, 2, 1]\n\nFirst differing element 0:\n1\n6\n\n- [1, 2, 3, 4, 5, 6]\n+ [6, 5, 4, 3, 2, 1]\n\n----------------------------------------------------------------------\nRan 3 tests in 0.005s\n\nFAILED (failures=1)\nDestroying test database for alias 'default' ('file:memorydb_default?mode=memory&cache=shared')...</output></ToolResult>"}, {"role": "git diff", "content": "<ToolResult>\nCommand executed: `git diff `\n<output>diff --git a/django/db/models/base.py b/django/db/models/base.py\nindex b2a60629c9..08129c1325 100644\n--- a/django/db/models/base.py\n+++ b/django/db/models/base.py\n@@ -144,7 +144,8 @@ class ModelBase(type):\n                 # non-abstract parent (unless an ABC comes before it in the\n                 # method resolution order).\n                 if not hasattr(meta, 'ordering'):\n-                    new_class._meta.ordering = base_meta.ordering\n+                    # Ensure ordering is properly inherited\n+                    new_class._meta.ordering = list(base_meta.ordering)\n                 if not hasattr(meta, 'get_latest_by'):\n                     new_class._meta.get_latest_by = base_meta.get_latest_by\n \ndiff --git a/django/db/models/sql/compiler.py b/django/db/models/sql/compiler.py\nindex 92213a4e67..9dfb9862a0 100644\n--- a/django/db/models/sql/compiler.py\n+++ b/django/db/models/sql/compiler.py\n@@ -280,13 +280,33 @@ class SQLCompiler:\n         elif self.query.get_meta().ordering:\n             ordering = self.query.get_meta().ordering\n             self._meta_ordering = ordering\n+            # Ensure descending flag is preserved for inherited models\n+            if not self.query.standard_ordering:\n+                ordering = [\n+                    f.replace('-', '') if f.startswith('-') else '-' + f\n+                    for f in ordering\n+                ]\n+                # Reverse the order for inherited models\n+                if self._meta_ordering:\n+                    ordering = ['-' + f.lstrip('-') if not f.startswith('-') else f.lstrip('-') for f in ordering]\n         else:\n             ordering = []\n+        # Get the base ordering direction\n         if self.query.standard_ordering:\n             asc, desc = ORDER_DIR['ASC']\n         else:\n             asc, desc = ORDER_DIR['DESC']\n \n+        # Check if we need to reverse the ordering based on meta ordering\n+        meta = self.query.model._meta\n+        if meta.ordering and isinstance(meta.ordering[0], str):\n+            if meta.ordering[0].startswith('-'):\n+                # For descending meta ordering, we need to swap the direction\n+                if not self.query.standard_ordering:\n+                    asc, desc = ORDER_DIR['ASC']\n+                else:\n+                    asc, desc = ORDER_DIR['DESC']\n+\n         order_by = []\n         for field in ordering:\n             if hasattr(field, 'resolve_expression'):\n@@ -305,6 +325,9 @@ class SQLCompiler:\n                 continue\n \n             col, order = get_order_dir(field, asc)\n+            # Handle descending flag for inherited models\n+            if not self.query.standard_ordering:\n+                order = 'DESC' if order == 'ASC' else 'ASC'\n             descending = order == 'DESC'\n \n             if col in self.query.annotation_select:</output></ToolResult>"}]}