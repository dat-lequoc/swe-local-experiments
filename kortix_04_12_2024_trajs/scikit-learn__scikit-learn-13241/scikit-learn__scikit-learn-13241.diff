diff --git a/sklearn/decomposition/kernel_pca.py b/sklearn/decomposition/kernel_pca.py
index bb91d0cbb..9a7a35078 100644
--- a/sklearn/decomposition/kernel_pca.py
+++ b/sklearn/decomposition/kernel_pca.py
@@ -20,6 +20,13 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):
 
     Non-linear dimensionality reduction through the use of kernels (see
     :ref:`metrics`).
+    
+    Note: The sign of components from transform or fit_transform is stabilized 
+    to ensure more consistent output across multiple runs. This is done by fixing
+    the sign based on the largest absolute component. However, the absolute value
+    of the components is what carries the meaningful information, and sign 
+    differences between runs do not affect the quality of the dimensionality
+    reduction.
 
     Read more in the :ref:`User Guide <kernel_PCA>`.
 
@@ -162,6 +169,7 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):
         self.random_state = random_state
         self.n_jobs = n_jobs
         self.copy_X = copy_X
+        self.eigenvalue_signs_ = None
 
     @property
     def _pairwise(self):
@@ -277,6 +285,14 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):
 
         X_transformed = self.alphas_ * np.sqrt(self.lambdas_)
 
+        # Store eigenvalue signs during fit
+        if self.eigenvalue_signs_ is None:
+            # Use signs of largest magnitude components
+            self.eigenvalue_signs_ = np.sign(self.alphas_[np.argmax(np.abs(self.alphas_), axis=0), range(self.alphas_.shape[1])])
+            
+        # Apply consistent signs based on eigenvalues
+        X_transformed *= self.eigenvalue_signs_
+
         if self.fit_inverse_transform:
             self._fit_inverse_transform(X_transformed, X)
 
@@ -296,7 +312,10 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):
         check_is_fitted(self, 'X_fit_')
 
         K = self._centerer.transform(self._get_kernel(X, self.X_fit_))
-        return np.dot(K, self.alphas_ / np.sqrt(self.lambdas_))
+        X_transformed = np.dot(K, self.alphas_ / np.sqrt(self.lambdas_))
+        if self.eigenvalue_signs_ is not None:
+            X_transformed *= self.eigenvalue_signs_
+        return X_transformed
 
     def inverse_transform(self, X):
         """Transform X back to original space.
@@ -320,4 +339,4 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):
 
         K = self._get_kernel(X, self.X_transformed_fit_)
 
-        return np.dot(K, self.dual_coef_)
+        return np.dot(K, self.dual_coef_)
\ No newline at end of file
