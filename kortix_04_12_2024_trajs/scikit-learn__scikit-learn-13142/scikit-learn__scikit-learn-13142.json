{
  "instance_id": "scikit-learn__scikit-learn-13142",
  "model_patch": "diff --git a/sklearn/mixture/base.py b/sklearn/mixture/base.py\nindex bd34333c0..1d30fa35f 100644\n--- a/sklearn/mixture/base.py\n+++ b/sklearn/mixture/base.py\n@@ -166,18 +166,9 @@ class BaseMixture(DensityMixin, BaseEstimator, metaclass=ABCMeta):\n         \"\"\"\n         pass\n \n-    def fit(self, X, y=None):\n+    def _fit(self, X, y=None):\n         \"\"\"Estimate model parameters with the EM algorithm.\n \n-        The method fits the model ``n_init`` times and sets the parameters with\n-        which the model has the largest likelihood or lower bound. Within each\n-        trial, the method iterates between E-step and M-step for ``max_iter``\n-        times until the change of likelihood or lower bound is less than\n-        ``tol``, otherwise, a ``ConvergenceWarning`` is raised.\n-        If ``warm_start`` is ``True``, then ``n_init`` is ignored and a single\n-        initialization is performed upon the first call. Upon consecutive\n-        calls, training starts where it left off.\n-\n         Parameters\n         ----------\n         X : array-like, shape (n_samples, n_features)\n@@ -188,36 +179,10 @@ class BaseMixture(DensityMixin, BaseEstimator, metaclass=ABCMeta):\n         -------\n         self\n         \"\"\"\n-        self.fit_predict(X, y)\n-        return self\n-\n-    def fit_predict(self, X, y=None):\n-        \"\"\"Estimate model parameters using X and predict the labels for X.\n-\n-        The method fits the model n_init times and sets the parameters with\n-        which the model has the largest likelihood or lower bound. Within each\n-        trial, the method iterates between E-step and M-step for `max_iter`\n-        times until the change of likelihood or lower bound is less than\n-        `tol`, otherwise, a `ConvergenceWarning` is raised. After fitting, it\n-        predicts the most probable label for the input data points.\n-\n-        .. versionadded:: 0.20\n-\n-        Parameters\n-        ----------\n-        X : array-like, shape (n_samples, n_features)\n-            List of n_features-dimensional data points. Each row\n-            corresponds to a single data point.\n-\n-        Returns\n-        -------\n-        labels : array, shape (n_samples,)\n-            Component labels.\n-        \"\"\"\n         X = _check_X(X, self.n_components, ensure_min_samples=2)\n         self._check_initial_parameters(X)\n \n-        # if we enable warm_start, we will have a unique initialisation\n+        # if we enable warm_start, we will have a unique initialization\n         do_init = not(self.warm_start and hasattr(self, 'converged_'))\n         n_init = self.n_init if do_init else 1\n \n@@ -257,11 +222,6 @@ class BaseMixture(DensityMixin, BaseEstimator, metaclass=ABCMeta):\n                 best_params = self._get_parameters()\n                 best_n_iter = n_iter\n \n-        # Always do a final e-step to guarantee that the labels returned by\n-        # fit_predict(X) are always consistent with fit(X).predict(X)\n-        # for any value of max_iter and tol (and any random_state).\n-        _, log_resp = self._e_step(X)\n-\n         if not self.converged_:\n             warnings.warn('Initialization %d did not converge. '\n                           'Try different init parameters, '\n@@ -273,7 +233,56 @@ class BaseMixture(DensityMixin, BaseEstimator, metaclass=ABCMeta):\n         self.n_iter_ = best_n_iter\n         self.lower_bound_ = max_lower_bound\n \n-        return log_resp.argmax(axis=1)\n+        return self\n+\n+    def fit(self, X, y=None):\n+        \"\"\"Estimate model parameters with the EM algorithm.\n+\n+        The method fits the model ``n_init`` times and sets the parameters with\n+        which the model has the largest likelihood or lower bound. Within each\n+        trial, the method iterates between E-step and M-step for ``max_iter``\n+        times until the change of likelihood or lower bound is less than\n+        ``tol``, otherwise, a ``ConvergenceWarning`` is raised.\n+        If ``warm_start`` is ``True``, then ``n_init`` is ignored and a single\n+        initialization is performed upon the first call. Upon consecutive\n+        calls, training starts where it left off.\n+\n+        Parameters\n+        ----------\n+        X : array-like, shape (n_samples, n_features)\n+            List of n_features-dimensional data points. Each row\n+            corresponds to a single data point.\n+\n+        Returns\n+        -------\n+        self\n+        \"\"\"\n+        return self._fit(X, y)\n+\n+    def fit_predict(self, X, y=None):\n+        \"\"\"Estimate model parameters using X and predict the labels for X.\n+\n+        The method fits the model n_init times and sets the parameters with\n+        which the model has the largest likelihood or lower bound. Within each\n+        trial, the method iterates between E-step and M-step for `max_iter`\n+        times until the change of likelihood or lower bound is less than\n+        `tol`, otherwise, a `ConvergenceWarning` is raised. After fitting, it\n+        predicts the most probable label for the input data points.\n+\n+        .. versionadded:: 0.20\n+\n+        Parameters\n+        ----------\n+        X : array-like, shape (n_samples, n_features)\n+            List of n_features-dimensional data points. Each row\n+            corresponds to a single data point.\n+\n+        Returns\n+        -------\n+        labels : array, shape (n_samples,)\n+            Component labels.\n+        \"\"\"\n+        return self._fit(X, y).predict(X)\n \n     def _e_step(self, X):\n         \"\"\"E step.\n@@ -533,4 +542,4 @@ class BaseMixture(DensityMixin, BaseEstimator, metaclass=ABCMeta):\n             print(\"Initialization converged: %s\" % self.converged_)\n         elif self.verbose >= 2:\n             print(\"Initialization converged: %s\\t time lapse %.5fs\\t ll %.5f\" %\n-                  (self.converged_, time() - self._init_prev_time, ll))\n+                  (self.converged_, time() - self._init_prev_time, ll))\n\\ No newline at end of file\ndiff --git a/sklearn/mixture/tests/test_gaussian_mixture.py b/sklearn/mixture/tests/test_gaussian_mixture.py\nindex 4d549ccd7..ca5336fa8 100644\n--- a/sklearn/mixture/tests/test_gaussian_mixture.py\n+++ b/sklearn/mixture/tests/test_gaussian_mixture.py\n@@ -583,6 +583,8 @@ def test_gaussian_mixture_fit_predict(seed, max_iter, tol):\n     for covar_type in COVARIANCE_TYPE:\n         X = rand_data.X[covar_type]\n         Y = rand_data.Y\n+        \n+        # Test with default n_init=1\n         g = GaussianMixture(n_components=rand_data.n_components,\n                             random_state=rng, weights_init=rand_data.weights,\n                             means_init=rand_data.means,\n@@ -596,6 +598,22 @@ def test_gaussian_mixture_fit_predict(seed, max_iter, tol):\n         Y_pred2 = g.fit_predict(X)\n         assert_array_equal(Y_pred1, Y_pred2)\n         assert_greater(adjusted_rand_score(Y, Y_pred2), .95)\n+        \n+        # Test with n_init>1\n+        g = GaussianMixture(n_components=rand_data.n_components,\n+                            random_state=rng, weights_init=rand_data.weights,\n+                            means_init=rand_data.means,\n+                            precisions_init=rand_data.precisions[covar_type],\n+                            covariance_type=covar_type,\n+                            max_iter=max_iter, tol=tol,\n+                            n_init=3)  # Test with multiple initializations\n+\n+        # check if fit_predict(X) is equivalent to fit(X).predict(X)\n+        f = copy.deepcopy(g)\n+        Y_pred1 = f.fit(X).predict(X)\n+        Y_pred2 = g.fit_predict(X)\n+        assert_array_equal(Y_pred1, Y_pred2)\n+        assert_greater(adjusted_rand_score(Y, Y_pred2), .95)\n \n \n def test_gaussian_mixture_fit():\n@@ -1032,4 +1050,4 @@ def test_init():\n         gmm2 = GaussianMixture(n_components=n_components, n_init=10,\n                                max_iter=1, random_state=random_state).fit(X)\n \n-        assert gmm2.lower_bound_ >= gmm1.lower_bound_\n+        assert gmm2.lower_bound_ >= gmm1.lower_bound_\n\\ No newline at end of file\n",
  "model_name_or_path": "KortixAI"
}