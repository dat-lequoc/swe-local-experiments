diff --git a/sklearn/mixture/base.py b/sklearn/mixture/base.py
index bd34333c0..1d30fa35f 100644
--- a/sklearn/mixture/base.py
+++ b/sklearn/mixture/base.py
@@ -166,18 +166,9 @@ class BaseMixture(DensityMixin, BaseEstimator, metaclass=ABCMeta):
         """
         pass
 
-    def fit(self, X, y=None):
+    def _fit(self, X, y=None):
         """Estimate model parameters with the EM algorithm.
 
-        The method fits the model ``n_init`` times and sets the parameters with
-        which the model has the largest likelihood or lower bound. Within each
-        trial, the method iterates between E-step and M-step for ``max_iter``
-        times until the change of likelihood or lower bound is less than
-        ``tol``, otherwise, a ``ConvergenceWarning`` is raised.
-        If ``warm_start`` is ``True``, then ``n_init`` is ignored and a single
-        initialization is performed upon the first call. Upon consecutive
-        calls, training starts where it left off.
-
         Parameters
         ----------
         X : array-like, shape (n_samples, n_features)
@@ -188,36 +179,10 @@ class BaseMixture(DensityMixin, BaseEstimator, metaclass=ABCMeta):
         -------
         self
         """
-        self.fit_predict(X, y)
-        return self
-
-    def fit_predict(self, X, y=None):
-        """Estimate model parameters using X and predict the labels for X.
-
-        The method fits the model n_init times and sets the parameters with
-        which the model has the largest likelihood or lower bound. Within each
-        trial, the method iterates between E-step and M-step for `max_iter`
-        times until the change of likelihood or lower bound is less than
-        `tol`, otherwise, a `ConvergenceWarning` is raised. After fitting, it
-        predicts the most probable label for the input data points.
-
-        .. versionadded:: 0.20
-
-        Parameters
-        ----------
-        X : array-like, shape (n_samples, n_features)
-            List of n_features-dimensional data points. Each row
-            corresponds to a single data point.
-
-        Returns
-        -------
-        labels : array, shape (n_samples,)
-            Component labels.
-        """
         X = _check_X(X, self.n_components, ensure_min_samples=2)
         self._check_initial_parameters(X)
 
-        # if we enable warm_start, we will have a unique initialisation
+        # if we enable warm_start, we will have a unique initialization
         do_init = not(self.warm_start and hasattr(self, 'converged_'))
         n_init = self.n_init if do_init else 1
 
@@ -257,11 +222,6 @@ class BaseMixture(DensityMixin, BaseEstimator, metaclass=ABCMeta):
                 best_params = self._get_parameters()
                 best_n_iter = n_iter
 
-        # Always do a final e-step to guarantee that the labels returned by
-        # fit_predict(X) are always consistent with fit(X).predict(X)
-        # for any value of max_iter and tol (and any random_state).
-        _, log_resp = self._e_step(X)
-
         if not self.converged_:
             warnings.warn('Initialization %d did not converge. '
                           'Try different init parameters, '
@@ -273,7 +233,56 @@ class BaseMixture(DensityMixin, BaseEstimator, metaclass=ABCMeta):
         self.n_iter_ = best_n_iter
         self.lower_bound_ = max_lower_bound
 
-        return log_resp.argmax(axis=1)
+        return self
+
+    def fit(self, X, y=None):
+        """Estimate model parameters with the EM algorithm.
+
+        The method fits the model ``n_init`` times and sets the parameters with
+        which the model has the largest likelihood or lower bound. Within each
+        trial, the method iterates between E-step and M-step for ``max_iter``
+        times until the change of likelihood or lower bound is less than
+        ``tol``, otherwise, a ``ConvergenceWarning`` is raised.
+        If ``warm_start`` is ``True``, then ``n_init`` is ignored and a single
+        initialization is performed upon the first call. Upon consecutive
+        calls, training starts where it left off.
+
+        Parameters
+        ----------
+        X : array-like, shape (n_samples, n_features)
+            List of n_features-dimensional data points. Each row
+            corresponds to a single data point.
+
+        Returns
+        -------
+        self
+        """
+        return self._fit(X, y)
+
+    def fit_predict(self, X, y=None):
+        """Estimate model parameters using X and predict the labels for X.
+
+        The method fits the model n_init times and sets the parameters with
+        which the model has the largest likelihood or lower bound. Within each
+        trial, the method iterates between E-step and M-step for `max_iter`
+        times until the change of likelihood or lower bound is less than
+        `tol`, otherwise, a `ConvergenceWarning` is raised. After fitting, it
+        predicts the most probable label for the input data points.
+
+        .. versionadded:: 0.20
+
+        Parameters
+        ----------
+        X : array-like, shape (n_samples, n_features)
+            List of n_features-dimensional data points. Each row
+            corresponds to a single data point.
+
+        Returns
+        -------
+        labels : array, shape (n_samples,)
+            Component labels.
+        """
+        return self._fit(X, y).predict(X)
 
     def _e_step(self, X):
         """E step.
@@ -533,4 +542,4 @@ class BaseMixture(DensityMixin, BaseEstimator, metaclass=ABCMeta):
             print("Initialization converged: %s" % self.converged_)
         elif self.verbose >= 2:
             print("Initialization converged: %s\t time lapse %.5fs\t ll %.5f" %
-                  (self.converged_, time() - self._init_prev_time, ll))
+                  (self.converged_, time() - self._init_prev_time, ll))
\ No newline at end of file
diff --git a/sklearn/mixture/tests/test_gaussian_mixture.py b/sklearn/mixture/tests/test_gaussian_mixture.py
index 4d549ccd7..ca5336fa8 100644
--- a/sklearn/mixture/tests/test_gaussian_mixture.py
+++ b/sklearn/mixture/tests/test_gaussian_mixture.py
@@ -583,6 +583,8 @@ def test_gaussian_mixture_fit_predict(seed, max_iter, tol):
     for covar_type in COVARIANCE_TYPE:
         X = rand_data.X[covar_type]
         Y = rand_data.Y
+        
+        # Test with default n_init=1
         g = GaussianMixture(n_components=rand_data.n_components,
                             random_state=rng, weights_init=rand_data.weights,
                             means_init=rand_data.means,
@@ -596,6 +598,22 @@ def test_gaussian_mixture_fit_predict(seed, max_iter, tol):
         Y_pred2 = g.fit_predict(X)
         assert_array_equal(Y_pred1, Y_pred2)
         assert_greater(adjusted_rand_score(Y, Y_pred2), .95)
+        
+        # Test with n_init>1
+        g = GaussianMixture(n_components=rand_data.n_components,
+                            random_state=rng, weights_init=rand_data.weights,
+                            means_init=rand_data.means,
+                            precisions_init=rand_data.precisions[covar_type],
+                            covariance_type=covar_type,
+                            max_iter=max_iter, tol=tol,
+                            n_init=3)  # Test with multiple initializations
+
+        # check if fit_predict(X) is equivalent to fit(X).predict(X)
+        f = copy.deepcopy(g)
+        Y_pred1 = f.fit(X).predict(X)
+        Y_pred2 = g.fit_predict(X)
+        assert_array_equal(Y_pred1, Y_pred2)
+        assert_greater(adjusted_rand_score(Y, Y_pred2), .95)
 
 
 def test_gaussian_mixture_fit():
@@ -1032,4 +1050,4 @@ def test_init():
         gmm2 = GaussianMixture(n_components=n_components, n_init=10,
                                max_iter=1, random_state=random_state).fit(X)
 
-        assert gmm2.lower_bound_ >= gmm1.lower_bound_
+        assert gmm2.lower_bound_ >= gmm1.lower_bound_
\ No newline at end of file
